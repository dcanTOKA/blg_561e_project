{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gmean\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboard_logger import Logger\n",
    "\n",
    "from resnet import resnet50\n",
    "from loss import *\n",
    "from datasets import AgeDB\n",
    "from utils import *\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "imbalanced_related = {\"dataset\": \"agedb\", \"start_epoch\": 0, \"epoch\": 10, \"best_loss\": 1e5, \"lr\": 1e-3,\n",
    "                      \"data_dir\": \"./data\", \"schedule\": [60, 80], \"loss\": \"l1\", \"print_freq\" : 1, \"evaluate\": \"\",\n",
    "                      \"fds\": False, \"bucket_num\": 100, \"bucket_start\": 3, \"start_update\": 0, \"resume\": \"\",\n",
    "                      \"start_smooth\": 1, \"fds_kernel\": \"gaussian\", \"fds_ks\": 9, \"fds_sigma\": 1, \"pretrained\": \"\",\n",
    "                      \"fds_mmt\": 0.9, \"retrain_fc\": False, \"img_size\": 224, \"reweight\": \"none\", \"optimizer\": \"adam\",\n",
    "                      \"lds\": False, \"lds_kernel\": \"gaussian\", \"lds_ks\": 9, \"lds_sigma\": 1, \"model\": \"resnet50\",\n",
    "                      \"batch_size\": 256, \"workers\": 8, \"store_root\": \"./checkpoints\", \"store_name\": \"Test_demo\"\n",
    "                     }\n",
    "args = SimpleNamespace(**imbalanced_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 20:21:11,531 | Args: namespace(batch_size=256, best_loss=100000.0, bucket_num=100, bucket_start=3, data_dir='./data', dataset='agedb', epoch=10, evaluate='', fds=False, fds_kernel='gaussian', fds_ks=9, fds_mmt=0.9, fds_sigma=1, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, loss='l1', lr=0.001, model='resnet50', optimizer='adam', pretrained='', print_freq=1, resume='', retrain_fc=False, reweight='none', schedule=[60, 80], start_epoch=0, start_smooth=1, start_update=0, store_name='agedb_resnet50_Test_demo_adam_l1_0.001_256', store_root='./checkpoints', workers=8)\n",
      "2022-01-19 20:21:11,532 | Store name: agedb_resnet50_Test_demo_adam_l1_0.001_256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Creating folder: ./checkpoints/agedb_resnet50_Test_demo_adam_l1_0.001_256\n"
     ]
    }
   ],
   "source": [
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.fds:\n",
    "    args.store_name += f'_fds_{args.fds_kernel[:3]}_{args.fds_ks}'\n",
    "    if args.fds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.fds_sigma}'\n",
    "    args.store_name += f'_{args.start_update}_{args.start_smooth}_{args.fds_mmt}'\n",
    "if args.retrain_fc:\n",
    "    args.store_name += f'_retrain_fc'\n",
    "args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "logging.root.handlers = []\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(args.store_root, args.store_name, 'training.log')),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "print = logging.info\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "tb_logger = Logger(logdir=os.path.join(args.store_root, args.store_name), flush_secs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 00:01:11,888 | ===> Resume is disabled\n"
     ]
    }
   ],
   "source": [
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "        checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "            torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        args.best_loss = checkpoint['best_loss']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "    else:\n",
    "        print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "        \n",
    "else:\n",
    "    print(\"===> Resume is disabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 20:21:23,633 | =====> Preparing data...\n",
      "2022-01-19 20:21:23,633 | File (.csv): agedb.csv\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('=====> Preparing data...')\n",
    "print(f\"File (.csv): {args.dataset}.csv\")\n",
    "df = pd.read_csv(os.path.join(args.data_dir, f\"{args.dataset}.csv\"))\n",
    "df_train, df_val, df_test = df[df['split'] == 'train'], df[df['split'] == 'val'], df[df['split'] == 'test']\n",
    "train_labels = df_train['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 20:21:28,721 |    age                                path  split\n",
      "0   31   AgeDB/11706_OliviaHussey_31_f.jpg  train\n",
      "1   59   AgeDB/11684_MireilleDarc_59_f.jpg    val\n",
      "2   44   AgeDB/7955_GilbertRoland_44_m.jpg  train\n",
      "3   61  AgeDB/9352_GeorgesMarchal_61_m.jpg    val\n",
      "4   28     AgeDB/3888_TomasMilian_28_m.jpg    val\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AgeDB(data_dir=args.data_dir, df=df_train, img_size=args.img_size, split='train',reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "val_dataset = AgeDB(data_dir=args.data_dir, df=df_val, img_size=args.img_size, split='val')\n",
    "test_dataset = AgeDB(data_dir=args.data_dir, df=df_test, img_size=args.img_size, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                          num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                        num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                         num_workers=args.workers, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 20:21:33,525 | Training data size: 12208\n",
      "2022-01-19 20:21:33,526 | Validation data size: 2140\n",
      "2022-01-19 20:21:33,526 | Test data size: 2140\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data size: {len(train_dataset)}\")\n",
    "print(f\"Validation data size: {len(val_dataset)}\")\n",
    "print(f\"Test data size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 00:01:20,351 | generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist_of_age(bins,ages):\n",
    "    plt.style.use('ggplot')\n",
    "    fig=plt.figure(figsize=(16,6))\n",
    "\n",
    "    plt.hist(ages, bins = bins, edgecolor = 'black')\n",
    "\n",
    "    plt.xlabel(\"Age\")\n",
    "    plt.ylabel(\"Age Disribution\")\n",
    "    plt.title(\"Age Distribution Histogram\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train Data for Age\n",
    "get_hist_of_age(bins=100, ages = train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 20:21:40,197 | =====> Building model...\n",
      "2022-01-19 20:21:40,914 | DataParallel(\n",
      "  (module): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "    (linear): Linear(in_features=2048, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('=====> Building model...')\n",
    "model = resnet50(fds=args.fds, bucket_num=args.bucket_num, bucket_start=args.bucket_start,\n",
    "                 start_update=args.start_update, start_smooth=args.start_smooth,\n",
    "                 kernel=args.fds_kernel, ks=args.fds_ks, sigma=args.fds_sigma, momentum=args.fds_mmt)\n",
    "model = torch.nn.DataParallel(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    print(\"Load train loader\")\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        print(\"===> Batch : \" + str(idx+1))\n",
    "        #inputs, targets, weights = \\\n",
    "        #    inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        if args.fds:\n",
    "            print(\"FDS enable\")\n",
    "            outputs, _ = model(inputs, targets, epoch)\n",
    "        else:\n",
    "            print(\"FDS disable\")\n",
    "            outputs = model(inputs, targets, epoch)\n",
    "\n",
    "        print(\"Calculate Loss\")\n",
    "        loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        print(\"Update Loss\")\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Backward\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx+1)\n",
    "\n",
    "    if args.fds and epoch >= args.start_update:\n",
    "        print(f\"Create Epoch [{epoch}] features of all training data...\")\n",
    "        encodings, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets, _) in tqdm(train_loader):\n",
    "                #inputs = inputs.cuda(non_blocking=True)\n",
    "                outputs, feature = model(inputs, targets, epoch)\n",
    "                encodings.extend(feature.data.squeeze().cpu().numpy())\n",
    "                labels.extend(targets.data.squeeze().cpu().numpy())\n",
    "\n",
    "        encodings, labels = torch.from_numpy(np.vstack(encodings)), torch.from_numpy(np.hstack(labels))\n",
    "        model.module.FDS.update_last_epoch_stats(epoch)\n",
    "        model.module.FDS.update_running_stats(encodings, labels, epoch)\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            #inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=100, low_shot_thr=20):\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "    return shot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 00:01:37,073 | Training...\n",
      "2022-01-19 00:01:37,075 | Load train loader\n",
      "2022-01-19 00:01:38,106 | ===> Batch : 1\n",
      "2022-01-19 00:01:38,107 | FDS disable\n",
      "2022-01-19 00:01:50,862 | Calculate Loss\n",
      "2022-01-19 00:01:50,864 | Update Loss\n",
      "2022-01-19 00:01:50,864 | Backward\n",
      "2022-01-19 00:02:12,782 | Epoch: [0][ 1/48]\tTime  35.71 ( 35.71)\tData 1.0306 (1.0306)\tLoss (L1) 45.939 (45.939)\n",
      "2022-01-19 00:02:12,827 | ===> Batch : 2\n",
      "2022-01-19 00:02:12,828 | FDS disable\n",
      "2022-01-19 00:02:24,999 | Calculate Loss\n",
      "2022-01-19 00:02:25,000 | Update Loss\n",
      "2022-01-19 00:02:25,004 | Backward\n",
      "2022-01-19 00:02:46,488 | Epoch: [0][ 2/48]\tTime  33.71 ( 34.71)\tData 0.0454 (0.5380)\tLoss (L1) 43.037 (44.488)\n",
      "2022-01-19 00:02:46,501 | ===> Batch : 3\n",
      "2022-01-19 00:02:46,502 | FDS disable\n",
      "2022-01-19 00:02:58,540 | Calculate Loss\n",
      "2022-01-19 00:02:58,541 | Update Loss\n",
      "2022-01-19 00:02:58,545 | Backward\n",
      "2022-01-19 00:03:20,046 | Epoch: [0][ 3/48]\tTime  33.56 ( 34.32)\tData 0.0132 (0.3631)\tLoss (L1) 34.809 (41.262)\n",
      "2022-01-19 00:03:20,061 | ===> Batch : 4\n",
      "2022-01-19 00:03:20,061 | FDS disable\n",
      "2022-01-19 00:03:32,148 | Calculate Loss\n",
      "2022-01-19 00:03:32,149 | Update Loss\n",
      "2022-01-19 00:03:32,153 | Backward\n",
      "2022-01-19 00:03:53,001 | Epoch: [0][ 4/48]\tTime  32.96 ( 33.98)\tData 0.0147 (0.2760)\tLoss (L1) 31.664 (38.862)\n",
      "2022-01-19 00:03:53,014 | ===> Batch : 5\n",
      "2022-01-19 00:03:53,015 | FDS disable\n",
      "2022-01-19 00:04:05,048 | Calculate Loss\n",
      "2022-01-19 00:04:05,050 | Update Loss\n",
      "2022-01-19 00:04:05,054 | Backward\n",
      "2022-01-19 00:04:26,136 | Epoch: [0][ 5/48]\tTime  33.13 ( 33.81)\tData 0.0132 (0.2234)\tLoss (L1) 24.341 (35.958)\n",
      "2022-01-19 00:04:26,152 | ===> Batch : 6\n",
      "2022-01-19 00:04:26,153 | FDS disable\n",
      "2022-01-19 00:04:38,163 | Calculate Loss\n",
      "2022-01-19 00:04:38,164 | Update Loss\n",
      "2022-01-19 00:04:38,168 | Backward\n",
      "2022-01-19 00:04:58,940 | Epoch: [0][ 6/48]\tTime  32.80 ( 33.64)\tData 0.0163 (0.1889)\tLoss (L1) 20.738 (33.421)\n",
      "2022-01-19 00:04:58,953 | ===> Batch : 7\n",
      "2022-01-19 00:04:58,954 | FDS disable\n",
      "2022-01-19 00:05:10,933 | Calculate Loss\n",
      "2022-01-19 00:05:10,934 | Update Loss\n",
      "2022-01-19 00:05:10,938 | Backward\n",
      "2022-01-19 00:05:32,235 | Epoch: [0][ 7/48]\tTime  33.29 ( 33.59)\tData 0.0134 (0.1638)\tLoss (L1) 16.902 (31.061)\n",
      "2022-01-19 00:05:32,248 | ===> Batch : 8\n",
      "2022-01-19 00:05:32,249 | FDS disable\n",
      "2022-01-19 00:05:44,204 | Calculate Loss\n",
      "2022-01-19 00:05:44,205 | Update Loss\n",
      "2022-01-19 00:05:44,209 | Backward\n",
      "2022-01-19 00:06:05,709 | Epoch: [0][ 8/48]\tTime  33.47 ( 33.58)\tData 0.0132 (0.1450)\tLoss (L1) 15.385 (29.102)\n",
      "2022-01-19 00:06:05,731 | ===> Batch : 9\n",
      "2022-01-19 00:06:05,732 | FDS disable\n",
      "2022-01-19 00:06:17,612 | Calculate Loss\n",
      "2022-01-19 00:06:17,613 | Update Loss\n",
      "2022-01-19 00:06:17,617 | Backward\n",
      "2022-01-19 00:06:38,528 | Epoch: [0][ 9/48]\tTime  32.82 ( 33.49)\tData 0.0225 (0.1314)\tLoss (L1) 14.979 (27.533)\n",
      "2022-01-19 00:06:38,541 | ===> Batch : 10\n",
      "2022-01-19 00:06:38,541 | FDS disable\n",
      "2022-01-19 00:06:50,601 | Calculate Loss\n",
      "2022-01-19 00:06:50,602 | Update Loss\n",
      "2022-01-19 00:06:50,606 | Backward\n",
      "2022-01-19 00:07:11,064 | Epoch: [0][10/48]\tTime  32.54 ( 33.40)\tData 0.0125 (0.1195)\tLoss (L1) 12.763 (26.056)\n",
      "2022-01-19 00:07:11,076 | ===> Batch : 11\n",
      "2022-01-19 00:07:11,077 | FDS disable\n",
      "2022-01-19 00:07:23,027 | Calculate Loss\n",
      "2022-01-19 00:07:23,028 | Update Loss\n",
      "2022-01-19 00:07:23,032 | Backward\n",
      "2022-01-19 00:07:44,231 | Epoch: [0][11/48]\tTime  33.17 ( 33.38)\tData 0.0126 (0.1098)\tLoss (L1) 12.651 (24.837)\n",
      "2022-01-19 00:07:44,246 | ===> Batch : 12\n",
      "2022-01-19 00:07:44,246 | FDS disable\n",
      "2022-01-19 00:07:56,141 | Calculate Loss\n",
      "2022-01-19 00:07:56,143 | Update Loss\n",
      "2022-01-19 00:07:56,147 | Backward\n",
      "2022-01-19 00:08:17,008 | Epoch: [0][12/48]\tTime  32.78 ( 33.33)\tData 0.0144 (0.1018)\tLoss (L1) 12.481 (23.807)\n",
      "2022-01-19 00:08:17,021 | ===> Batch : 13\n",
      "2022-01-19 00:08:17,022 | FDS disable\n",
      "2022-01-19 00:08:28,909 | Calculate Loss\n",
      "2022-01-19 00:08:28,910 | Update Loss\n",
      "2022-01-19 00:08:28,914 | Backward\n",
      "2022-01-19 00:08:49,825 | Epoch: [0][13/48]\tTime  32.82 ( 33.29)\tData 0.0128 (0.0950)\tLoss (L1) 11.678 (22.874)\n",
      "2022-01-19 00:08:49,838 | ===> Batch : 14\n",
      "2022-01-19 00:08:49,838 | FDS disable\n",
      "2022-01-19 00:09:01,685 | Calculate Loss\n",
      "2022-01-19 00:09:01,688 | Update Loss\n",
      "2022-01-19 00:09:01,692 | Backward\n",
      "2022-01-19 00:09:22,408 | Epoch: [0][14/48]\tTime  32.58 ( 33.24)\tData 0.0129 (0.0891)\tLoss (L1) 12.933 (22.164)\n",
      "2022-01-19 00:09:22,420 | ===> Batch : 15\n",
      "2022-01-19 00:09:22,421 | FDS disable\n",
      "2022-01-19 00:09:34,293 | Calculate Loss\n",
      "2022-01-19 00:09:34,294 | Update Loss\n",
      "2022-01-19 00:09:34,298 | Backward\n",
      "2022-01-19 00:09:55,938 | Epoch: [0][15/48]\tTime  33.53 ( 33.26)\tData 0.0126 (0.0840)\tLoss (L1) 11.788 (21.473)\n",
      "2022-01-19 00:09:55,951 | ===> Batch : 16\n",
      "2022-01-19 00:09:55,951 | FDS disable\n",
      "2022-01-19 00:10:07,850 | Calculate Loss\n",
      "2022-01-19 00:10:07,854 | Update Loss\n",
      "2022-01-19 00:10:07,858 | Backward\n",
      "2022-01-19 00:10:29,218 | Epoch: [0][16/48]\tTime  33.28 ( 33.26)\tData 0.0129 (0.0796)\tLoss (L1) 12.748 (20.927)\n",
      "2022-01-19 00:10:29,232 | ===> Batch : 17\n",
      "2022-01-19 00:10:29,232 | FDS disable\n",
      "2022-01-19 00:10:41,119 | Calculate Loss\n",
      "2022-01-19 00:10:41,121 | Update Loss\n",
      "2022-01-19 00:10:41,125 | Backward\n",
      "2022-01-19 00:11:02,265 | Epoch: [0][17/48]\tTime  33.05 ( 33.25)\tData 0.0143 (0.0757)\tLoss (L1) 12.470 (20.430)\n",
      "2022-01-19 00:11:02,279 | ===> Batch : 18\n",
      "2022-01-19 00:11:02,280 | FDS disable\n",
      "2022-01-19 00:11:14,161 | Calculate Loss\n",
      "2022-01-19 00:11:14,163 | Update Loss\n",
      "2022-01-19 00:11:14,167 | Backward\n",
      "2022-01-19 00:11:35,762 | Epoch: [0][18/48]\tTime  33.50 ( 33.26)\tData 0.0142 (0.0723)\tLoss (L1) 13.186 (20.027)\n",
      "2022-01-19 00:11:35,776 | ===> Batch : 19\n",
      "2022-01-19 00:11:35,777 | FDS disable\n",
      "2022-01-19 00:11:47,673 | Calculate Loss\n",
      "2022-01-19 00:11:47,674 | Update Loss\n",
      "2022-01-19 00:11:47,679 | Backward\n",
      "2022-01-19 00:12:08,984 | Epoch: [0][19/48]\tTime  33.22 ( 33.26)\tData 0.0140 (0.0692)\tLoss (L1) 12.623 (19.638)\n",
      "2022-01-19 00:12:08,998 | ===> Batch : 20\n",
      "2022-01-19 00:12:08,998 | FDS disable\n",
      "2022-01-19 00:12:20,849 | Calculate Loss\n",
      "2022-01-19 00:12:20,850 | Update Loss\n",
      "2022-01-19 00:12:20,854 | Backward\n",
      "2022-01-19 00:12:42,399 | Epoch: [0][20/48]\tTime  33.41 ( 33.27)\tData 0.0140 (0.0665)\tLoss (L1) 11.877 (19.250)\n",
      "2022-01-19 00:12:42,413 | ===> Batch : 21\n",
      "2022-01-19 00:12:42,413 | FDS disable\n",
      "2022-01-19 00:12:54,319 | Calculate Loss\n",
      "2022-01-19 00:12:54,321 | Update Loss\n",
      "2022-01-19 00:12:54,325 | Backward\n",
      "2022-01-19 00:13:15,318 | Epoch: [0][21/48]\tTime  32.92 ( 33.25)\tData 0.0141 (0.0640)\tLoss (L1) 11.943 (18.902)\n",
      "2022-01-19 00:13:15,332 | ===> Batch : 22\n",
      "2022-01-19 00:13:15,332 | FDS disable\n",
      "2022-01-19 00:13:27,159 | Calculate Loss\n",
      "2022-01-19 00:13:27,160 | Update Loss\n",
      "2022-01-19 00:13:27,163 | Backward\n",
      "2022-01-19 00:13:48,580 | Epoch: [0][22/48]\tTime  33.26 ( 33.25)\tData 0.0138 (0.0617)\tLoss (L1) 11.694 (18.574)\n",
      "2022-01-19 00:13:48,593 | ===> Batch : 23\n",
      "2022-01-19 00:13:48,594 | FDS disable\n",
      "2022-01-19 00:14:00,488 | Calculate Loss\n",
      "2022-01-19 00:14:00,489 | Update Loss\n",
      "2022-01-19 00:14:00,493 | Backward\n",
      "2022-01-19 00:14:21,988 | Epoch: [0][23/48]\tTime  33.41 ( 33.26)\tData 0.0137 (0.0596)\tLoss (L1) 12.060 (18.291)\n",
      "2022-01-19 00:14:22,002 | ===> Batch : 24\n",
      "2022-01-19 00:14:22,002 | FDS disable\n",
      "2022-01-19 00:14:33,841 | Calculate Loss\n",
      "2022-01-19 00:14:33,842 | Update Loss\n",
      "2022-01-19 00:14:33,846 | Backward\n",
      "2022-01-19 00:14:54,598 | Epoch: [0][24/48]\tTime  32.61 ( 33.23)\tData 0.0140 (0.0577)\tLoss (L1) 11.515 (18.009)\n",
      "2022-01-19 00:14:54,612 | ===> Batch : 25\n",
      "2022-01-19 00:14:54,613 | FDS disable\n",
      "2022-01-19 00:15:06,556 | Calculate Loss\n",
      "2022-01-19 00:15:06,557 | Update Loss\n",
      "2022-01-19 00:15:06,561 | Backward\n",
      "2022-01-19 00:15:27,997 | Epoch: [0][25/48]\tTime  33.40 ( 33.24)\tData 0.0139 (0.0560)\tLoss (L1) 11.222 (17.737)\n",
      "2022-01-19 00:15:28,011 | ===> Batch : 26\n",
      "2022-01-19 00:15:28,011 | FDS disable\n",
      "2022-01-19 00:15:39,862 | Calculate Loss\n",
      "2022-01-19 00:15:39,863 | Update Loss\n",
      "2022-01-19 00:15:39,867 | Backward\n",
      "2022-01-19 00:16:00,672 | Epoch: [0][26/48]\tTime  32.67 ( 33.22)\tData 0.0140 (0.0544)\tLoss (L1) 11.128 (17.483)\n",
      "2022-01-19 00:16:00,686 | ===> Batch : 27\n",
      "2022-01-19 00:16:00,686 | FDS disable\n",
      "2022-01-19 00:16:12,514 | Calculate Loss\n",
      "2022-01-19 00:16:12,516 | Update Loss\n",
      "2022-01-19 00:16:12,519 | Backward\n",
      "2022-01-19 00:16:33,351 | Epoch: [0][27/48]\tTime  32.68 ( 33.20)\tData 0.0141 (0.0529)\tLoss (L1) 12.515 (17.299)\n",
      "2022-01-19 00:16:33,365 | ===> Batch : 28\n",
      "2022-01-19 00:16:33,365 | FDS disable\n",
      "2022-01-19 00:16:45,223 | Calculate Loss\n",
      "2022-01-19 00:16:45,224 | Update Loss\n",
      "2022-01-19 00:16:45,228 | Backward\n",
      "2022-01-19 00:17:06,167 | Epoch: [0][28/48]\tTime  32.82 ( 33.18)\tData 0.0139 (0.0515)\tLoss (L1) 11.778 (17.102)\n",
      "2022-01-19 00:17:06,182 | ===> Batch : 29\n",
      "2022-01-19 00:17:06,182 | FDS disable\n",
      "2022-01-19 00:17:17,949 | Calculate Loss\n",
      "2022-01-19 00:17:17,950 | Update Loss\n",
      "2022-01-19 00:17:17,954 | Backward\n",
      "2022-01-19 00:17:38,628 | Epoch: [0][29/48]\tTime  32.46 ( 33.16)\tData 0.0145 (0.0502)\tLoss (L1) 11.269 (16.901)\n",
      "2022-01-19 00:17:38,641 | ===> Batch : 30\n",
      "2022-01-19 00:17:38,642 | FDS disable\n",
      "2022-01-19 00:17:50,437 | Calculate Loss\n",
      "2022-01-19 00:17:50,439 | Update Loss\n",
      "2022-01-19 00:17:50,443 | Backward\n",
      "2022-01-19 00:18:11,548 | Epoch: [0][30/48]\tTime  32.92 ( 33.15)\tData 0.0137 (0.0490)\tLoss (L1) 11.189 (16.710)\n",
      "2022-01-19 00:18:11,569 | ===> Batch : 31\n",
      "2022-01-19 00:18:11,570 | FDS disable\n",
      "2022-01-19 00:18:23,342 | Calculate Loss\n",
      "2022-01-19 00:18:23,344 | Update Loss\n",
      "2022-01-19 00:18:23,347 | Backward\n",
      "2022-01-19 00:18:44,333 | Epoch: [0][31/48]\tTime  32.79 ( 33.14)\tData 0.0216 (0.0481)\tLoss (L1) 11.407 (16.539)\n",
      "2022-01-19 00:18:44,348 | ===> Batch : 32\n",
      "2022-01-19 00:18:44,348 | FDS disable\n",
      "2022-01-19 00:18:56,121 | Calculate Loss\n",
      "2022-01-19 00:18:56,123 | Update Loss\n",
      "2022-01-19 00:18:56,127 | Backward\n",
      "2022-01-19 00:19:17,126 | Epoch: [0][32/48]\tTime  32.79 ( 33.13)\tData 0.0147 (0.0470)\tLoss (L1) 12.077 (16.400)\n",
      "2022-01-19 00:19:17,139 | ===> Batch : 33\n",
      "2022-01-19 00:19:17,140 | FDS disable\n",
      "2022-01-19 00:19:28,814 | Calculate Loss\n",
      "2022-01-19 00:19:28,815 | Update Loss\n",
      "2022-01-19 00:19:28,819 | Backward\n",
      "2022-01-19 00:19:49,800 | Epoch: [0][33/48]\tTime  32.67 ( 33.11)\tData 0.0137 (0.0460)\tLoss (L1) 12.225 (16.273)\n",
      "2022-01-19 00:19:49,813 | ===> Batch : 34\n",
      "2022-01-19 00:19:49,814 | FDS disable\n",
      "2022-01-19 00:20:01,500 | Calculate Loss\n",
      "2022-01-19 00:20:01,502 | Update Loss\n",
      "2022-01-19 00:20:01,505 | Backward\n",
      "2022-01-19 00:20:22,481 | Epoch: [0][34/48]\tTime  32.68 ( 33.10)\tData 0.0136 (0.0451)\tLoss (L1) 11.365 (16.129)\n",
      "2022-01-19 00:20:22,495 | ===> Batch : 35\n",
      "2022-01-19 00:20:22,495 | FDS disable\n",
      "2022-01-19 00:20:34,142 | Calculate Loss\n",
      "2022-01-19 00:20:34,143 | Update Loss\n",
      "2022-01-19 00:20:34,147 | Backward\n",
      "2022-01-19 00:20:55,204 | Epoch: [0][35/48]\tTime  32.72 ( 33.09)\tData 0.0139 (0.0442)\tLoss (L1) 11.815 (16.006)\n",
      "2022-01-19 00:20:55,218 | ===> Batch : 36\n",
      "2022-01-19 00:20:55,218 | FDS disable\n",
      "2022-01-19 00:21:06,832 | Calculate Loss\n",
      "2022-01-19 00:21:06,833 | Update Loss\n",
      "2022-01-19 00:21:06,837 | Backward\n",
      "2022-01-19 00:21:27,890 | Epoch: [0][36/48]\tTime  32.69 ( 33.08)\tData 0.0141 (0.0434)\tLoss (L1) 11.573 (15.882)\n",
      "2022-01-19 00:21:27,904 | ===> Batch : 37\n",
      "2022-01-19 00:21:27,904 | FDS disable\n",
      "2022-01-19 00:21:39,514 | Calculate Loss\n",
      "2022-01-19 00:21:39,516 | Update Loss\n",
      "2022-01-19 00:21:39,520 | Backward\n",
      "2022-01-19 00:22:00,576 | Epoch: [0][37/48]\tTime  32.69 ( 33.07)\tData 0.0144 (0.0426)\tLoss (L1) 11.575 (15.766)\n",
      "2022-01-19 00:22:00,590 | ===> Batch : 38\n",
      "2022-01-19 00:22:00,591 | FDS disable\n",
      "2022-01-19 00:22:12,207 | Calculate Loss\n",
      "2022-01-19 00:22:12,208 | Update Loss\n",
      "2022-01-19 00:22:12,212 | Backward\n",
      "2022-01-19 00:22:33,967 | Epoch: [0][38/48]\tTime  33.39 ( 33.08)\tData 0.0145 (0.0418)\tLoss (L1) 12.858 (15.689)\n",
      "2022-01-19 00:22:33,981 | ===> Batch : 39\n",
      "2022-01-19 00:22:33,982 | FDS disable\n",
      "2022-01-19 00:22:45,655 | Calculate Loss\n",
      "2022-01-19 00:22:45,656 | Update Loss\n",
      "2022-01-19 00:22:45,660 | Backward\n",
      "2022-01-19 00:23:06,353 | Epoch: [0][39/48]\tTime  32.39 ( 33.06)\tData 0.0138 (0.0411)\tLoss (L1) 12.142 (15.599)\n",
      "2022-01-19 00:23:06,367 | ===> Batch : 40\n",
      "2022-01-19 00:23:06,368 | FDS disable\n",
      "2022-01-19 00:23:18,044 | Calculate Loss\n",
      "2022-01-19 00:23:18,046 | Update Loss\n",
      "2022-01-19 00:23:18,049 | Backward\n",
      "2022-01-19 00:23:38,787 | Epoch: [0][40/48]\tTime  32.43 ( 33.04)\tData 0.0140 (0.0404)\tLoss (L1) 11.130 (15.487)\n",
      "2022-01-19 00:23:38,801 | ===> Batch : 41\n",
      "2022-01-19 00:23:38,801 | FDS disable\n",
      "2022-01-19 00:23:50,344 | Calculate Loss\n",
      "2022-01-19 00:23:50,345 | Update Loss\n",
      "2022-01-19 00:23:50,349 | Backward\n",
      "2022-01-19 00:24:11,298 | Epoch: [0][41/48]\tTime  32.51 ( 33.03)\tData 0.0142 (0.0398)\tLoss (L1) 11.267 (15.384)\n",
      "2022-01-19 00:24:11,313 | ===> Batch : 42\n",
      "2022-01-19 00:24:11,314 | FDS disable\n",
      "2022-01-19 00:24:22,906 | Calculate Loss\n",
      "2022-01-19 00:24:22,907 | Update Loss\n",
      "2022-01-19 00:24:22,911 | Backward\n",
      "2022-01-19 00:24:43,587 | Epoch: [0][42/48]\tTime  32.29 ( 33.01)\tData 0.0147 (0.0392)\tLoss (L1) 11.848 (15.300)\n",
      "2022-01-19 00:24:43,601 | ===> Batch : 43\n",
      "2022-01-19 00:24:43,602 | FDS disable\n",
      "2022-01-19 00:24:55,208 | Calculate Loss\n",
      "2022-01-19 00:24:55,209 | Update Loss\n",
      "2022-01-19 00:24:55,213 | Backward\n",
      "2022-01-19 00:25:16,637 | Epoch: [0][43/48]\tTime  33.05 ( 33.01)\tData 0.0141 (0.0386)\tLoss (L1) 11.811 (15.219)\n",
      "2022-01-19 00:25:16,652 | ===> Batch : 44\n",
      "2022-01-19 00:25:16,652 | FDS disable\n",
      "2022-01-19 00:25:28,293 | Calculate Loss\n",
      "2022-01-19 00:25:28,294 | Update Loss\n",
      "2022-01-19 00:25:28,298 | Backward\n",
      "2022-01-19 00:25:49,524 | Epoch: [0][44/48]\tTime  32.89 ( 33.01)\tData 0.0150 (0.0381)\tLoss (L1) 11.041 (15.124)\n",
      "2022-01-19 00:25:49,539 | ===> Batch : 45\n",
      "2022-01-19 00:25:49,540 | FDS disable\n",
      "2022-01-19 00:26:01,134 | Calculate Loss\n",
      "2022-01-19 00:26:01,136 | Update Loss\n",
      "2022-01-19 00:26:01,139 | Backward\n",
      "2022-01-19 00:26:22,497 | Epoch: [0][45/48]\tTime  32.97 ( 33.01)\tData 0.0147 (0.0376)\tLoss (L1) 11.741 (15.048)\n",
      "2022-01-19 00:26:22,510 | ===> Batch : 46\n",
      "2022-01-19 00:26:22,511 | FDS disable\n",
      "2022-01-19 00:26:34,112 | Calculate Loss\n",
      "2022-01-19 00:26:34,113 | Update Loss\n",
      "2022-01-19 00:26:34,117 | Backward\n",
      "2022-01-19 00:26:55,126 | Epoch: [0][46/48]\tTime  32.63 ( 33.00)\tData 0.0139 (0.0370)\tLoss (L1) 11.910 (14.980)\n",
      "2022-01-19 00:26:55,141 | ===> Batch : 47\n",
      "2022-01-19 00:26:55,141 | FDS disable\n",
      "2022-01-19 00:27:06,768 | Calculate Loss\n",
      "2022-01-19 00:27:06,769 | Update Loss\n",
      "2022-01-19 00:27:06,773 | Backward\n",
      "2022-01-19 00:27:27,681 | Epoch: [0][47/48]\tTime  32.55 ( 32.99)\tData 0.0145 (0.0366)\tLoss (L1) 12.029 (14.917)\n",
      "2022-01-19 00:27:27,695 | ===> Batch : 48\n",
      "2022-01-19 00:27:27,696 | FDS disable\n",
      "2022-01-19 00:27:35,630 | Calculate Loss\n",
      "2022-01-19 00:27:35,631 | Update Loss\n",
      "2022-01-19 00:27:35,635 | Backward\n",
      "2022-01-19 00:27:50,516 | Epoch: [0][48/48]\tTime  22.84 ( 32.78)\tData 0.0143 (0.0361)\tLoss (L1) 11.625 (14.870)\n",
      "2022-01-19 00:28:02,408 | Val: [0/9]\tTime 11.843 (11.843)\tLoss (MSE) 325.518 (325.518)\tLoss (L1) 14.898 (14.898)\n",
      "2022-01-19 00:28:11,196 | Val: [1/9]\tTime  8.788 (10.315)\tLoss (MSE) 308.297 (316.907)\tLoss (L1) 14.413 (14.655)\n",
      "2022-01-19 00:28:19,976 | Val: [2/9]\tTime  8.781 ( 9.804)\tLoss (MSE) 361.287 (331.700)\tLoss (L1) 15.315 (14.875)\n",
      "2022-01-19 00:28:28,718 | Val: [3/9]\tTime  8.742 ( 9.538)\tLoss (MSE) 340.617 (333.930)\tLoss (L1) 14.813 (14.860)\n",
      "2022-01-19 00:28:37,491 | Val: [4/9]\tTime  8.772 ( 9.385)\tLoss (MSE) 344.677 (336.079)\tLoss (L1) 14.945 (14.877)\n",
      "2022-01-19 00:28:46,264 | Val: [5/9]\tTime  8.773 ( 9.283)\tLoss (MSE) 331.311 (335.285)\tLoss (L1) 14.862 (14.874)\n",
      "2022-01-19 00:28:55,011 | Val: [6/9]\tTime  8.747 ( 9.207)\tLoss (MSE) 299.630 (330.191)\tLoss (L1) 14.000 (14.749)\n",
      "2022-01-19 00:29:03,795 | Val: [7/9]\tTime  8.784 ( 9.154)\tLoss (MSE) 392.130 (337.933)\tLoss (L1) 15.877 (14.890)\n",
      "2022-01-19 00:29:07,121 | Val: [8/9]\tTime  3.326 ( 8.506)\tLoss (MSE) 345.973 (338.279)\tLoss (L1) 14.682 (14.881)\n",
      "2022-01-19 00:29:07,255 |  * Overall: MSE 338.279\tL1 14.881\tG-Mean 10.060\n",
      "2022-01-19 00:29:07,255 |  * Many: MSE 240.671\tL1 12.406\tG-Mean 8.259\n",
      "2022-01-19 00:29:07,256 |  * Median: MSE 465.622\tL1 18.496\tG-Mean 13.919\n",
      "2022-01-19 00:29:07,256 |  * Low: MSE 907.971\tL1 28.261\tG-Mean 26.417\n",
      "2022-01-19 00:29:07,261 | Best L1 Loss: 14.881\n",
      "2022-01-19 00:29:07,484 | ===> Saving current best checkpoint...\n",
      "2022-01-19 00:29:07,593 | Epoch #0: Train loss [14.8700]; Val loss: MSE [338.2791], L1 [14.8815], G-Mean [10.0601]\n",
      "2022-01-19 00:29:07,594 | Training...\n",
      "2022-01-19 00:29:07,595 | Load train loader\n",
      "2022-01-19 00:29:08,703 | ===> Batch : 1\n",
      "2022-01-19 00:29:08,704 | FDS disable\n",
      "2022-01-19 00:29:22,857 | Calculate Loss\n",
      "2022-01-19 00:29:22,858 | Update Loss\n",
      "2022-01-19 00:29:22,987 | Backward\n",
      "2022-01-19 00:29:45,067 | Epoch: [1][ 1/48]\tTime  37.47 ( 37.47)\tData 1.1073 (1.1073)\tLoss (L1) 10.876 (10.876)\n",
      "2022-01-19 00:29:45,086 | ===> Batch : 2\n",
      "2022-01-19 00:29:45,086 | FDS disable\n",
      "2022-01-19 00:29:56,901 | Calculate Loss\n",
      "2022-01-19 00:29:56,903 | Update Loss\n",
      "2022-01-19 00:29:56,907 | Backward\n",
      "2022-01-19 00:30:17,989 | Epoch: [1][ 2/48]\tTime  32.92 ( 35.20)\tData 0.0189 (0.5631)\tLoss (L1) 11.420 (11.148)\n",
      "2022-01-19 00:30:18,002 | ===> Batch : 3\n",
      "2022-01-19 00:30:18,002 | FDS disable\n",
      "2022-01-19 00:30:29,857 | Calculate Loss\n",
      "2022-01-19 00:30:29,858 | Update Loss\n",
      "2022-01-19 00:30:29,862 | Backward\n",
      "2022-01-19 00:30:50,784 | Epoch: [1][ 3/48]\tTime  32.80 ( 34.40)\tData 0.0130 (0.3797)\tLoss (L1) 9.983 (10.760)\n",
      "2022-01-19 00:30:50,798 | ===> Batch : 4\n",
      "2022-01-19 00:30:50,798 | FDS disable\n",
      "2022-01-19 00:31:02,621 | Calculate Loss\n",
      "2022-01-19 00:31:02,623 | Update Loss\n",
      "2022-01-19 00:31:02,627 | Backward\n",
      "2022-01-19 00:31:23,886 | Epoch: [1][ 4/48]\tTime  33.10 ( 34.07)\tData 0.0134 (0.2881)\tLoss (L1) 11.089 (10.842)\n",
      "2022-01-19 00:31:23,902 | ===> Batch : 5\n",
      "2022-01-19 00:31:23,902 | FDS disable\n",
      "2022-01-19 00:31:35,773 | Calculate Loss\n",
      "2022-01-19 00:31:35,775 | Update Loss\n",
      "2022-01-19 00:31:35,778 | Backward\n",
      "2022-01-19 00:31:56,817 | Epoch: [1][ 5/48]\tTime  32.93 ( 33.84)\tData 0.0160 (0.2337)\tLoss (L1) 11.590 (10.992)\n",
      "2022-01-19 00:31:56,830 | ===> Batch : 6\n",
      "2022-01-19 00:31:56,831 | FDS disable\n",
      "2022-01-19 00:32:08,640 | Calculate Loss\n",
      "2022-01-19 00:32:08,642 | Update Loss\n",
      "2022-01-19 00:32:08,646 | Backward\n",
      "2022-01-19 00:32:30,244 | Epoch: [1][ 6/48]\tTime  33.43 ( 33.77)\tData 0.0131 (0.1969)\tLoss (L1) 11.285 (11.040)\n",
      "2022-01-19 00:32:30,256 | ===> Batch : 7\n",
      "2022-01-19 00:32:30,257 | FDS disable\n",
      "2022-01-19 00:32:42,122 | Calculate Loss\n",
      "2022-01-19 00:32:42,123 | Update Loss\n",
      "2022-01-19 00:32:42,127 | Backward\n",
      "2022-01-19 00:33:03,631 | Epoch: [1][ 7/48]\tTime  33.39 ( 33.72)\tData 0.0129 (0.1706)\tLoss (L1) 11.546 (11.113)\n",
      "2022-01-19 00:33:03,646 | ===> Batch : 8\n",
      "2022-01-19 00:33:03,646 | FDS disable\n",
      "2022-01-19 00:33:15,501 | Calculate Loss\n",
      "2022-01-19 00:33:15,503 | Update Loss\n",
      "2022-01-19 00:33:15,507 | Backward\n",
      "2022-01-19 00:33:37,029 | Epoch: [1][ 8/48]\tTime  33.40 ( 33.68)\tData 0.0145 (0.1511)\tLoss (L1) 11.007 (11.100)\n",
      "2022-01-19 00:33:37,045 | ===> Batch : 9\n",
      "2022-01-19 00:33:37,046 | FDS disable\n",
      "2022-01-19 00:33:48,885 | Calculate Loss\n",
      "2022-01-19 00:33:48,886 | Update Loss\n",
      "2022-01-19 00:33:48,890 | Backward\n",
      "2022-01-19 00:34:10,165 | Epoch: [1][ 9/48]\tTime  33.14 ( 33.62)\tData 0.0163 (0.1361)\tLoss (L1) 11.074 (11.097)\n",
      "2022-01-19 00:34:10,183 | ===> Batch : 10\n",
      "2022-01-19 00:34:10,184 | FDS disable\n",
      "2022-01-19 00:34:21,992 | Calculate Loss\n",
      "2022-01-19 00:34:21,993 | Update Loss\n",
      "2022-01-19 00:34:21,997 | Backward\n",
      "2022-01-19 00:34:42,828 | Epoch: [1][10/48]\tTime  32.66 ( 33.52)\tData 0.0177 (0.1243)\tLoss (L1) 11.584 (11.145)\n",
      "2022-01-19 00:34:42,840 | ===> Batch : 11\n",
      "2022-01-19 00:34:42,841 | FDS disable\n",
      "2022-01-19 00:34:54,701 | Calculate Loss\n",
      "2022-01-19 00:34:54,702 | Update Loss\n",
      "2022-01-19 00:34:54,706 | Backward\n",
      "2022-01-19 00:35:15,982 | Epoch: [1][11/48]\tTime  33.15 ( 33.49)\tData 0.0128 (0.1142)\tLoss (L1) 11.283 (11.158)\n",
      "2022-01-19 00:35:15,995 | ===> Batch : 12\n",
      "2022-01-19 00:35:15,996 | FDS disable\n",
      "2022-01-19 00:35:27,817 | Calculate Loss\n",
      "2022-01-19 00:35:27,818 | Update Loss\n",
      "2022-01-19 00:35:27,822 | Backward\n",
      "2022-01-19 00:35:49,311 | Epoch: [1][12/48]\tTime  33.33 ( 33.48)\tData 0.0133 (0.1057)\tLoss (L1) 11.827 (11.214)\n",
      "2022-01-19 00:35:49,324 | ===> Batch : 13\n",
      "2022-01-19 00:35:49,325 | FDS disable\n",
      "2022-01-19 00:36:01,187 | Calculate Loss\n",
      "2022-01-19 00:36:01,188 | Update Loss\n",
      "2022-01-19 00:36:01,192 | Backward\n",
      "2022-01-19 00:36:22,087 | Epoch: [1][13/48]\tTime  32.78 ( 33.42)\tData 0.0129 (0.0986)\tLoss (L1) 11.154 (11.209)\n",
      "2022-01-19 00:36:22,101 | ===> Batch : 14\n",
      "2022-01-19 00:36:22,101 | FDS disable\n",
      "2022-01-19 00:36:33,946 | Calculate Loss\n",
      "2022-01-19 00:36:33,947 | Update Loss\n",
      "2022-01-19 00:36:33,951 | Backward\n",
      "2022-01-19 00:36:55,513 | Epoch: [1][14/48]\tTime  33.43 ( 33.42)\tData 0.0141 (0.0926)\tLoss (L1) 12.632 (11.311)\n",
      "2022-01-19 00:36:55,526 | ===> Batch : 15\n",
      "2022-01-19 00:36:55,527 | FDS disable\n",
      "2022-01-19 00:37:07,377 | Calculate Loss\n",
      "2022-01-19 00:37:07,378 | Update Loss\n",
      "2022-01-19 00:37:07,382 | Backward\n",
      "2022-01-19 00:37:28,907 | Epoch: [1][15/48]\tTime  33.39 ( 33.42)\tData 0.0126 (0.0872)\tLoss (L1) 11.134 (11.299)\n",
      "2022-01-19 00:37:28,920 | ===> Batch : 16\n",
      "2022-01-19 00:37:28,921 | FDS disable\n",
      "2022-01-19 00:37:40,801 | Calculate Loss\n",
      "2022-01-19 00:37:40,803 | Update Loss\n",
      "2022-01-19 00:37:40,806 | Backward\n",
      "2022-01-19 00:38:02,335 | Epoch: [1][16/48]\tTime  33.43 ( 33.42)\tData 0.0126 (0.0826)\tLoss (L1) 12.154 (11.352)\n",
      "2022-01-19 00:38:02,349 | ===> Batch : 17\n",
      "2022-01-19 00:38:02,350 | FDS disable\n",
      "2022-01-19 00:38:14,191 | Calculate Loss\n",
      "2022-01-19 00:38:14,193 | Update Loss\n",
      "2022-01-19 00:38:14,196 | Backward\n",
      "2022-01-19 00:38:35,385 | Epoch: [1][17/48]\tTime  33.05 ( 33.40)\tData 0.0142 (0.0785)\tLoss (L1) 12.395 (11.414)\n",
      "2022-01-19 00:38:35,399 | ===> Batch : 18\n",
      "2022-01-19 00:38:35,400 | FDS disable\n",
      "2022-01-19 00:38:47,286 | Calculate Loss\n",
      "2022-01-19 00:38:47,288 | Update Loss\n",
      "2022-01-19 00:38:47,292 | Backward\n",
      "2022-01-19 00:39:08,871 | Epoch: [1][18/48]\tTime  33.49 ( 33.40)\tData 0.0141 (0.0750)\tLoss (L1) 11.929 (11.442)\n",
      "2022-01-19 00:39:08,885 | ===> Batch : 19\n",
      "2022-01-19 00:39:08,885 | FDS disable\n",
      "2022-01-19 00:39:20,682 | Calculate Loss\n",
      "2022-01-19 00:39:20,683 | Update Loss\n",
      "2022-01-19 00:39:20,687 | Backward\n",
      "2022-01-19 00:39:41,575 | Epoch: [1][19/48]\tTime  32.70 ( 33.37)\tData 0.0139 (0.0718)\tLoss (L1) 11.607 (11.451)\n",
      "2022-01-19 00:39:41,596 | ===> Batch : 20\n",
      "2022-01-19 00:39:41,597 | FDS disable\n",
      "2022-01-19 00:39:53,470 | Calculate Loss\n",
      "2022-01-19 00:39:53,472 | Update Loss\n",
      "2022-01-19 00:39:53,475 | Backward\n",
      "2022-01-19 00:40:14,921 | Epoch: [1][20/48]\tTime  33.35 ( 33.37)\tData 0.0209 (0.0692)\tLoss (L1) 11.627 (11.460)\n",
      "2022-01-19 00:40:14,935 | ===> Batch : 21\n",
      "2022-01-19 00:40:14,935 | FDS disable\n",
      "2022-01-19 00:40:26,817 | Calculate Loss\n",
      "2022-01-19 00:40:26,818 | Update Loss\n",
      "2022-01-19 00:40:26,822 | Backward\n",
      "2022-01-19 00:40:47,384 | Epoch: [1][21/48]\tTime  32.46 ( 33.32)\tData 0.0140 (0.0666)\tLoss (L1) 12.275 (11.499)\n",
      "2022-01-19 00:40:47,399 | ===> Batch : 22\n",
      "2022-01-19 00:40:47,400 | FDS disable\n",
      "2022-01-19 00:40:59,207 | Calculate Loss\n",
      "2022-01-19 00:40:59,208 | Update Loss\n",
      "2022-01-19 00:40:59,212 | Backward\n",
      "2022-01-19 00:41:20,802 | Epoch: [1][22/48]\tTime  33.42 ( 33.33)\tData 0.0155 (0.0643)\tLoss (L1) 10.635 (11.459)\n",
      "2022-01-19 00:41:20,818 | ===> Batch : 23\n",
      "2022-01-19 00:41:20,818 | FDS disable\n",
      "2022-01-19 00:41:32,830 | Calculate Loss\n",
      "2022-01-19 00:41:32,831 | Update Loss\n",
      "2022-01-19 00:41:32,835 | Backward\n",
      "2022-01-19 00:41:54,268 | Epoch: [1][23/48]\tTime  33.47 ( 33.33)\tData 0.0159 (0.0622)\tLoss (L1) 11.239 (11.450)\n",
      "2022-01-19 00:41:54,282 | ===> Batch : 24\n",
      "2022-01-19 00:41:54,283 | FDS disable\n",
      "2022-01-19 00:42:06,152 | Calculate Loss\n",
      "2022-01-19 00:42:06,154 | Update Loss\n",
      "2022-01-19 00:42:06,158 | Backward\n",
      "2022-01-19 00:42:27,036 | Epoch: [1][24/48]\tTime  32.77 ( 33.31)\tData 0.0144 (0.0602)\tLoss (L1) 11.235 (11.441)\n",
      "2022-01-19 00:42:27,050 | ===> Batch : 25\n",
      "2022-01-19 00:42:27,050 | FDS disable\n",
      "2022-01-19 00:42:38,850 | Calculate Loss\n",
      "2022-01-19 00:42:38,851 | Update Loss\n",
      "2022-01-19 00:42:38,854 | Backward\n",
      "2022-01-19 00:42:59,860 | Epoch: [1][25/48]\tTime  32.82 ( 33.29)\tData 0.0140 (0.0583)\tLoss (L1) 11.887 (11.459)\n",
      "2022-01-19 00:42:59,873 | ===> Batch : 26\n",
      "2022-01-19 00:42:59,874 | FDS disable\n",
      "2022-01-19 00:43:11,701 | Calculate Loss\n",
      "2022-01-19 00:43:11,703 | Update Loss\n",
      "2022-01-19 00:43:11,706 | Backward\n",
      "2022-01-19 00:43:32,857 | Epoch: [1][26/48]\tTime  33.00 ( 33.28)\tData 0.0136 (0.0566)\tLoss (L1) 11.008 (11.441)\n",
      "2022-01-19 00:43:32,871 | ===> Batch : 27\n",
      "2022-01-19 00:43:32,871 | FDS disable\n",
      "2022-01-19 00:43:44,645 | Calculate Loss\n",
      "2022-01-19 00:43:44,647 | Update Loss\n",
      "2022-01-19 00:43:44,650 | Backward\n",
      "2022-01-19 00:44:05,500 | Epoch: [1][27/48]\tTime  32.64 ( 33.26)\tData 0.0137 (0.0550)\tLoss (L1) 11.967 (11.461)\n",
      "2022-01-19 00:44:05,521 | ===> Batch : 28\n",
      "2022-01-19 00:44:05,522 | FDS disable\n",
      "2022-01-19 00:44:17,342 | Calculate Loss\n",
      "2022-01-19 00:44:17,343 | Update Loss\n",
      "2022-01-19 00:44:17,347 | Backward\n",
      "2022-01-19 00:44:38,894 | Epoch: [1][28/48]\tTime  33.39 ( 33.26)\tData 0.0218 (0.0538)\tLoss (L1) 12.183 (11.487)\n",
      "2022-01-19 00:44:38,908 | ===> Batch : 29\n",
      "2022-01-19 00:44:38,909 | FDS disable\n",
      "2022-01-19 00:44:50,750 | Calculate Loss\n",
      "2022-01-19 00:44:50,751 | Update Loss\n",
      "2022-01-19 00:44:50,755 | Backward\n",
      "2022-01-19 00:45:11,646 | Epoch: [1][29/48]\tTime  32.75 ( 33.24)\tData 0.0139 (0.0524)\tLoss (L1) 11.950 (11.502)\n",
      "2022-01-19 00:45:11,662 | ===> Batch : 30\n",
      "2022-01-19 00:45:11,663 | FDS disable\n",
      "2022-01-19 00:45:23,541 | Calculate Loss\n",
      "2022-01-19 00:45:23,542 | Update Loss\n",
      "2022-01-19 00:45:23,546 | Backward\n",
      "2022-01-19 00:45:45,087 | Epoch: [1][30/48]\tTime  33.44 ( 33.25)\tData 0.0157 (0.0512)\tLoss (L1) 10.388 (11.465)\n",
      "2022-01-19 00:45:45,101 | ===> Batch : 31\n",
      "2022-01-19 00:45:45,102 | FDS disable\n",
      "2022-01-19 00:45:56,951 | Calculate Loss\n",
      "2022-01-19 00:45:56,952 | Update Loss\n",
      "2022-01-19 00:45:56,956 | Backward\n",
      "2022-01-19 00:46:18,105 | Epoch: [1][31/48]\tTime  33.02 ( 33.24)\tData 0.0143 (0.0500)\tLoss (L1) 11.362 (11.462)\n",
      "2022-01-19 00:46:18,119 | ===> Batch : 32\n",
      "2022-01-19 00:46:18,120 | FDS disable\n",
      "2022-01-19 00:46:29,908 | Calculate Loss\n",
      "2022-01-19 00:46:29,909 | Update Loss\n",
      "2022-01-19 00:46:29,913 | Backward\n",
      "2022-01-19 00:46:51,507 | Epoch: [1][32/48]\tTime  33.40 ( 33.25)\tData 0.0147 (0.0489)\tLoss (L1) 10.233 (11.424)\n",
      "2022-01-19 00:46:51,522 | ===> Batch : 33\n",
      "2022-01-19 00:46:51,522 | FDS disable\n",
      "2022-01-19 00:47:03,253 | Calculate Loss\n",
      "2022-01-19 00:47:03,254 | Update Loss\n",
      "2022-01-19 00:47:03,258 | Backward\n",
      "2022-01-19 00:47:23,969 | Epoch: [1][33/48]\tTime  32.46 ( 33.22)\tData 0.0147 (0.0479)\tLoss (L1) 10.273 (11.389)\n",
      "2022-01-19 00:47:23,984 | ===> Batch : 34\n",
      "2022-01-19 00:47:23,985 | FDS disable\n",
      "2022-01-19 00:47:35,650 | Calculate Loss\n",
      "2022-01-19 00:47:35,652 | Update Loss\n",
      "2022-01-19 00:47:35,655 | Backward\n",
      "2022-01-19 00:47:57,133 | Epoch: [1][34/48]\tTime  33.16 ( 33.22)\tData 0.0148 (0.0469)\tLoss (L1) 12.172 (11.412)\n",
      "2022-01-19 00:47:57,147 | ===> Batch : 35\n",
      "2022-01-19 00:47:57,147 | FDS disable\n",
      "2022-01-19 00:48:08,872 | Calculate Loss\n",
      "2022-01-19 00:48:08,873 | Update Loss\n",
      "2022-01-19 00:48:08,877 | Backward\n",
      "2022-01-19 00:48:29,736 | Epoch: [1][35/48]\tTime  32.60 ( 33.20)\tData 0.0135 (0.0460)\tLoss (L1) 11.604 (11.417)\n",
      "2022-01-19 00:48:29,751 | ===> Batch : 36\n",
      "2022-01-19 00:48:29,751 | FDS disable\n",
      "2022-01-19 00:48:41,429 | Calculate Loss\n",
      "2022-01-19 00:48:41,431 | Update Loss\n",
      "2022-01-19 00:48:41,434 | Backward\n",
      "2022-01-19 00:49:02,970 | Epoch: [1][36/48]\tTime  33.23 ( 33.20)\tData 0.0143 (0.0451)\tLoss (L1) 11.771 (11.427)\n",
      "2022-01-19 00:49:02,986 | ===> Batch : 37\n",
      "2022-01-19 00:49:02,987 | FDS disable\n",
      "2022-01-19 00:49:14,687 | Calculate Loss\n",
      "2022-01-19 00:49:14,688 | Update Loss\n",
      "2022-01-19 00:49:14,692 | Backward\n",
      "2022-01-19 00:49:35,771 | Epoch: [1][37/48]\tTime  32.80 ( 33.19)\tData 0.0159 (0.0443)\tLoss (L1) 10.849 (11.411)\n",
      "2022-01-19 00:49:35,785 | ===> Batch : 38\n",
      "2022-01-19 00:49:35,785 | FDS disable\n",
      "2022-01-19 00:49:47,499 | Calculate Loss\n",
      "2022-01-19 00:49:47,500 | Update Loss\n",
      "2022-01-19 00:49:47,504 | Backward\n",
      "2022-01-19 00:50:08,542 | Epoch: [1][38/48]\tTime  32.77 ( 33.18)\tData 0.0139 (0.0435)\tLoss (L1) 11.476 (11.413)\n",
      "2022-01-19 00:50:08,557 | ===> Batch : 39\n",
      "2022-01-19 00:50:08,558 | FDS disable\n",
      "2022-01-19 00:50:20,282 | Calculate Loss\n",
      "2022-01-19 00:50:20,283 | Update Loss\n",
      "2022-01-19 00:50:20,287 | Backward\n",
      "2022-01-19 00:50:41,899 | Epoch: [1][39/48]\tTime  33.36 ( 33.19)\tData 0.0155 (0.0428)\tLoss (L1) 11.546 (11.417)\n",
      "2022-01-19 00:50:41,913 | ===> Batch : 40\n",
      "2022-01-19 00:50:41,914 | FDS disable\n",
      "2022-01-19 00:50:53,636 | Calculate Loss\n",
      "2022-01-19 00:50:53,638 | Update Loss\n",
      "2022-01-19 00:50:53,641 | Backward\n",
      "2022-01-19 00:51:15,063 | Epoch: [1][40/48]\tTime  33.16 ( 33.19)\tData 0.0144 (0.0421)\tLoss (L1) 10.814 (11.401)\n",
      "2022-01-19 00:51:15,077 | ===> Batch : 41\n",
      "2022-01-19 00:51:15,077 | FDS disable\n",
      "2022-01-19 00:51:26,763 | Calculate Loss\n",
      "2022-01-19 00:51:26,764 | Update Loss\n",
      "2022-01-19 00:51:26,768 | Backward\n",
      "2022-01-19 00:51:47,497 | Epoch: [1][41/48]\tTime  32.43 ( 33.17)\tData 0.0142 (0.0414)\tLoss (L1) 11.130 (11.395)\n",
      "2022-01-19 00:51:47,511 | ===> Batch : 42\n",
      "2022-01-19 00:51:47,511 | FDS disable\n",
      "2022-01-19 00:51:59,217 | Calculate Loss\n",
      "2022-01-19 00:51:59,218 | Update Loss\n",
      "2022-01-19 00:51:59,222 | Backward\n",
      "2022-01-19 00:52:20,777 | Epoch: [1][42/48]\tTime  33.28 ( 33.17)\tData 0.0138 (0.0407)\tLoss (L1) 11.815 (11.405)\n",
      "2022-01-19 00:52:20,792 | ===> Batch : 43\n",
      "2022-01-19 00:52:20,792 | FDS disable\n",
      "2022-01-19 00:52:32,463 | Calculate Loss\n",
      "2022-01-19 00:52:32,465 | Update Loss\n",
      "2022-01-19 00:52:32,468 | Backward\n",
      "2022-01-19 00:52:53,571 | Epoch: [1][43/48]\tTime  32.79 ( 33.16)\tData 0.0145 (0.0401)\tLoss (L1) 10.996 (11.395)\n",
      "2022-01-19 00:52:53,585 | ===> Batch : 44\n",
      "2022-01-19 00:52:53,586 | FDS disable\n",
      "2022-01-19 00:53:05,268 | Calculate Loss\n",
      "2022-01-19 00:53:05,270 | Update Loss\n",
      "2022-01-19 00:53:05,273 | Backward\n",
      "2022-01-19 00:53:26,817 | Epoch: [1][44/48]\tTime  33.25 ( 33.16)\tData 0.0146 (0.0395)\tLoss (L1) 11.564 (11.399)\n",
      "2022-01-19 00:53:26,832 | ===> Batch : 45\n",
      "2022-01-19 00:53:26,832 | FDS disable\n",
      "2022-01-19 00:53:38,603 | Calculate Loss\n",
      "2022-01-19 00:53:38,605 | Update Loss\n",
      "2022-01-19 00:53:38,608 | Backward\n",
      "2022-01-19 00:54:00,020 | Epoch: [1][45/48]\tTime  33.20 ( 33.16)\tData 0.0141 (0.0390)\tLoss (L1) 11.455 (11.400)\n",
      "2022-01-19 00:54:00,035 | ===> Batch : 46\n",
      "2022-01-19 00:54:00,035 | FDS disable\n",
      "2022-01-19 00:54:11,778 | Calculate Loss\n",
      "2022-01-19 00:54:11,779 | Update Loss\n",
      "2022-01-19 00:54:11,783 | Backward\n",
      "2022-01-19 00:54:32,956 | Epoch: [1][46/48]\tTime  32.94 ( 33.16)\tData 0.0152 (0.0385)\tLoss (L1) 12.618 (11.427)\n",
      "2022-01-19 00:54:32,970 | ===> Batch : 47\n",
      "2022-01-19 00:54:32,970 | FDS disable\n",
      "2022-01-19 00:54:44,645 | Calculate Loss\n",
      "2022-01-19 00:54:44,646 | Update Loss\n",
      "2022-01-19 00:54:44,650 | Backward\n",
      "2022-01-19 00:55:06,215 | Epoch: [1][47/48]\tTime  33.26 ( 33.16)\tData 0.0138 (0.0379)\tLoss (L1) 11.822 (11.435)\n",
      "2022-01-19 00:55:06,232 | ===> Batch : 48\n",
      "2022-01-19 00:55:06,233 | FDS disable\n",
      "2022-01-19 00:55:14,867 | Calculate Loss\n",
      "2022-01-19 00:55:14,868 | Update Loss\n",
      "2022-01-19 00:55:14,873 | Backward\n",
      "2022-01-19 00:55:29,436 | Epoch: [1][48/48]\tTime  23.22 ( 32.96)\tData 0.0169 (0.0375)\tLoss (L1) 11.290 (11.433)\n",
      "2022-01-19 00:55:41,324 | Val: [0/9]\tTime 11.731 (11.731)\tLoss (MSE) 294.599 (294.599)\tLoss (L1) 13.897 (13.897)\n",
      "2022-01-19 00:55:50,930 | Val: [1/9]\tTime  9.607 (10.669)\tLoss (MSE) 277.989 (286.294)\tLoss (L1) 13.845 (13.871)\n",
      "2022-01-19 00:56:00,496 | Val: [2/9]\tTime  9.565 (10.301)\tLoss (MSE) 344.997 (305.862)\tLoss (L1) 15.084 (14.275)\n",
      "2022-01-19 00:56:10,096 | Val: [3/9]\tTime  9.601 (10.126)\tLoss (MSE) 299.746 (304.333)\tLoss (L1) 13.896 (14.180)\n",
      "2022-01-19 00:56:19,671 | Val: [4/9]\tTime  9.574 (10.015)\tLoss (MSE) 318.488 (307.164)\tLoss (L1) 14.745 (14.293)\n",
      "2022-01-19 00:56:29,295 | Val: [5/9]\tTime  9.625 ( 9.950)\tLoss (MSE) 319.236 (309.176)\tLoss (L1) 14.679 (14.358)\n",
      "2022-01-19 00:56:38,860 | Val: [6/9]\tTime  9.565 ( 9.895)\tLoss (MSE) 291.393 (306.635)\tLoss (L1) 13.755 (14.272)\n",
      "2022-01-19 00:56:48,427 | Val: [7/9]\tTime  9.566 ( 9.854)\tLoss (MSE) 343.860 (311.289)\tLoss (L1) 14.722 (14.328)\n",
      "2022-01-19 00:56:51,653 | Val: [8/9]\tTime  3.226 ( 9.118)\tLoss (MSE) 302.580 (310.914)\tLoss (L1) 13.577 (14.296)\n",
      "2022-01-19 00:56:51,779 |  * Overall: MSE 310.914\tL1 14.296\tG-Mean 9.705\n",
      "2022-01-19 00:56:51,780 |  * Many: MSE 203.193\tL1 11.347\tG-Mean 7.544\n",
      "2022-01-19 00:56:51,781 |  * Median: MSE 483.494\tL1 19.574\tG-Mean 16.076\n",
      "2022-01-19 00:56:51,781 |  * Low: MSE 851.024\tL1 27.546\tG-Mean 25.938\n",
      "2022-01-19 00:56:51,786 | Best L1 Loss: 14.296\n",
      "2022-01-19 00:56:53,043 | ===> Saving current best checkpoint...\n",
      "2022-01-19 00:56:54,187 | Epoch #1: Train loss [11.4332]; Val loss: MSE [310.9141], L1 [14.2956], G-Mean [9.7045]\n",
      "2022-01-19 00:56:54,188 | Training...\n",
      "2022-01-19 00:56:54,189 | Load train loader\n",
      "2022-01-19 00:56:55,417 | ===> Batch : 1\n",
      "2022-01-19 00:56:55,418 | FDS disable\n",
      "2022-01-19 00:57:09,501 | Calculate Loss\n",
      "2022-01-19 00:57:09,503 | Update Loss\n",
      "2022-01-19 00:57:09,628 | Backward\n",
      "2022-01-19 00:57:31,641 | Epoch: [2][ 1/48]\tTime  37.45 ( 37.45)\tData 1.2274 (1.2274)\tLoss (L1) 11.569 (11.569)\n",
      "2022-01-19 00:57:31,657 | ===> Batch : 2\n",
      "2022-01-19 00:57:31,658 | FDS disable\n",
      "2022-01-19 00:57:43,462 | Calculate Loss\n",
      "2022-01-19 00:57:43,463 | Update Loss\n",
      "2022-01-19 00:57:43,467 | Backward\n",
      "2022-01-19 00:58:04,549 | Epoch: [2][ 2/48]\tTime  32.91 ( 35.18)\tData 0.0159 (0.6217)\tLoss (L1) 11.731 (11.650)\n",
      "2022-01-19 00:58:04,562 | ===> Batch : 3\n",
      "2022-01-19 00:58:04,563 | FDS disable\n",
      "2022-01-19 00:58:16,352 | Calculate Loss\n",
      "2022-01-19 00:58:16,353 | Update Loss\n",
      "2022-01-19 00:58:16,357 | Backward\n",
      "2022-01-19 00:58:37,590 | Epoch: [2][ 3/48]\tTime  33.04 ( 34.47)\tData 0.0130 (0.4188)\tLoss (L1) 12.011 (11.770)\n",
      "2022-01-19 00:58:37,605 | ===> Batch : 4\n",
      "2022-01-19 00:58:37,606 | FDS disable\n",
      "2022-01-19 00:58:49,463 | Calculate Loss\n",
      "2022-01-19 00:58:49,466 | Update Loss\n",
      "2022-01-19 00:58:49,470 | Backward\n",
      "2022-01-19 00:59:10,792 | Epoch: [2][ 4/48]\tTime  33.20 ( 34.15)\tData 0.0151 (0.3179)\tLoss (L1) 10.947 (11.564)\n",
      "2022-01-19 00:59:10,805 | ===> Batch : 5\n",
      "2022-01-19 00:59:10,808 | FDS disable\n",
      "2022-01-19 00:59:22,665 | Calculate Loss\n",
      "2022-01-19 00:59:22,667 | Update Loss\n",
      "2022-01-19 00:59:22,670 | Backward\n",
      "2022-01-19 00:59:43,505 | Epoch: [2][ 5/48]\tTime  32.71 ( 33.86)\tData 0.0133 (0.2569)\tLoss (L1) 10.369 (11.325)\n",
      "2022-01-19 00:59:43,520 | ===> Batch : 6\n",
      "2022-01-19 00:59:43,520 | FDS disable\n",
      "2022-01-19 00:59:55,383 | Calculate Loss\n",
      "2022-01-19 00:59:55,384 | Update Loss\n",
      "2022-01-19 00:59:55,388 | Backward\n",
      "2022-01-19 01:00:16,385 | Epoch: [2][ 6/48]\tTime  32.88 ( 33.70)\tData 0.0146 (0.2166)\tLoss (L1) 10.794 (11.237)\n",
      "2022-01-19 01:00:16,399 | ===> Batch : 7\n",
      "2022-01-19 01:00:16,399 | FDS disable\n",
      "2022-01-19 01:00:28,189 | Calculate Loss\n",
      "2022-01-19 01:00:28,190 | Update Loss\n",
      "2022-01-19 01:00:28,194 | Backward\n",
      "2022-01-19 01:00:49,695 | Epoch: [2][ 7/48]\tTime  33.31 ( 33.64)\tData 0.0132 (0.1875)\tLoss (L1) 10.560 (11.140)\n",
      "2022-01-19 01:00:49,707 | ===> Batch : 8\n",
      "2022-01-19 01:00:49,708 | FDS disable\n",
      "2022-01-19 01:01:01,496 | Calculate Loss\n",
      "2022-01-19 01:01:01,497 | Update Loss\n",
      "2022-01-19 01:01:01,501 | Backward\n",
      "2022-01-19 01:01:23,145 | Epoch: [2][ 8/48]\tTime  33.45 ( 33.62)\tData 0.0128 (0.1657)\tLoss (L1) 10.631 (11.076)\n",
      "2022-01-19 01:01:23,163 | ===> Batch : 9\n",
      "2022-01-19 01:01:23,163 | FDS disable\n",
      "2022-01-19 01:01:34,944 | Calculate Loss\n",
      "2022-01-19 01:01:34,945 | Update Loss\n",
      "2022-01-19 01:01:34,949 | Backward\n",
      "2022-01-19 01:01:56,354 | Epoch: [2][ 9/48]\tTime  33.21 ( 33.57)\tData 0.0177 (0.1492)\tLoss (L1) 10.741 (11.039)\n",
      "2022-01-19 01:01:56,370 | ===> Batch : 10\n",
      "2022-01-19 01:01:56,371 | FDS disable\n",
      "2022-01-19 01:02:08,187 | Calculate Loss\n",
      "2022-01-19 01:02:08,188 | Update Loss\n",
      "2022-01-19 01:02:08,192 | Backward\n",
      "2022-01-19 01:02:29,901 | Epoch: [2][10/48]\tTime  33.55 ( 33.57)\tData 0.0159 (0.1359)\tLoss (L1) 11.249 (11.060)\n",
      "2022-01-19 01:02:29,914 | ===> Batch : 11\n",
      "2022-01-19 01:02:29,915 | FDS disable\n",
      "2022-01-19 01:02:41,750 | Calculate Loss\n",
      "2022-01-19 01:02:41,751 | Update Loss\n",
      "2022-01-19 01:02:41,755 | Backward\n",
      "2022-01-19 01:03:02,916 | Epoch: [2][11/48]\tTime  33.01 ( 33.52)\tData 0.0131 (0.1247)\tLoss (L1) 11.155 (11.069)\n",
      "2022-01-19 01:03:02,931 | ===> Batch : 12\n",
      "2022-01-19 01:03:02,931 | FDS disable\n",
      "2022-01-19 01:03:14,732 | Calculate Loss\n",
      "2022-01-19 01:03:14,733 | Update Loss\n",
      "2022-01-19 01:03:14,737 | Backward\n",
      "2022-01-19 01:03:35,829 | Epoch: [2][12/48]\tTime  32.91 ( 33.47)\tData 0.0151 (0.1156)\tLoss (L1) 11.608 (11.114)\n",
      "2022-01-19 01:03:35,842 | ===> Batch : 13\n",
      "2022-01-19 01:03:35,843 | FDS disable\n",
      "2022-01-19 01:03:47,667 | Calculate Loss\n",
      "2022-01-19 01:03:47,668 | Update Loss\n",
      "2022-01-19 01:03:47,672 | Backward\n",
      "2022-01-19 01:04:08,740 | Epoch: [2][13/48]\tTime  32.91 ( 33.43)\tData 0.0135 (0.1077)\tLoss (L1) 11.391 (11.135)\n",
      "2022-01-19 01:04:08,753 | ===> Batch : 14\n",
      "2022-01-19 01:04:08,754 | FDS disable\n",
      "2022-01-19 01:04:20,561 | Calculate Loss\n",
      "2022-01-19 01:04:20,562 | Update Loss\n",
      "2022-01-19 01:04:20,566 | Backward\n",
      "2022-01-19 01:04:42,147 | Epoch: [2][14/48]\tTime  33.41 ( 33.43)\tData 0.0130 (0.1010)\tLoss (L1) 11.452 (11.158)\n",
      "2022-01-19 01:04:42,160 | ===> Batch : 15\n",
      "2022-01-19 01:04:42,161 | FDS disable\n",
      "2022-01-19 01:04:53,966 | Calculate Loss\n",
      "2022-01-19 01:04:53,967 | Update Loss\n",
      "2022-01-19 01:04:53,971 | Backward\n",
      "2022-01-19 01:05:15,400 | Epoch: [2][15/48]\tTime  33.25 ( 33.41)\tData 0.0126 (0.0951)\tLoss (L1) 10.969 (11.145)\n",
      "2022-01-19 01:05:15,413 | ===> Batch : 16\n",
      "2022-01-19 01:05:15,413 | FDS disable\n",
      "2022-01-19 01:05:27,254 | Calculate Loss\n",
      "2022-01-19 01:05:27,255 | Update Loss\n",
      "2022-01-19 01:05:27,260 | Backward\n",
      "2022-01-19 01:05:48,426 | Epoch: [2][16/48]\tTime  33.03 ( 33.39)\tData 0.0125 (0.0899)\tLoss (L1) 11.693 (11.179)\n",
      "2022-01-19 01:05:48,440 | ===> Batch : 17\n",
      "2022-01-19 01:05:48,441 | FDS disable\n",
      "2022-01-19 01:06:00,242 | Calculate Loss\n",
      "2022-01-19 01:06:00,243 | Update Loss\n",
      "2022-01-19 01:06:00,247 | Backward\n",
      "2022-01-19 01:06:21,933 | Epoch: [2][17/48]\tTime  33.51 ( 33.40)\tData 0.0145 (0.0855)\tLoss (L1) 10.742 (11.154)\n",
      "2022-01-19 01:06:21,946 | ===> Batch : 18\n",
      "2022-01-19 01:06:21,947 | FDS disable\n",
      "2022-01-19 01:06:33,769 | Calculate Loss\n",
      "2022-01-19 01:06:33,770 | Update Loss\n",
      "2022-01-19 01:06:33,774 | Backward\n",
      "2022-01-19 01:06:54,448 | Epoch: [2][18/48]\tTime  32.52 ( 33.35)\tData 0.0136 (0.0815)\tLoss (L1) 11.265 (11.160)\n",
      "2022-01-19 01:06:54,463 | ===> Batch : 19\n",
      "2022-01-19 01:06:54,463 | FDS disable\n",
      "2022-01-19 01:07:06,240 | Calculate Loss\n",
      "2022-01-19 01:07:06,242 | Update Loss\n",
      "2022-01-19 01:07:06,245 | Backward\n",
      "2022-01-19 01:07:27,382 | Epoch: [2][19/48]\tTime  32.93 ( 33.33)\tData 0.0146 (0.0780)\tLoss (L1) 10.874 (11.145)\n",
      "2022-01-19 01:07:27,396 | ===> Batch : 20\n",
      "2022-01-19 01:07:27,396 | FDS disable\n",
      "2022-01-19 01:07:39,203 | Calculate Loss\n",
      "2022-01-19 01:07:39,205 | Update Loss\n",
      "2022-01-19 01:07:39,209 | Backward\n",
      "2022-01-19 01:08:00,851 | Epoch: [2][20/48]\tTime  33.47 ( 33.33)\tData 0.0138 (0.0748)\tLoss (L1) 10.970 (11.136)\n",
      "2022-01-19 01:08:00,866 | ===> Batch : 21\n",
      "2022-01-19 01:08:00,866 | FDS disable\n",
      "2022-01-19 01:08:12,695 | Calculate Loss\n",
      "2022-01-19 01:08:12,696 | Update Loss\n",
      "2022-01-19 01:08:12,700 | Backward\n",
      "2022-01-19 01:08:34,305 | Epoch: [2][21/48]\tTime  33.45 ( 33.34)\tData 0.0152 (0.0719)\tLoss (L1) 10.431 (11.102)\n",
      "2022-01-19 01:08:34,320 | ===> Batch : 22\n",
      "2022-01-19 01:08:34,320 | FDS disable\n",
      "2022-01-19 01:08:46,117 | Calculate Loss\n",
      "2022-01-19 01:08:46,118 | Update Loss\n",
      "2022-01-19 01:08:46,122 | Backward\n",
      "2022-01-19 01:09:07,212 | Epoch: [2][22/48]\tTime  32.91 ( 33.32)\tData 0.0151 (0.0693)\tLoss (L1) 11.664 (11.128)\n",
      "2022-01-19 01:09:07,233 | ===> Batch : 23\n",
      "2022-01-19 01:09:07,234 | FDS disable\n",
      "2022-01-19 01:09:18,986 | Calculate Loss\n",
      "2022-01-19 01:09:18,987 | Update Loss\n",
      "2022-01-19 01:09:18,991 | Backward\n",
      "2022-01-19 01:09:39,802 | Epoch: [2][23/48]\tTime  32.59 ( 33.29)\tData 0.0217 (0.0673)\tLoss (L1) 10.961 (11.121)\n",
      "2022-01-19 01:09:39,816 | ===> Batch : 24\n",
      "2022-01-19 01:09:39,816 | FDS disable\n",
      "2022-01-19 01:09:51,639 | Calculate Loss\n",
      "2022-01-19 01:09:51,641 | Update Loss\n",
      "2022-01-19 01:09:51,644 | Backward\n",
      "2022-01-19 01:10:13,018 | Epoch: [2][24/48]\tTime  33.22 ( 33.28)\tData 0.0144 (0.0651)\tLoss (L1) 11.039 (11.117)\n",
      "2022-01-19 01:10:13,032 | ===> Batch : 25\n",
      "2022-01-19 01:10:13,032 | FDS disable\n",
      "2022-01-19 01:10:24,935 | Calculate Loss\n",
      "2022-01-19 01:10:24,936 | Update Loss\n",
      "2022-01-19 01:10:24,940 | Backward\n",
      "2022-01-19 01:10:46,563 | Epoch: [2][25/48]\tTime  33.55 ( 33.29)\tData 0.0141 (0.0630)\tLoss (L1) 11.024 (11.113)\n",
      "2022-01-19 01:10:46,577 | ===> Batch : 26\n",
      "2022-01-19 01:10:46,578 | FDS disable\n",
      "2022-01-19 01:10:58,412 | Calculate Loss\n",
      "2022-01-19 01:10:58,414 | Update Loss\n",
      "2022-01-19 01:10:58,418 | Backward\n",
      "2022-01-19 01:11:19,384 | Epoch: [2][26/48]\tTime  32.82 ( 33.28)\tData 0.0137 (0.0611)\tLoss (L1) 10.554 (11.092)\n",
      "2022-01-19 01:11:19,398 | ===> Batch : 27\n",
      "2022-01-19 01:11:19,399 | FDS disable\n",
      "2022-01-19 01:11:31,289 | Calculate Loss\n",
      "2022-01-19 01:11:31,290 | Update Loss\n",
      "2022-01-19 01:11:31,294 | Backward\n",
      "2022-01-19 01:11:52,858 | Epoch: [2][27/48]\tTime  33.47 ( 33.28)\tData 0.0146 (0.0594)\tLoss (L1) 11.164 (11.095)\n",
      "2022-01-19 01:11:52,872 | ===> Batch : 28\n",
      "2022-01-19 01:11:52,872 | FDS disable\n",
      "2022-01-19 01:12:04,808 | Calculate Loss\n",
      "2022-01-19 01:12:04,809 | Update Loss\n",
      "2022-01-19 01:12:04,813 | Backward\n",
      "2022-01-19 01:12:25,279 | Epoch: [2][28/48]\tTime  32.42 ( 33.25)\tData 0.0139 (0.0578)\tLoss (L1) 11.160 (11.097)\n",
      "2022-01-19 01:12:25,293 | ===> Batch : 29\n",
      "2022-01-19 01:12:25,294 | FDS disable\n",
      "2022-01-19 01:12:37,151 | Calculate Loss\n",
      "2022-01-19 01:12:37,152 | Update Loss\n",
      "2022-01-19 01:12:37,156 | Backward\n",
      "2022-01-19 01:12:58,873 | Epoch: [2][29/48]\tTime  33.59 ( 33.26)\tData 0.0142 (0.0563)\tLoss (L1) 11.075 (11.096)\n",
      "2022-01-19 01:12:58,888 | ===> Batch : 30\n",
      "2022-01-19 01:12:58,888 | FDS disable\n",
      "2022-01-19 01:13:10,752 | Calculate Loss\n",
      "2022-01-19 01:13:10,754 | Update Loss\n",
      "2022-01-19 01:13:10,758 | Backward\n",
      "2022-01-19 01:13:31,867 | Epoch: [2][30/48]\tTime  32.99 ( 33.26)\tData 0.0148 (0.0549)\tLoss (L1) 10.996 (11.093)\n",
      "2022-01-19 01:13:31,881 | ===> Batch : 31\n",
      "2022-01-19 01:13:31,882 | FDS disable\n",
      "2022-01-19 01:13:43,766 | Calculate Loss\n",
      "2022-01-19 01:13:43,767 | Update Loss\n",
      "2022-01-19 01:13:43,771 | Backward\n",
      "2022-01-19 01:14:05,222 | Epoch: [2][31/48]\tTime  33.36 ( 33.26)\tData 0.0145 (0.0536)\tLoss (L1) 11.045 (11.091)\n",
      "2022-01-19 01:14:05,237 | ===> Batch : 32\n",
      "2022-01-19 01:14:05,237 | FDS disable\n",
      "2022-01-19 01:14:16,988 | Calculate Loss\n",
      "2022-01-19 01:14:16,990 | Update Loss\n",
      "2022-01-19 01:14:16,994 | Backward\n",
      "2022-01-19 01:14:38,540 | Epoch: [2][32/48]\tTime  33.32 ( 33.26)\tData 0.0143 (0.0524)\tLoss (L1) 11.877 (11.116)\n",
      "2022-01-19 01:14:38,554 | ===> Batch : 33\n",
      "2022-01-19 01:14:38,554 | FDS disable\n",
      "2022-01-19 01:14:50,265 | Calculate Loss\n",
      "2022-01-19 01:14:50,266 | Update Loss\n",
      "2022-01-19 01:14:50,270 | Backward\n",
      "2022-01-19 01:15:11,793 | Epoch: [2][33/48]\tTime  33.25 ( 33.26)\tData 0.0141 (0.0512)\tLoss (L1) 11.427 (11.125)\n",
      "2022-01-19 01:15:11,807 | ===> Batch : 34\n",
      "2022-01-19 01:15:11,808 | FDS disable\n",
      "2022-01-19 01:15:23,538 | Calculate Loss\n",
      "2022-01-19 01:15:23,539 | Update Loss\n",
      "2022-01-19 01:15:23,543 | Backward\n",
      "2022-01-19 01:15:44,496 | Epoch: [2][34/48]\tTime  32.70 ( 33.24)\tData 0.0140 (0.0501)\tLoss (L1) 11.234 (11.129)\n",
      "2022-01-19 01:15:44,511 | ===> Batch : 35\n",
      "2022-01-19 01:15:44,511 | FDS disable\n",
      "2022-01-19 01:15:56,296 | Calculate Loss\n",
      "2022-01-19 01:15:56,298 | Update Loss\n",
      "2022-01-19 01:15:56,301 | Backward\n",
      "2022-01-19 01:16:17,101 | Epoch: [2][35/48]\tTime  32.61 ( 33.23)\tData 0.0148 (0.0491)\tLoss (L1) 10.932 (11.123)\n",
      "2022-01-19 01:16:17,115 | ===> Batch : 36\n",
      "2022-01-19 01:16:17,116 | FDS disable\n",
      "2022-01-19 01:16:28,891 | Calculate Loss\n",
      "2022-01-19 01:16:28,893 | Update Loss\n",
      "2022-01-19 01:16:28,897 | Backward\n",
      "2022-01-19 01:16:50,500 | Epoch: [2][36/48]\tTime  33.40 ( 33.23)\tData 0.0141 (0.0481)\tLoss (L1) 12.288 (11.155)\n",
      "2022-01-19 01:16:50,515 | ===> Batch : 37\n",
      "2022-01-19 01:16:50,516 | FDS disable\n",
      "2022-01-19 01:17:02,257 | Calculate Loss\n",
      "2022-01-19 01:17:02,258 | Update Loss\n",
      "2022-01-19 01:17:02,262 | Backward\n",
      "2022-01-19 01:17:23,767 | Epoch: [2][37/48]\tTime  33.27 ( 33.23)\tData 0.0148 (0.0472)\tLoss (L1) 10.590 (11.140)\n",
      "2022-01-19 01:17:23,783 | ===> Batch : 38\n",
      "2022-01-19 01:17:23,783 | FDS disable\n",
      "2022-01-19 01:17:35,463 | Calculate Loss\n",
      "2022-01-19 01:17:35,465 | Update Loss\n",
      "2022-01-19 01:17:35,469 | Backward\n",
      "2022-01-19 01:17:56,278 | Epoch: [2][38/48]\tTime  32.51 ( 33.21)\tData 0.0152 (0.0464)\tLoss (L1) 10.642 (11.127)\n",
      "2022-01-19 01:17:56,292 | ===> Batch : 39\n",
      "2022-01-19 01:17:56,293 | FDS disable\n",
      "2022-01-19 01:18:07,971 | Calculate Loss\n",
      "2022-01-19 01:18:07,972 | Update Loss\n",
      "2022-01-19 01:18:07,976 | Backward\n",
      "2022-01-19 01:18:29,099 | Epoch: [2][39/48]\tTime  32.82 ( 33.20)\tData 0.0139 (0.0456)\tLoss (L1) 11.070 (11.125)\n",
      "2022-01-19 01:18:29,114 | ===> Batch : 40\n",
      "2022-01-19 01:18:29,114 | FDS disable\n",
      "2022-01-19 01:18:40,771 | Calculate Loss\n",
      "2022-01-19 01:18:40,773 | Update Loss\n",
      "2022-01-19 01:18:40,776 | Backward\n",
      "2022-01-19 01:19:02,355 | Epoch: [2][40/48]\tTime  33.26 ( 33.20)\tData 0.0144 (0.0448)\tLoss (L1) 10.803 (11.117)\n",
      "2022-01-19 01:19:02,369 | ===> Batch : 41\n",
      "2022-01-19 01:19:02,370 | FDS disable\n",
      "2022-01-19 01:19:14,078 | Calculate Loss\n",
      "2022-01-19 01:19:14,079 | Update Loss\n",
      "2022-01-19 01:19:14,083 | Backward\n",
      "2022-01-19 01:19:34,760 | Epoch: [2][41/48]\tTime  32.41 ( 33.18)\tData 0.0149 (0.0440)\tLoss (L1) 10.549 (11.103)\n",
      "2022-01-19 01:19:34,775 | ===> Batch : 42\n",
      "2022-01-19 01:19:34,775 | FDS disable\n",
      "2022-01-19 01:19:46,501 | Calculate Loss\n",
      "2022-01-19 01:19:46,502 | Update Loss\n",
      "2022-01-19 01:19:46,506 | Backward\n",
      "2022-01-19 01:20:08,119 | Epoch: [2][42/48]\tTime  33.36 ( 33.19)\tData 0.0142 (0.0433)\tLoss (L1) 10.516 (11.089)\n",
      "2022-01-19 01:20:08,134 | ===> Batch : 43\n",
      "2022-01-19 01:20:08,135 | FDS disable\n",
      "2022-01-19 01:20:19,866 | Calculate Loss\n",
      "2022-01-19 01:20:19,867 | Update Loss\n",
      "2022-01-19 01:20:19,871 | Backward\n",
      "2022-01-19 01:20:41,402 | Epoch: [2][43/48]\tTime  33.28 ( 33.19)\tData 0.0147 (0.0427)\tLoss (L1) 11.588 (11.101)\n",
      "2022-01-19 01:20:41,417 | ===> Batch : 44\n",
      "2022-01-19 01:20:41,417 | FDS disable\n",
      "2022-01-19 01:20:53,142 | Calculate Loss\n",
      "2022-01-19 01:20:53,143 | Update Loss\n",
      "2022-01-19 01:20:53,147 | Backward\n",
      "2022-01-19 01:21:14,534 | Epoch: [2][44/48]\tTime  33.13 ( 33.19)\tData 0.0145 (0.0420)\tLoss (L1) 11.603 (11.113)\n",
      "2022-01-19 01:21:14,548 | ===> Batch : 45\n",
      "2022-01-19 01:21:14,549 | FDS disable\n",
      "2022-01-19 01:21:26,236 | Calculate Loss\n",
      "2022-01-19 01:21:26,240 | Update Loss\n",
      "2022-01-19 01:21:26,244 | Backward\n",
      "2022-01-19 01:21:46,825 | Epoch: [2][45/48]\tTime  32.29 ( 33.17)\tData 0.0144 (0.0414)\tLoss (L1) 11.946 (11.131)\n",
      "2022-01-19 01:21:46,839 | ===> Batch : 46\n",
      "2022-01-19 01:21:46,840 | FDS disable\n",
      "2022-01-19 01:21:58,529 | Calculate Loss\n",
      "2022-01-19 01:21:58,531 | Update Loss\n",
      "2022-01-19 01:21:58,535 | Backward\n",
      "2022-01-19 01:22:20,199 | Epoch: [2][46/48]\tTime  33.37 ( 33.17)\tData 0.0144 (0.0408)\tLoss (L1) 11.889 (11.147)\n",
      "2022-01-19 01:22:20,214 | ===> Batch : 47\n",
      "2022-01-19 01:22:20,215 | FDS disable\n",
      "2022-01-19 01:22:31,913 | Calculate Loss\n",
      "2022-01-19 01:22:31,914 | Update Loss\n",
      "2022-01-19 01:22:31,918 | Backward\n",
      "2022-01-19 01:22:52,256 | Epoch: [2][47/48]\tTime  32.06 ( 33.15)\tData 0.0152 (0.0403)\tLoss (L1) 11.684 (11.159)\n",
      "2022-01-19 01:22:52,271 | ===> Batch : 48\n",
      "2022-01-19 01:22:52,272 | FDS disable\n",
      "2022-01-19 01:23:00,955 | Calculate Loss\n",
      "2022-01-19 01:23:00,956 | Update Loss\n",
      "2022-01-19 01:23:00,960 | Backward\n",
      "2022-01-19 01:23:15,679 | Epoch: [2][48/48]\tTime  23.42 ( 32.95)\tData 0.0150 (0.0398)\tLoss (L1) 10.950 (11.156)\n",
      "2022-01-19 01:23:27,748 | Val: [0/9]\tTime 11.911 (11.911)\tLoss (MSE) 339.608 (339.608)\tLoss (L1) 15.308 (15.308)\n",
      "2022-01-19 01:23:37,572 | Val: [1/9]\tTime  9.824 (10.867)\tLoss (MSE) 303.940 (321.774)\tLoss (L1) 14.309 (14.808)\n",
      "2022-01-19 01:23:47,338 | Val: [2/9]\tTime  9.767 (10.501)\tLoss (MSE) 299.152 (314.233)\tLoss (L1) 14.085 (14.567)\n",
      "2022-01-19 01:23:57,219 | Val: [3/9]\tTime  9.881 (10.346)\tLoss (MSE) 289.980 (308.170)\tLoss (L1) 14.220 (14.480)\n",
      "2022-01-19 01:24:07,046 | Val: [4/9]\tTime  9.827 (10.242)\tLoss (MSE) 341.270 (314.790)\tLoss (L1) 15.646 (14.714)\n",
      "2022-01-19 01:24:16,897 | Val: [5/9]\tTime  9.852 (10.177)\tLoss (MSE) 315.122 (314.845)\tLoss (L1) 14.709 (14.713)\n",
      "2022-01-19 01:24:26,791 | Val: [6/9]\tTime  9.894 (10.136)\tLoss (MSE) 287.393 (310.924)\tLoss (L1) 14.118 (14.628)\n",
      "2022-01-19 01:24:36,671 | Val: [7/9]\tTime  9.880 (10.104)\tLoss (MSE) 348.709 (315.647)\tLoss (L1) 15.018 (14.677)\n",
      "2022-01-19 01:24:40,071 | Val: [8/9]\tTime  3.400 ( 9.359)\tLoss (MSE) 283.458 (314.263)\tLoss (L1) 13.537 (14.628)\n",
      "2022-01-19 01:24:40,189 |  * Overall: MSE 314.263\tL1 14.628\tG-Mean 10.203\n",
      "2022-01-19 01:24:40,190 |  * Many: MSE 230.991\tL1 12.537\tG-Mean 8.744\n",
      "2022-01-19 01:24:40,190 |  * Median: MSE 422.313\tL1 17.583\tG-Mean 12.790\n",
      "2022-01-19 01:24:40,191 |  * Low: MSE 801.914\tL1 26.200\tG-Mean 23.458\n",
      "2022-01-19 01:24:40,197 | Best L1 Loss: 14.296\n",
      "2022-01-19 01:24:41,457 | Epoch #2: Train loss [11.1559]; Val loss: MSE [314.2629], L1 [14.6277], G-Mean [10.2030]\n",
      "2022-01-19 01:24:41,458 | Training...\n",
      "2022-01-19 01:24:41,460 | Load train loader\n",
      "2022-01-19 01:24:42,737 | ===> Batch : 1\n",
      "2022-01-19 01:24:42,738 | FDS disable\n",
      "2022-01-19 01:24:56,935 | Calculate Loss\n",
      "2022-01-19 01:24:56,937 | Update Loss\n",
      "2022-01-19 01:24:57,068 | Backward\n",
      "2022-01-19 01:25:19,142 | Epoch: [3][ 1/48]\tTime  37.68 ( 37.68)\tData 1.2770 (1.2770)\tLoss (L1) 11.899 (11.899)\n",
      "2022-01-19 01:25:19,161 | ===> Batch : 2\n",
      "2022-01-19 01:25:19,161 | FDS disable\n",
      "2022-01-19 01:25:30,979 | Calculate Loss\n",
      "2022-01-19 01:25:30,981 | Update Loss\n",
      "2022-01-19 01:25:30,984 | Backward\n",
      "2022-01-19 01:25:52,616 | Epoch: [3][ 2/48]\tTime  33.47 ( 35.58)\tData 0.0189 (0.6479)\tLoss (L1) 10.205 (11.052)\n",
      "2022-01-19 01:25:52,630 | ===> Batch : 3\n",
      "2022-01-19 01:25:52,630 | FDS disable\n",
      "2022-01-19 01:26:04,509 | Calculate Loss\n",
      "2022-01-19 01:26:04,511 | Update Loss\n",
      "2022-01-19 01:26:04,515 | Backward\n",
      "2022-01-19 01:26:25,495 | Epoch: [3][ 3/48]\tTime  32.88 ( 34.68)\tData 0.0135 (0.4364)\tLoss (L1) 11.359 (11.154)\n",
      "2022-01-19 01:26:25,508 | ===> Batch : 4\n",
      "2022-01-19 01:26:25,509 | FDS disable\n",
      "2022-01-19 01:26:37,298 | Calculate Loss\n",
      "2022-01-19 01:26:37,300 | Update Loss\n",
      "2022-01-19 01:26:37,304 | Backward\n",
      "2022-01-19 01:26:58,893 | Epoch: [3][ 4/48]\tTime  33.40 ( 34.36)\tData 0.0135 (0.3307)\tLoss (L1) 11.288 (11.188)\n",
      "2022-01-19 01:26:58,906 | ===> Batch : 5\n",
      "2022-01-19 01:26:58,907 | FDS disable\n",
      "2022-01-19 01:27:10,747 | Calculate Loss\n",
      "2022-01-19 01:27:10,748 | Update Loss\n",
      "2022-01-19 01:27:10,752 | Backward\n",
      "2022-01-19 01:27:31,704 | Epoch: [3][ 5/48]\tTime  32.81 ( 34.05)\tData 0.0135 (0.2673)\tLoss (L1) 11.512 (11.253)\n",
      "2022-01-19 01:27:31,717 | ===> Batch : 6\n",
      "2022-01-19 01:27:31,718 | FDS disable\n",
      "2022-01-19 01:27:43,597 | Calculate Loss\n",
      "2022-01-19 01:27:43,598 | Update Loss\n",
      "2022-01-19 01:27:43,603 | Backward\n",
      "2022-01-19 01:28:05,190 | Epoch: [3][ 6/48]\tTime  33.49 ( 33.96)\tData 0.0135 (0.2250)\tLoss (L1) 10.256 (11.087)\n",
      "2022-01-19 01:28:05,203 | ===> Batch : 7\n",
      "2022-01-19 01:28:05,204 | FDS disable\n",
      "2022-01-19 01:28:17,045 | Calculate Loss\n",
      "2022-01-19 01:28:17,047 | Update Loss\n",
      "2022-01-19 01:28:17,051 | Backward\n",
      "2022-01-19 01:28:38,168 | Epoch: [3][ 7/48]\tTime  32.98 ( 33.82)\tData 0.0130 (0.1947)\tLoss (L1) 11.367 (11.127)\n",
      "2022-01-19 01:28:38,181 | ===> Batch : 8\n",
      "2022-01-19 01:28:38,182 | FDS disable\n",
      "2022-01-19 01:28:50,019 | Calculate Loss\n",
      "2022-01-19 01:28:50,020 | Update Loss\n",
      "2022-01-19 01:28:50,023 | Backward\n",
      "2022-01-19 01:29:10,849 | Epoch: [3][ 8/48]\tTime  32.68 ( 33.67)\tData 0.0132 (0.1720)\tLoss (L1) 11.057 (11.118)\n",
      "2022-01-19 01:29:10,869 | ===> Batch : 9\n",
      "2022-01-19 01:29:10,870 | FDS disable\n",
      "2022-01-19 01:29:22,663 | Calculate Loss\n",
      "2022-01-19 01:29:22,665 | Update Loss\n",
      "2022-01-19 01:29:22,669 | Backward\n",
      "2022-01-19 01:29:44,331 | Epoch: [3][ 9/48]\tTime  33.48 ( 33.65)\tData 0.0200 (0.1551)\tLoss (L1) 10.578 (11.058)\n",
      "2022-01-19 01:29:44,343 | ===> Batch : 10\n",
      "2022-01-19 01:29:44,344 | FDS disable\n",
      "2022-01-19 01:29:56,172 | Calculate Loss\n",
      "2022-01-19 01:29:56,173 | Update Loss\n",
      "2022-01-19 01:29:56,176 | Backward\n",
      "2022-01-19 01:30:17,852 | Epoch: [3][10/48]\tTime  33.52 ( 33.64)\tData 0.0128 (0.1409)\tLoss (L1) 11.229 (11.075)\n",
      "2022-01-19 01:30:17,865 | ===> Batch : 11\n",
      "2022-01-19 01:30:17,865 | FDS disable\n",
      "2022-01-19 01:30:29,688 | Calculate Loss\n",
      "2022-01-19 01:30:29,689 | Update Loss\n",
      "2022-01-19 01:30:29,693 | Backward\n",
      "2022-01-19 01:30:51,124 | Epoch: [3][11/48]\tTime  33.27 ( 33.61)\tData 0.0131 (0.1293)\tLoss (L1) 10.619 (11.034)\n",
      "2022-01-19 01:30:51,137 | ===> Batch : 12\n",
      "2022-01-19 01:30:51,137 | FDS disable\n",
      "2022-01-19 01:31:02,941 | Calculate Loss\n",
      "2022-01-19 01:31:02,942 | Update Loss\n",
      "2022-01-19 01:31:02,946 | Backward\n",
      "2022-01-19 01:31:24,224 | Epoch: [3][12/48]\tTime  33.10 ( 33.56)\tData 0.0130 (0.1196)\tLoss (L1) 10.612 (10.998)\n",
      "2022-01-19 01:31:24,237 | ===> Batch : 13\n",
      "2022-01-19 01:31:24,238 | FDS disable\n",
      "2022-01-19 01:31:36,088 | Calculate Loss\n",
      "2022-01-19 01:31:36,090 | Update Loss\n",
      "2022-01-19 01:31:36,094 | Backward\n",
      "2022-01-19 01:31:57,757 | Epoch: [3][13/48]\tTime  33.53 ( 33.56)\tData 0.0129 (0.1114)\tLoss (L1) 11.761 (11.057)\n",
      "2022-01-19 01:31:57,773 | ===> Batch : 14\n",
      "2022-01-19 01:31:57,773 | FDS disable\n",
      "2022-01-19 01:32:09,603 | Calculate Loss\n",
      "2022-01-19 01:32:09,604 | Update Loss\n",
      "2022-01-19 01:32:09,608 | Backward\n",
      "2022-01-19 01:32:31,164 | Epoch: [3][14/48]\tTime  33.41 ( 33.55)\tData 0.0154 (0.1045)\tLoss (L1) 10.663 (11.029)\n",
      "2022-01-19 01:32:31,178 | ===> Batch : 15\n",
      "2022-01-19 01:32:31,178 | FDS disable\n",
      "2022-01-19 01:32:43,045 | Calculate Loss\n",
      "2022-01-19 01:32:43,046 | Update Loss\n",
      "2022-01-19 01:32:43,050 | Backward\n",
      "2022-01-19 01:33:03,687 | Epoch: [3][15/48]\tTime  32.52 ( 33.48)\tData 0.0135 (0.0984)\tLoss (L1) 10.629 (11.002)\n",
      "2022-01-19 01:33:03,700 | ===> Batch : 16\n",
      "2022-01-19 01:33:03,700 | FDS disable\n",
      "2022-01-19 01:33:15,500 | Calculate Loss\n",
      "2022-01-19 01:33:15,502 | Update Loss\n",
      "2022-01-19 01:33:15,506 | Backward\n",
      "2022-01-19 01:33:37,076 | Epoch: [3][16/48]\tTime  33.39 ( 33.48)\tData 0.0131 (0.0931)\tLoss (L1) 11.739 (11.048)\n",
      "2022-01-19 01:33:37,090 | ===> Batch : 17\n",
      "2022-01-19 01:33:37,090 | FDS disable\n",
      "2022-01-19 01:33:48,923 | Calculate Loss\n",
      "2022-01-19 01:33:48,924 | Update Loss\n",
      "2022-01-19 01:33:48,928 | Backward\n",
      "2022-01-19 01:34:10,463 | Epoch: [3][17/48]\tTime  33.39 ( 33.47)\tData 0.0141 (0.0885)\tLoss (L1) 10.553 (11.019)\n",
      "2022-01-19 01:34:10,478 | ===> Batch : 18\n",
      "2022-01-19 01:34:10,479 | FDS disable\n",
      "2022-01-19 01:34:22,301 | Calculate Loss\n",
      "2022-01-19 01:34:22,302 | Update Loss\n",
      "2022-01-19 01:34:22,306 | Backward\n",
      "2022-01-19 01:34:43,183 | Epoch: [3][18/48]\tTime  32.72 ( 33.43)\tData 0.0154 (0.0844)\tLoss (L1) 10.588 (10.995)\n",
      "2022-01-19 01:34:43,197 | ===> Batch : 19\n",
      "2022-01-19 01:34:43,197 | FDS disable\n",
      "2022-01-19 01:34:55,065 | Calculate Loss\n",
      "2022-01-19 01:34:55,066 | Update Loss\n",
      "2022-01-19 01:34:55,070 | Backward\n",
      "2022-01-19 01:35:16,364 | Epoch: [3][19/48]\tTime  33.18 ( 33.42)\tData 0.0142 (0.0807)\tLoss (L1) 9.652 (10.925)\n",
      "2022-01-19 01:35:16,379 | ===> Batch : 20\n",
      "2022-01-19 01:35:16,379 | FDS disable\n",
      "2022-01-19 01:35:28,200 | Calculate Loss\n",
      "2022-01-19 01:35:28,202 | Update Loss\n",
      "2022-01-19 01:35:28,205 | Backward\n",
      "2022-01-19 01:35:49,452 | Epoch: [3][20/48]\tTime  33.09 ( 33.40)\tData 0.0152 (0.0774)\tLoss (L1) 11.402 (10.948)\n",
      "2022-01-19 01:35:49,469 | ===> Batch : 21\n",
      "2022-01-19 01:35:49,469 | FDS disable\n",
      "2022-01-19 01:36:01,284 | Calculate Loss\n",
      "2022-01-19 01:36:01,285 | Update Loss\n",
      "2022-01-19 01:36:01,289 | Backward\n",
      "2022-01-19 01:36:22,964 | Epoch: [3][21/48]\tTime  33.51 ( 33.40)\tData 0.0170 (0.0746)\tLoss (L1) 10.566 (10.930)\n",
      "2022-01-19 01:36:22,978 | ===> Batch : 22\n",
      "2022-01-19 01:36:22,978 | FDS disable\n",
      "2022-01-19 01:36:34,848 | Calculate Loss\n",
      "2022-01-19 01:36:34,849 | Update Loss\n",
      "2022-01-19 01:36:34,853 | Backward\n",
      "2022-01-19 01:36:55,443 | Epoch: [3][22/48]\tTime  32.48 ( 33.36)\tData 0.0138 (0.0718)\tLoss (L1) 11.458 (10.954)\n",
      "2022-01-19 01:36:55,457 | ===> Batch : 23\n",
      "2022-01-19 01:36:55,458 | FDS disable\n",
      "2022-01-19 01:37:07,308 | Calculate Loss\n",
      "2022-01-19 01:37:07,309 | Update Loss\n",
      "2022-01-19 01:37:07,313 | Backward\n",
      "2022-01-19 01:37:28,453 | Epoch: [3][23/48]\tTime  33.01 ( 33.35)\tData 0.0142 (0.0693)\tLoss (L1) 11.590 (10.982)\n",
      "2022-01-19 01:37:28,467 | ===> Batch : 24\n",
      "2022-01-19 01:37:28,467 | FDS disable\n",
      "2022-01-19 01:37:40,283 | Calculate Loss\n",
      "2022-01-19 01:37:40,284 | Update Loss\n",
      "2022-01-19 01:37:40,288 | Backward\n",
      "2022-01-19 01:38:01,998 | Epoch: [3][24/48]\tTime  33.55 ( 33.36)\tData 0.0140 (0.0670)\tLoss (L1) 12.467 (11.044)\n",
      "2022-01-19 01:38:02,014 | ===> Batch : 25\n",
      "2022-01-19 01:38:02,014 | FDS disable\n",
      "2022-01-19 01:38:13,816 | Calculate Loss\n",
      "2022-01-19 01:38:13,817 | Update Loss\n",
      "2022-01-19 01:38:13,820 | Backward\n",
      "2022-01-19 01:38:34,575 | Epoch: [3][25/48]\tTime  32.58 ( 33.32)\tData 0.0157 (0.0649)\tLoss (L1) 10.386 (11.017)\n",
      "2022-01-19 01:38:34,590 | ===> Batch : 26\n",
      "2022-01-19 01:38:34,590 | FDS disable\n",
      "2022-01-19 01:38:46,413 | Calculate Loss\n",
      "2022-01-19 01:38:46,414 | Update Loss\n",
      "2022-01-19 01:38:46,418 | Backward\n",
      "2022-01-19 01:39:07,920 | Epoch: [3][26/48]\tTime  33.34 ( 33.33)\tData 0.0145 (0.0630)\tLoss (L1) 11.340 (11.030)\n",
      "2022-01-19 01:39:07,934 | ===> Batch : 27\n",
      "2022-01-19 01:39:07,934 | FDS disable\n",
      "2022-01-19 01:39:19,772 | Calculate Loss\n",
      "2022-01-19 01:39:19,773 | Update Loss\n",
      "2022-01-19 01:39:19,777 | Backward\n",
      "2022-01-19 01:39:41,336 | Epoch: [3][27/48]\tTime  33.42 ( 33.33)\tData 0.0141 (0.0612)\tLoss (L1) 10.335 (11.004)\n",
      "2022-01-19 01:39:41,351 | ===> Batch : 28\n",
      "2022-01-19 01:39:41,351 | FDS disable\n",
      "2022-01-19 01:39:53,209 | Calculate Loss\n",
      "2022-01-19 01:39:53,211 | Update Loss\n",
      "2022-01-19 01:39:53,214 | Backward\n",
      "2022-01-19 01:40:14,583 | Epoch: [3][28/48]\tTime  33.25 ( 33.33)\tData 0.0142 (0.0595)\tLoss (L1) 10.728 (10.994)\n",
      "2022-01-19 01:40:14,597 | ===> Batch : 29\n",
      "2022-01-19 01:40:14,597 | FDS disable\n",
      "2022-01-19 01:40:26,434 | Calculate Loss\n",
      "2022-01-19 01:40:26,435 | Update Loss\n",
      "2022-01-19 01:40:26,439 | Backward\n",
      "2022-01-19 01:40:47,910 | Epoch: [3][29/48]\tTime  33.33 ( 33.33)\tData 0.0141 (0.0579)\tLoss (L1) 10.003 (10.960)\n",
      "2022-01-19 01:40:47,924 | ===> Batch : 30\n",
      "2022-01-19 01:40:47,924 | FDS disable\n",
      "2022-01-19 01:40:59,733 | Calculate Loss\n",
      "2022-01-19 01:40:59,734 | Update Loss\n",
      "2022-01-19 01:40:59,738 | Backward\n",
      "2022-01-19 01:41:20,739 | Epoch: [3][30/48]\tTime  32.83 ( 33.31)\tData 0.0140 (0.0565)\tLoss (L1) 10.886 (10.958)\n",
      "2022-01-19 01:41:20,754 | ===> Batch : 31\n",
      "2022-01-19 01:41:20,754 | FDS disable\n",
      "2022-01-19 01:41:32,571 | Calculate Loss\n",
      "2022-01-19 01:41:32,572 | Update Loss\n",
      "2022-01-19 01:41:32,576 | Backward\n",
      "2022-01-19 01:41:54,127 | Epoch: [3][31/48]\tTime  33.39 ( 33.31)\tData 0.0146 (0.0551)\tLoss (L1) 11.115 (10.963)\n",
      "2022-01-19 01:41:54,142 | ===> Batch : 32\n",
      "2022-01-19 01:41:54,142 | FDS disable\n",
      "2022-01-19 01:42:05,924 | Calculate Loss\n",
      "2022-01-19 01:42:05,926 | Update Loss\n",
      "2022-01-19 01:42:05,930 | Backward\n",
      "2022-01-19 01:42:27,613 | Epoch: [3][32/48]\tTime  33.49 ( 33.32)\tData 0.0144 (0.0539)\tLoss (L1) 10.979 (10.963)\n",
      "2022-01-19 01:42:27,627 | ===> Batch : 33\n",
      "2022-01-19 01:42:27,628 | FDS disable\n",
      "2022-01-19 01:42:39,325 | Calculate Loss\n",
      "2022-01-19 01:42:39,326 | Update Loss\n",
      "2022-01-19 01:42:39,330 | Backward\n",
      "2022-01-19 01:43:00,745 | Epoch: [3][33/48]\tTime  33.13 ( 33.31)\tData 0.0146 (0.0527)\tLoss (L1) 11.172 (10.970)\n",
      "2022-01-19 01:43:00,760 | ===> Batch : 34\n",
      "2022-01-19 01:43:00,760 | FDS disable\n",
      "2022-01-19 01:43:12,459 | Calculate Loss\n",
      "2022-01-19 01:43:12,460 | Update Loss\n",
      "2022-01-19 01:43:12,464 | Backward\n",
      "2022-01-19 01:43:34,057 | Epoch: [3][34/48]\tTime  33.31 ( 33.31)\tData 0.0146 (0.0515)\tLoss (L1) 10.582 (10.958)\n",
      "2022-01-19 01:43:34,072 | ===> Batch : 35\n",
      "2022-01-19 01:43:34,073 | FDS disable\n",
      "2022-01-19 01:43:45,767 | Calculate Loss\n",
      "2022-01-19 01:43:45,768 | Update Loss\n",
      "2022-01-19 01:43:45,772 | Backward\n",
      "2022-01-19 01:44:07,364 | Epoch: [3][35/48]\tTime  33.31 ( 33.31)\tData 0.0151 (0.0505)\tLoss (L1) 10.704 (10.951)\n",
      "2022-01-19 01:44:07,378 | ===> Batch : 36\n",
      "2022-01-19 01:44:07,379 | FDS disable\n",
      "2022-01-19 01:44:19,131 | Calculate Loss\n",
      "2022-01-19 01:44:19,133 | Update Loss\n",
      "2022-01-19 01:44:19,137 | Backward\n",
      "2022-01-19 01:44:40,202 | Epoch: [3][36/48]\tTime  32.84 ( 33.30)\tData 0.0142 (0.0495)\tLoss (L1) 11.254 (10.959)\n",
      "2022-01-19 01:44:40,218 | ===> Batch : 37\n",
      "2022-01-19 01:44:40,219 | FDS disable\n",
      "2022-01-19 01:44:52,048 | Calculate Loss\n",
      "2022-01-19 01:44:52,049 | Update Loss\n",
      "2022-01-19 01:44:52,053 | Backward\n",
      "2022-01-19 01:45:13,648 | Epoch: [3][37/48]\tTime  33.45 ( 33.30)\tData 0.0167 (0.0486)\tLoss (L1) 10.949 (10.959)\n",
      "2022-01-19 01:45:13,663 | ===> Batch : 38\n",
      "2022-01-19 01:45:13,663 | FDS disable\n",
      "2022-01-19 01:45:25,371 | Calculate Loss\n",
      "2022-01-19 01:45:25,373 | Update Loss\n",
      "2022-01-19 01:45:25,377 | Backward\n",
      "2022-01-19 01:45:46,497 | Epoch: [3][38/48]\tTime  32.85 ( 33.29)\tData 0.0145 (0.0477)\tLoss (L1) 11.466 (10.972)\n",
      "2022-01-19 01:45:46,511 | ===> Batch : 39\n",
      "2022-01-19 01:45:46,511 | FDS disable\n",
      "2022-01-19 01:45:58,212 | Calculate Loss\n",
      "2022-01-19 01:45:58,213 | Update Loss\n",
      "2022-01-19 01:45:58,217 | Backward\n",
      "2022-01-19 01:46:19,843 | Epoch: [3][39/48]\tTime  33.35 ( 33.29)\tData 0.0138 (0.0468)\tLoss (L1) 10.962 (10.972)\n",
      "2022-01-19 01:46:19,857 | ===> Batch : 40\n",
      "2022-01-19 01:46:19,857 | FDS disable\n",
      "2022-01-19 01:46:31,560 | Calculate Loss\n",
      "2022-01-19 01:46:31,562 | Update Loss\n",
      "2022-01-19 01:46:31,565 | Backward\n",
      "2022-01-19 01:46:52,644 | Epoch: [3][40/48]\tTime  32.80 ( 33.28)\tData 0.0138 (0.0460)\tLoss (L1) 11.608 (10.988)\n",
      "2022-01-19 01:46:52,658 | ===> Batch : 41\n",
      "2022-01-19 01:46:52,658 | FDS disable\n",
      "2022-01-19 01:47:04,393 | Calculate Loss\n",
      "2022-01-19 01:47:04,395 | Update Loss\n",
      "2022-01-19 01:47:04,399 | Backward\n",
      "2022-01-19 01:47:25,965 | Epoch: [3][41/48]\tTime  33.32 ( 33.28)\tData 0.0137 (0.0452)\tLoss (L1) 11.474 (11.000)\n",
      "2022-01-19 01:47:25,980 | ===> Batch : 42\n",
      "2022-01-19 01:47:25,981 | FDS disable\n",
      "2022-01-19 01:47:37,723 | Calculate Loss\n",
      "2022-01-19 01:47:37,724 | Update Loss\n",
      "2022-01-19 01:47:37,728 | Backward\n",
      "2022-01-19 01:47:59,089 | Epoch: [3][42/48]\tTime  33.12 ( 33.28)\tData 0.0150 (0.0445)\tLoss (L1) 11.019 (11.000)\n",
      "2022-01-19 01:47:59,104 | ===> Batch : 43\n",
      "2022-01-19 01:47:59,105 | FDS disable\n",
      "2022-01-19 01:48:10,843 | Calculate Loss\n",
      "2022-01-19 01:48:10,844 | Update Loss\n",
      "2022-01-19 01:48:10,848 | Backward\n",
      "2022-01-19 01:48:31,888 | Epoch: [3][43/48]\tTime  32.80 ( 33.27)\tData 0.0148 (0.0438)\tLoss (L1) 11.679 (11.016)\n",
      "2022-01-19 01:48:31,904 | ===> Batch : 44\n",
      "2022-01-19 01:48:31,904 | FDS disable\n",
      "2022-01-19 01:48:43,585 | Calculate Loss\n",
      "2022-01-19 01:48:43,586 | Update Loss\n",
      "2022-01-19 01:48:43,590 | Backward\n",
      "2022-01-19 01:49:04,967 | Epoch: [3][44/48]\tTime  33.08 ( 33.26)\tData 0.0155 (0.0432)\tLoss (L1) 10.346 (11.001)\n",
      "2022-01-19 01:49:04,982 | ===> Batch : 45\n",
      "2022-01-19 01:49:04,983 | FDS disable\n",
      "2022-01-19 01:49:16,725 | Calculate Loss\n",
      "2022-01-19 01:49:16,726 | Update Loss\n",
      "2022-01-19 01:49:16,730 | Backward\n",
      "2022-01-19 01:49:38,366 | Epoch: [3][45/48]\tTime  33.40 ( 33.26)\tData 0.0155 (0.0426)\tLoss (L1) 11.347 (11.008)\n",
      "2022-01-19 01:49:38,380 | ===> Batch : 46\n",
      "2022-01-19 01:49:38,381 | FDS disable\n",
      "2022-01-19 01:49:50,123 | Calculate Loss\n",
      "2022-01-19 01:49:50,124 | Update Loss\n",
      "2022-01-19 01:49:50,128 | Backward\n",
      "2022-01-19 01:50:10,967 | Epoch: [3][46/48]\tTime  32.60 ( 33.25)\tData 0.0143 (0.0419)\tLoss (L1) 10.892 (11.006)\n",
      "2022-01-19 01:50:10,981 | ===> Batch : 47\n",
      "2022-01-19 01:50:10,981 | FDS disable\n",
      "2022-01-19 01:50:22,726 | Calculate Loss\n",
      "2022-01-19 01:50:22,728 | Update Loss\n",
      "2022-01-19 01:50:22,731 | Backward\n",
      "2022-01-19 01:50:44,075 | Epoch: [3][47/48]\tTime  33.11 ( 33.25)\tData 0.0144 (0.0414)\tLoss (L1) 11.709 (11.021)\n",
      "2022-01-19 01:50:44,093 | ===> Batch : 48\n",
      "2022-01-19 01:50:44,093 | FDS disable\n",
      "2022-01-19 01:50:52,652 | Calculate Loss\n",
      "2022-01-19 01:50:52,654 | Update Loss\n",
      "2022-01-19 01:50:52,658 | Backward\n",
      "2022-01-19 01:51:07,360 | Epoch: [3][48/48]\tTime  23.29 ( 33.04)\tData 0.0181 (0.0409)\tLoss (L1) 11.395 (11.026)\n",
      "2022-01-19 01:51:19,241 | Val: [0/9]\tTime 11.721 (11.721)\tLoss (MSE) 304.373 (304.373)\tLoss (L1) 14.014 (14.014)\n",
      "2022-01-19 01:51:28,869 | Val: [1/9]\tTime  9.628 (10.675)\tLoss (MSE) 286.813 (295.593)\tLoss (L1) 13.687 (13.851)\n",
      "2022-01-19 01:51:38,492 | Val: [2/9]\tTime  9.623 (10.324)\tLoss (MSE) 310.522 (300.569)\tLoss (L1) 14.462 (14.054)\n",
      "2022-01-19 01:51:47,993 | Val: [3/9]\tTime  9.501 (10.118)\tLoss (MSE) 295.934 (299.410)\tLoss (L1) 14.249 (14.103)\n",
      "2022-01-19 01:51:57,590 | Val: [4/9]\tTime  9.597 (10.014)\tLoss (MSE) 295.774 (298.683)\tLoss (L1) 14.436 (14.170)\n",
      "2022-01-19 01:52:07,182 | Val: [5/9]\tTime  9.592 ( 9.944)\tLoss (MSE) 311.211 (300.771)\tLoss (L1) 14.472 (14.220)\n",
      "2022-01-19 01:52:16,788 | Val: [6/9]\tTime  9.606 ( 9.896)\tLoss (MSE) 282.512 (298.163)\tLoss (L1) 13.942 (14.180)\n",
      "2022-01-19 01:52:26,396 | Val: [7/9]\tTime  9.608 ( 9.860)\tLoss (MSE) 331.043 (302.273)\tLoss (L1) 14.855 (14.265)\n",
      "2022-01-19 01:52:29,658 | Val: [8/9]\tTime  3.262 ( 9.127)\tLoss (MSE) 290.101 (301.749)\tLoss (L1) 13.361 (14.226)\n",
      "2022-01-19 01:52:29,773 |  * Overall: MSE 301.749\tL1 14.226\tG-Mean 9.895\n",
      "2022-01-19 01:52:29,774 |  * Many: MSE 165.934\tL1 10.541\tG-Mean 7.336\n",
      "2022-01-19 01:52:29,774 |  * Median: MSE 565.664\tL1 21.854\tG-Mean 19.214\n",
      "2022-01-19 01:52:29,775 |  * Low: MSE 854.613\tL1 27.934\tG-Mean 26.634\n",
      "2022-01-19 01:52:29,780 | Best L1 Loss: 14.226\n",
      "2022-01-19 01:52:31,033 | ===> Saving current best checkpoint...\n",
      "2022-01-19 01:52:32,183 | Epoch #3: Train loss [11.0263]; Val loss: MSE [301.7494], L1 [14.2259], G-Mean [9.8949]\n",
      "2022-01-19 01:52:32,184 | Training...\n",
      "2022-01-19 01:52:32,185 | Load train loader\n",
      "2022-01-19 01:52:33,451 | ===> Batch : 1\n",
      "2022-01-19 01:52:33,453 | FDS disable\n",
      "2022-01-19 01:52:47,467 | Calculate Loss\n",
      "2022-01-19 01:52:47,469 | Update Loss\n",
      "2022-01-19 01:52:47,590 | Backward\n",
      "2022-01-19 01:53:09,656 | Epoch: [4][ 1/48]\tTime  37.47 ( 37.47)\tData 1.2660 (1.2660)\tLoss (L1) 11.031 (11.031)\n",
      "2022-01-19 01:53:09,669 | ===> Batch : 2\n",
      "2022-01-19 01:53:09,670 | FDS disable\n",
      "2022-01-19 01:53:21,542 | Calculate Loss\n",
      "2022-01-19 01:53:21,543 | Update Loss\n",
      "2022-01-19 01:53:21,547 | Backward\n",
      "2022-01-19 01:53:42,973 | Epoch: [4][ 2/48]\tTime  33.32 ( 35.39)\tData 0.0132 (0.6396)\tLoss (L1) 10.635 (10.833)\n",
      "2022-01-19 01:53:42,986 | ===> Batch : 3\n",
      "2022-01-19 01:53:42,987 | FDS disable\n",
      "2022-01-19 01:53:54,807 | Calculate Loss\n",
      "2022-01-19 01:53:54,809 | Update Loss\n",
      "2022-01-19 01:53:54,812 | Backward\n",
      "2022-01-19 01:54:16,309 | Epoch: [4][ 3/48]\tTime  33.34 ( 34.71)\tData 0.0135 (0.4309)\tLoss (L1) 10.996 (10.887)\n",
      "2022-01-19 01:54:16,323 | ===> Batch : 4\n",
      "2022-01-19 01:54:16,323 | FDS disable\n",
      "2022-01-19 01:54:28,146 | Calculate Loss\n",
      "2022-01-19 01:54:28,147 | Update Loss\n",
      "2022-01-19 01:54:28,151 | Backward\n",
      "2022-01-19 01:54:49,557 | Epoch: [4][ 4/48]\tTime  33.25 ( 34.34)\tData 0.0140 (0.3267)\tLoss (L1) 10.757 (10.855)\n",
      "2022-01-19 01:54:49,570 | ===> Batch : 5\n",
      "2022-01-19 01:54:49,571 | FDS disable\n",
      "2022-01-19 01:55:01,434 | Calculate Loss\n",
      "2022-01-19 01:55:01,436 | Update Loss\n",
      "2022-01-19 01:55:01,439 | Backward\n",
      "2022-01-19 01:55:22,457 | Epoch: [4][ 5/48]\tTime  32.90 ( 34.05)\tData 0.0133 (0.2640)\tLoss (L1) 10.496 (10.783)\n",
      "2022-01-19 01:55:22,470 | ===> Batch : 6\n",
      "2022-01-19 01:55:22,470 | FDS disable\n",
      "2022-01-19 01:55:34,338 | Calculate Loss\n",
      "2022-01-19 01:55:34,340 | Update Loss\n",
      "2022-01-19 01:55:34,343 | Backward\n",
      "2022-01-19 01:55:55,442 | Epoch: [4][ 6/48]\tTime  32.99 ( 33.88)\tData 0.0134 (0.2222)\tLoss (L1) 10.764 (10.780)\n",
      "2022-01-19 01:55:55,456 | ===> Batch : 7\n",
      "2022-01-19 01:55:55,456 | FDS disable\n",
      "2022-01-19 01:56:07,265 | Calculate Loss\n",
      "2022-01-19 01:56:07,267 | Update Loss\n",
      "2022-01-19 01:56:07,271 | Backward\n",
      "2022-01-19 01:56:28,413 | Epoch: [4][ 7/48]\tTime  32.97 ( 33.75)\tData 0.0135 (0.1924)\tLoss (L1) 9.635 (10.616)\n",
      "2022-01-19 01:56:28,426 | ===> Batch : 8\n",
      "2022-01-19 01:56:28,427 | FDS disable\n",
      "2022-01-19 01:56:40,243 | Calculate Loss\n",
      "2022-01-19 01:56:40,244 | Update Loss\n",
      "2022-01-19 01:56:40,248 | Backward\n",
      "2022-01-19 01:57:01,226 | Epoch: [4][ 8/48]\tTime  32.81 ( 33.63)\tData 0.0134 (0.1700)\tLoss (L1) 10.916 (10.654)\n",
      "2022-01-19 01:57:01,243 | ===> Batch : 9\n",
      "2022-01-19 01:57:01,244 | FDS disable\n",
      "2022-01-19 01:57:13,029 | Calculate Loss\n",
      "2022-01-19 01:57:13,031 | Update Loss\n",
      "2022-01-19 01:57:13,034 | Backward\n",
      "2022-01-19 01:57:34,586 | Epoch: [4][ 9/48]\tTime  33.36 ( 33.60)\tData 0.0178 (0.1531)\tLoss (L1) 9.644 (10.542)\n",
      "2022-01-19 01:57:34,602 | ===> Batch : 10\n",
      "2022-01-19 01:57:34,602 | FDS disable\n",
      "2022-01-19 01:57:46,431 | Calculate Loss\n",
      "2022-01-19 01:57:46,432 | Update Loss\n",
      "2022-01-19 01:57:46,436 | Backward\n",
      "2022-01-19 01:58:08,010 | Epoch: [4][10/48]\tTime  33.42 ( 33.58)\tData 0.0156 (0.1394)\tLoss (L1) 11.235 (10.611)\n",
      "2022-01-19 01:58:08,023 | ===> Batch : 11\n",
      "2022-01-19 01:58:08,024 | FDS disable\n",
      "2022-01-19 01:58:19,851 | Calculate Loss\n",
      "2022-01-19 01:58:19,853 | Update Loss\n",
      "2022-01-19 01:58:19,856 | Backward\n",
      "2022-01-19 01:58:40,585 | Epoch: [4][11/48]\tTime  32.58 ( 33.49)\tData 0.0132 (0.1279)\tLoss (L1) 11.615 (10.702)\n",
      "2022-01-19 01:58:40,598 | ===> Batch : 12\n",
      "2022-01-19 01:58:40,598 | FDS disable\n",
      "2022-01-19 01:58:52,386 | Calculate Loss\n",
      "2022-01-19 01:58:52,387 | Update Loss\n",
      "2022-01-19 01:58:52,391 | Backward\n",
      "2022-01-19 01:59:14,002 | Epoch: [4][12/48]\tTime  33.42 ( 33.48)\tData 0.0127 (0.1183)\tLoss (L1) 10.836 (10.713)\n",
      "2022-01-19 01:59:14,015 | ===> Batch : 13\n",
      "2022-01-19 01:59:14,015 | FDS disable\n",
      "2022-01-19 01:59:25,850 | Calculate Loss\n",
      "2022-01-19 01:59:25,852 | Update Loss\n",
      "2022-01-19 01:59:25,856 | Backward\n",
      "2022-01-19 01:59:46,998 | Epoch: [4][13/48]\tTime  33.00 ( 33.45)\tData 0.0127 (0.1102)\tLoss (L1) 10.516 (10.698)\n",
      "2022-01-19 01:59:47,013 | ===> Batch : 14\n",
      "2022-01-19 01:59:47,014 | FDS disable\n",
      "2022-01-19 01:59:58,832 | Calculate Loss\n",
      "2022-01-19 01:59:58,833 | Update Loss\n",
      "2022-01-19 01:59:58,837 | Backward\n",
      "2022-01-19 02:00:20,377 | Epoch: [4][14/48]\tTime  33.38 ( 33.44)\tData 0.0149 (0.1034)\tLoss (L1) 11.343 (10.744)\n",
      "2022-01-19 02:00:20,391 | ===> Batch : 15\n",
      "2022-01-19 02:00:20,392 | FDS disable\n",
      "2022-01-19 02:00:32,248 | Calculate Loss\n",
      "2022-01-19 02:00:32,249 | Update Loss\n",
      "2022-01-19 02:00:32,253 | Backward\n",
      "2022-01-19 02:00:53,366 | Epoch: [4][15/48]\tTime  32.99 ( 33.41)\tData 0.0143 (0.0974)\tLoss (L1) 11.794 (10.814)\n",
      "2022-01-19 02:00:53,380 | ===> Batch : 16\n",
      "2022-01-19 02:00:53,381 | FDS disable\n",
      "2022-01-19 02:01:05,213 | Calculate Loss\n",
      "2022-01-19 02:01:05,215 | Update Loss\n",
      "2022-01-19 02:01:05,219 | Backward\n",
      "2022-01-19 02:01:26,347 | Epoch: [4][16/48]\tTime  32.98 ( 33.39)\tData 0.0142 (0.0922)\tLoss (L1) 10.801 (10.813)\n",
      "2022-01-19 02:01:26,361 | ===> Batch : 17\n",
      "2022-01-19 02:01:26,362 | FDS disable\n",
      "2022-01-19 02:01:38,166 | Calculate Loss\n",
      "2022-01-19 02:01:38,168 | Update Loss\n",
      "2022-01-19 02:01:38,172 | Backward\n",
      "2022-01-19 02:01:59,792 | Epoch: [4][17/48]\tTime  33.45 ( 33.39)\tData 0.0146 (0.0877)\tLoss (L1) 10.351 (10.786)\n",
      "2022-01-19 02:01:59,806 | ===> Batch : 18\n",
      "2022-01-19 02:01:59,807 | FDS disable\n",
      "2022-01-19 02:02:11,618 | Calculate Loss\n",
      "2022-01-19 02:02:11,619 | Update Loss\n",
      "2022-01-19 02:02:11,623 | Backward\n",
      "2022-01-19 02:02:32,429 | Epoch: [4][18/48]\tTime  32.64 ( 33.35)\tData 0.0146 (0.0836)\tLoss (L1) 10.665 (10.779)\n",
      "2022-01-19 02:02:32,450 | ===> Batch : 19\n",
      "2022-01-19 02:02:32,451 | FDS disable\n",
      "2022-01-19 02:02:44,263 | Calculate Loss\n",
      "2022-01-19 02:02:44,265 | Update Loss\n",
      "2022-01-19 02:02:44,269 | Backward\n",
      "2022-01-19 02:03:05,953 | Epoch: [4][19/48]\tTime  33.52 ( 33.36)\tData 0.0215 (0.0803)\tLoss (L1) 10.486 (10.764)\n",
      "2022-01-19 02:03:05,968 | ===> Batch : 20\n",
      "2022-01-19 02:03:05,968 | FDS disable\n",
      "2022-01-19 02:03:17,770 | Calculate Loss\n",
      "2022-01-19 02:03:17,771 | Update Loss\n",
      "2022-01-19 02:03:17,775 | Backward\n",
      "2022-01-19 02:03:39,101 | Epoch: [4][20/48]\tTime  33.15 ( 33.35)\tData 0.0146 (0.0771)\tLoss (L1) 10.715 (10.762)\n",
      "2022-01-19 02:03:39,115 | ===> Batch : 21\n",
      "2022-01-19 02:03:39,116 | FDS disable\n",
      "2022-01-19 02:03:50,938 | Calculate Loss\n",
      "2022-01-19 02:03:50,939 | Update Loss\n",
      "2022-01-19 02:03:50,942 | Backward\n",
      "2022-01-19 02:04:11,349 | Epoch: [4][21/48]\tTime  32.25 ( 33.29)\tData 0.0137 (0.0740)\tLoss (L1) 10.322 (10.741)\n",
      "2022-01-19 02:04:11,370 | ===> Batch : 22\n",
      "2022-01-19 02:04:11,370 | FDS disable\n",
      "2022-01-19 02:04:23,213 | Calculate Loss\n",
      "2022-01-19 02:04:23,214 | Update Loss\n",
      "2022-01-19 02:04:23,218 | Backward\n",
      "2022-01-19 02:04:44,817 | Epoch: [4][22/48]\tTime  33.47 ( 33.30)\tData 0.0208 (0.0716)\tLoss (L1) 11.091 (10.757)\n",
      "2022-01-19 02:04:44,833 | ===> Batch : 23\n",
      "2022-01-19 02:04:44,834 | FDS disable\n",
      "2022-01-19 02:04:56,741 | Calculate Loss\n",
      "2022-01-19 02:04:56,743 | Update Loss\n",
      "2022-01-19 02:04:56,747 | Backward\n",
      "2022-01-19 02:05:17,935 | Epoch: [4][23/48]\tTime  33.12 ( 33.29)\tData 0.0161 (0.0692)\tLoss (L1) 10.955 (10.765)\n",
      "2022-01-19 02:05:17,951 | ===> Batch : 24\n",
      "2022-01-19 02:05:17,951 | FDS disable\n",
      "2022-01-19 02:05:29,829 | Calculate Loss\n",
      "2022-01-19 02:05:29,830 | Update Loss\n",
      "2022-01-19 02:05:29,834 | Backward\n",
      "2022-01-19 02:05:51,367 | Epoch: [4][24/48]\tTime  33.43 ( 33.30)\tData 0.0152 (0.0670)\tLoss (L1) 10.960 (10.773)\n",
      "2022-01-19 02:05:51,382 | ===> Batch : 25\n",
      "2022-01-19 02:05:51,382 | FDS disable\n",
      "2022-01-19 02:06:03,165 | Calculate Loss\n",
      "2022-01-19 02:06:03,166 | Update Loss\n",
      "2022-01-19 02:06:03,170 | Backward\n",
      "2022-01-19 02:06:24,647 | Epoch: [4][25/48]\tTime  33.28 ( 33.30)\tData 0.0148 (0.0649)\tLoss (L1) 10.915 (10.779)\n",
      "2022-01-19 02:06:24,661 | ===> Batch : 26\n",
      "2022-01-19 02:06:24,661 | FDS disable\n",
      "2022-01-19 02:06:36,517 | Calculate Loss\n",
      "2022-01-19 02:06:36,519 | Update Loss\n",
      "2022-01-19 02:06:36,523 | Backward\n",
      "2022-01-19 02:06:57,341 | Epoch: [4][26/48]\tTime  32.69 ( 33.28)\tData 0.0146 (0.0629)\tLoss (L1) 10.346 (10.762)\n",
      "2022-01-19 02:06:57,356 | ===> Batch : 27\n",
      "2022-01-19 02:06:57,356 | FDS disable\n",
      "2022-01-19 02:07:09,178 | Calculate Loss\n",
      "2022-01-19 02:07:09,179 | Update Loss\n",
      "2022-01-19 02:07:09,183 | Backward\n",
      "2022-01-19 02:07:30,404 | Epoch: [4][27/48]\tTime  33.06 ( 33.27)\tData 0.0145 (0.0611)\tLoss (L1) 10.375 (10.748)\n",
      "2022-01-19 02:07:30,418 | ===> Batch : 28\n",
      "2022-01-19 02:07:30,419 | FDS disable\n",
      "2022-01-19 02:07:42,178 | Calculate Loss\n",
      "2022-01-19 02:07:42,179 | Update Loss\n",
      "2022-01-19 02:07:42,183 | Backward\n",
      "2022-01-19 02:08:03,918 | Epoch: [4][28/48]\tTime  33.51 ( 33.28)\tData 0.0141 (0.0595)\tLoss (L1) 11.053 (10.759)\n",
      "2022-01-19 02:08:03,939 | ===> Batch : 29\n",
      "2022-01-19 02:08:03,939 | FDS disable\n",
      "2022-01-19 02:08:15,767 | Calculate Loss\n",
      "2022-01-19 02:08:15,768 | Update Loss\n",
      "2022-01-19 02:08:15,772 | Backward\n",
      "2022-01-19 02:08:36,893 | Epoch: [4][29/48]\tTime  32.98 ( 33.27)\tData 0.0209 (0.0581)\tLoss (L1) 11.560 (10.786)\n",
      "2022-01-19 02:08:36,907 | ===> Batch : 30\n",
      "2022-01-19 02:08:36,907 | FDS disable\n",
      "2022-01-19 02:08:48,759 | Calculate Loss\n",
      "2022-01-19 02:08:48,760 | Update Loss\n",
      "2022-01-19 02:08:48,764 | Backward\n",
      "2022-01-19 02:09:10,231 | Epoch: [4][30/48]\tTime  33.34 ( 33.27)\tData 0.0138 (0.0566)\tLoss (L1) 10.482 (10.776)\n",
      "2022-01-19 02:09:10,249 | ===> Batch : 31\n",
      "2022-01-19 02:09:10,249 | FDS disable\n",
      "2022-01-19 02:09:22,069 | Calculate Loss\n",
      "2022-01-19 02:09:22,070 | Update Loss\n",
      "2022-01-19 02:09:22,074 | Backward\n",
      "2022-01-19 02:09:43,268 | Epoch: [4][31/48]\tTime  33.04 ( 33.26)\tData 0.0175 (0.0554)\tLoss (L1) 10.170 (10.757)\n",
      "2022-01-19 02:09:43,282 | ===> Batch : 32\n",
      "2022-01-19 02:09:43,283 | FDS disable\n",
      "2022-01-19 02:09:55,023 | Calculate Loss\n",
      "2022-01-19 02:09:55,024 | Update Loss\n",
      "2022-01-19 02:09:55,028 | Backward\n",
      "2022-01-19 02:10:16,446 | Epoch: [4][32/48]\tTime  33.18 ( 33.26)\tData 0.0147 (0.0541)\tLoss (L1) 11.991 (10.795)\n",
      "2022-01-19 02:10:16,460 | ===> Batch : 33\n",
      "2022-01-19 02:10:16,460 | FDS disable\n",
      "2022-01-19 02:10:28,136 | Calculate Loss\n",
      "2022-01-19 02:10:28,138 | Update Loss\n",
      "2022-01-19 02:10:28,142 | Backward\n",
      "2022-01-19 02:10:49,328 | Epoch: [4][33/48]\tTime  32.88 ( 33.25)\tData 0.0139 (0.0529)\tLoss (L1) 11.004 (10.802)\n",
      "2022-01-19 02:10:49,342 | ===> Batch : 34\n",
      "2022-01-19 02:10:49,343 | FDS disable\n",
      "2022-01-19 02:11:01,035 | Calculate Loss\n",
      "2022-01-19 02:11:01,037 | Update Loss\n",
      "2022-01-19 02:11:01,040 | Backward\n",
      "2022-01-19 02:11:22,190 | Epoch: [4][34/48]\tTime  32.86 ( 33.24)\tData 0.0143 (0.0518)\tLoss (L1) 10.510 (10.793)\n",
      "2022-01-19 02:11:22,204 | ===> Batch : 35\n",
      "2022-01-19 02:11:22,205 | FDS disable\n",
      "2022-01-19 02:11:33,994 | Calculate Loss\n",
      "2022-01-19 02:11:33,995 | Update Loss\n",
      "2022-01-19 02:11:33,999 | Backward\n",
      "2022-01-19 02:11:55,200 | Epoch: [4][35/48]\tTime  33.01 ( 33.23)\tData 0.0148 (0.0507)\tLoss (L1) 10.750 (10.792)\n",
      "2022-01-19 02:11:55,215 | ===> Batch : 36\n",
      "2022-01-19 02:11:55,216 | FDS disable\n",
      "2022-01-19 02:12:06,967 | Calculate Loss\n",
      "2022-01-19 02:12:06,969 | Update Loss\n",
      "2022-01-19 02:12:06,972 | Backward\n",
      "2022-01-19 02:12:27,929 | Epoch: [4][36/48]\tTime  32.73 ( 33.22)\tData 0.0154 (0.0497)\tLoss (L1) 11.581 (10.814)\n",
      "2022-01-19 02:12:27,943 | ===> Batch : 37\n",
      "2022-01-19 02:12:27,943 | FDS disable\n",
      "2022-01-19 02:12:39,684 | Calculate Loss\n",
      "2022-01-19 02:12:39,685 | Update Loss\n",
      "2022-01-19 02:12:39,689 | Backward\n",
      "2022-01-19 02:13:00,765 | Epoch: [4][37/48]\tTime  32.84 ( 33.20)\tData 0.0142 (0.0488)\tLoss (L1) 10.814 (10.814)\n",
      "2022-01-19 02:13:00,780 | ===> Batch : 38\n",
      "2022-01-19 02:13:00,781 | FDS disable\n",
      "2022-01-19 02:13:12,475 | Calculate Loss\n",
      "2022-01-19 02:13:12,477 | Update Loss\n",
      "2022-01-19 02:13:12,481 | Backward\n",
      "2022-01-19 02:13:34,150 | Epoch: [4][38/48]\tTime  33.39 ( 33.21)\tData 0.0154 (0.0479)\tLoss (L1) 10.007 (10.793)\n",
      "2022-01-19 02:13:34,165 | ===> Batch : 39\n",
      "2022-01-19 02:13:34,166 | FDS disable\n",
      "2022-01-19 02:13:45,835 | Calculate Loss\n",
      "2022-01-19 02:13:45,836 | Update Loss\n",
      "2022-01-19 02:13:45,840 | Backward\n",
      "2022-01-19 02:14:07,249 | Epoch: [4][39/48]\tTime  33.10 ( 33.21)\tData 0.0149 (0.0470)\tLoss (L1) 10.217 (10.778)\n",
      "2022-01-19 02:14:07,263 | ===> Batch : 40\n",
      "2022-01-19 02:14:07,263 | FDS disable\n",
      "2022-01-19 02:14:19,003 | Calculate Loss\n",
      "2022-01-19 02:14:19,005 | Update Loss\n",
      "2022-01-19 02:14:19,009 | Backward\n",
      "2022-01-19 02:14:39,805 | Epoch: [4][40/48]\tTime  32.56 ( 33.19)\tData 0.0140 (0.0462)\tLoss (L1) 10.603 (10.773)\n",
      "2022-01-19 02:14:39,819 | ===> Batch : 41\n",
      "2022-01-19 02:14:39,820 | FDS disable\n",
      "2022-01-19 02:14:51,514 | Calculate Loss\n",
      "2022-01-19 02:14:51,516 | Update Loss\n",
      "2022-01-19 02:14:51,520 | Backward\n",
      "2022-01-19 02:15:13,110 | Epoch: [4][41/48]\tTime  33.30 ( 33.19)\tData 0.0144 (0.0454)\tLoss (L1) 10.758 (10.773)\n",
      "2022-01-19 02:15:13,124 | ===> Batch : 42\n",
      "2022-01-19 02:15:13,124 | FDS disable\n",
      "2022-01-19 02:15:24,895 | Calculate Loss\n",
      "2022-01-19 02:15:24,896 | Update Loss\n",
      "2022-01-19 02:15:24,900 | Backward\n",
      "2022-01-19 02:15:45,578 | Epoch: [4][42/48]\tTime  32.47 ( 33.18)\tData 0.0145 (0.0447)\tLoss (L1) 11.514 (10.791)\n",
      "2022-01-19 02:15:45,592 | ===> Batch : 43\n",
      "2022-01-19 02:15:45,592 | FDS disable\n",
      "2022-01-19 02:15:57,278 | Calculate Loss\n",
      "2022-01-19 02:15:57,280 | Update Loss\n",
      "2022-01-19 02:15:57,283 | Backward\n",
      "2022-01-19 02:16:18,747 | Epoch: [4][43/48]\tTime  33.17 ( 33.18)\tData 0.0142 (0.0440)\tLoss (L1) 10.345 (10.780)\n",
      "2022-01-19 02:16:18,762 | ===> Batch : 44\n",
      "2022-01-19 02:16:18,762 | FDS disable\n",
      "2022-01-19 02:16:30,469 | Calculate Loss\n",
      "2022-01-19 02:16:30,470 | Update Loss\n",
      "2022-01-19 02:16:30,474 | Backward\n",
      "2022-01-19 02:16:51,481 | Epoch: [4][44/48]\tTime  32.73 ( 33.17)\tData 0.0145 (0.0433)\tLoss (L1) 10.749 (10.780)\n",
      "2022-01-19 02:16:51,495 | ===> Batch : 45\n",
      "2022-01-19 02:16:51,496 | FDS disable\n",
      "2022-01-19 02:17:03,218 | Calculate Loss\n",
      "2022-01-19 02:17:03,219 | Update Loss\n",
      "2022-01-19 02:17:03,223 | Backward\n",
      "2022-01-19 02:17:24,827 | Epoch: [4][45/48]\tTime  33.35 ( 33.17)\tData 0.0142 (0.0427)\tLoss (L1) 11.844 (10.803)\n",
      "2022-01-19 02:17:24,843 | ===> Batch : 46\n",
      "2022-01-19 02:17:24,843 | FDS disable\n",
      "2022-01-19 02:17:36,550 | Calculate Loss\n",
      "2022-01-19 02:17:36,551 | Update Loss\n",
      "2022-01-19 02:17:36,555 | Backward\n",
      "2022-01-19 02:17:57,756 | Epoch: [4][46/48]\tTime  32.93 ( 33.16)\tData 0.0152 (0.0421)\tLoss (L1) 11.493 (10.818)\n",
      "2022-01-19 02:17:57,771 | ===> Batch : 47\n",
      "2022-01-19 02:17:57,772 | FDS disable\n",
      "2022-01-19 02:18:09,480 | Calculate Loss\n",
      "2022-01-19 02:18:09,482 | Update Loss\n",
      "2022-01-19 02:18:09,486 | Backward\n",
      "2022-01-19 02:18:30,421 | Epoch: [4][47/48]\tTime  32.66 ( 33.15)\tData 0.0147 (0.0415)\tLoss (L1) 9.988 (10.801)\n",
      "2022-01-19 02:18:30,438 | ===> Batch : 48\n",
      "2022-01-19 02:18:30,438 | FDS disable\n",
      "2022-01-19 02:18:39,100 | Calculate Loss\n",
      "2022-01-19 02:18:39,101 | Update Loss\n",
      "2022-01-19 02:18:39,105 | Backward\n",
      "2022-01-19 02:18:53,577 | Epoch: [4][48/48]\tTime  23.16 ( 32.95)\tData 0.0166 (0.0410)\tLoss (L1) 10.209 (10.792)\n",
      "2022-01-19 02:19:05,483 | Val: [0/9]\tTime 11.751 (11.751)\tLoss (MSE) 286.656 (286.656)\tLoss (L1) 13.757 (13.757)\n",
      "2022-01-19 02:19:15,070 | Val: [1/9]\tTime  9.588 (10.669)\tLoss (MSE) 254.060 (270.358)\tLoss (L1) 12.790 (13.273)\n",
      "2022-01-19 02:19:24,568 | Val: [2/9]\tTime  9.498 (10.279)\tLoss (MSE) 289.735 (276.817)\tLoss (L1) 13.550 (13.366)\n",
      "2022-01-19 02:19:34,159 | Val: [3/9]\tTime  9.592 (10.107)\tLoss (MSE) 265.316 (273.942)\tLoss (L1) 13.112 (13.302)\n",
      "2022-01-19 02:19:43,785 | Val: [4/9]\tTime  9.626 (10.011)\tLoss (MSE) 291.789 (277.511)\tLoss (L1) 14.171 (13.476)\n",
      "2022-01-19 02:19:53,357 | Val: [5/9]\tTime  9.572 ( 9.938)\tLoss (MSE) 299.149 (281.117)\tLoss (L1) 13.836 (13.536)\n",
      "2022-01-19 02:20:02,983 | Val: [6/9]\tTime  9.625 ( 9.893)\tLoss (MSE) 268.570 (279.325)\tLoss (L1) 13.128 (13.478)\n",
      "2022-01-19 02:20:12,541 | Val: [7/9]\tTime  9.559 ( 9.851)\tLoss (MSE) 314.819 (283.762)\tLoss (L1) 14.143 (13.561)\n",
      "2022-01-19 02:20:15,843 | Val: [8/9]\tTime  3.301 ( 9.123)\tLoss (MSE) 300.042 (284.461)\tLoss (L1) 12.909 (13.533)\n",
      "2022-01-19 02:20:15,970 |  * Overall: MSE 284.461\tL1 13.533\tG-Mean 9.099\n",
      "2022-01-19 02:20:15,971 |  * Many: MSE 201.471\tL1 11.298\tG-Mean 7.511\n",
      "2022-01-19 02:20:15,971 |  * Median: MSE 430.778\tL1 17.668\tG-Mean 13.163\n",
      "2022-01-19 02:20:15,972 |  * Low: MSE 663.630\tL1 23.208\tG-Mean 20.062\n",
      "2022-01-19 02:20:15,977 | Best L1 Loss: 13.533\n",
      "2022-01-19 02:20:17,243 | ===> Saving current best checkpoint...\n",
      "2022-01-19 02:20:18,397 | Epoch #4: Train loss [10.7920]; Val loss: MSE [284.4614], L1 [13.5328], G-Mean [9.0991]\n",
      "2022-01-19 02:20:18,398 | Training...\n",
      "2022-01-19 02:20:18,399 | Load train loader\n",
      "2022-01-19 02:20:19,678 | ===> Batch : 1\n",
      "2022-01-19 02:20:19,679 | FDS disable\n",
      "2022-01-19 02:20:33,738 | Calculate Loss\n",
      "2022-01-19 02:20:33,739 | Update Loss\n",
      "2022-01-19 02:20:33,861 | Backward\n",
      "2022-01-19 02:20:55,866 | Epoch: [5][ 1/48]\tTime  37.47 ( 37.47)\tData 1.2784 (1.2784)\tLoss (L1) 11.767 (11.767)\n",
      "2022-01-19 02:20:55,879 | ===> Batch : 2\n",
      "2022-01-19 02:20:55,880 | FDS disable\n",
      "2022-01-19 02:21:07,710 | Calculate Loss\n",
      "2022-01-19 02:21:07,712 | Update Loss\n",
      "2022-01-19 02:21:07,716 | Backward\n",
      "2022-01-19 02:21:29,188 | Epoch: [5][ 2/48]\tTime  33.32 ( 35.39)\tData 0.0131 (0.6458)\tLoss (L1) 9.941 (10.854)\n",
      "2022-01-19 02:21:29,201 | ===> Batch : 3\n",
      "2022-01-19 02:21:29,202 | FDS disable\n",
      "2022-01-19 02:21:41,007 | Calculate Loss\n",
      "2022-01-19 02:21:41,009 | Update Loss\n",
      "2022-01-19 02:21:41,012 | Backward\n",
      "2022-01-19 02:22:02,306 | Epoch: [5][ 3/48]\tTime  33.12 ( 34.64)\tData 0.0132 (0.4349)\tLoss (L1) 11.304 (11.004)\n",
      "2022-01-19 02:22:02,320 | ===> Batch : 4\n",
      "2022-01-19 02:22:02,320 | FDS disable\n",
      "2022-01-19 02:22:14,144 | Calculate Loss\n",
      "2022-01-19 02:22:14,148 | Update Loss\n",
      "2022-01-19 02:22:14,151 | Backward\n",
      "2022-01-19 02:22:35,389 | Epoch: [5][ 4/48]\tTime  33.08 ( 34.25)\tData 0.0137 (0.3296)\tLoss (L1) 10.000 (10.753)\n",
      "2022-01-19 02:22:35,403 | ===> Batch : 5\n",
      "2022-01-19 02:22:35,403 | FDS disable\n",
      "2022-01-19 02:22:47,195 | Calculate Loss\n",
      "2022-01-19 02:22:47,196 | Update Loss\n",
      "2022-01-19 02:22:47,200 | Backward\n",
      "2022-01-19 02:23:08,104 | Epoch: [5][ 5/48]\tTime  32.71 ( 33.94)\tData 0.0134 (0.2664)\tLoss (L1) 10.754 (10.753)\n",
      "2022-01-19 02:23:08,117 | ===> Batch : 6\n",
      "2022-01-19 02:23:08,117 | FDS disable\n",
      "2022-01-19 02:23:19,958 | Calculate Loss\n",
      "2022-01-19 02:23:19,959 | Update Loss\n",
      "2022-01-19 02:23:19,963 | Backward\n",
      "2022-01-19 02:23:41,470 | Epoch: [5][ 6/48]\tTime  33.37 ( 33.85)\tData 0.0129 (0.2241)\tLoss (L1) 11.850 (10.936)\n",
      "2022-01-19 02:23:41,484 | ===> Batch : 7\n",
      "2022-01-19 02:23:41,484 | FDS disable\n",
      "2022-01-19 02:23:53,315 | Calculate Loss\n",
      "2022-01-19 02:23:53,317 | Update Loss\n",
      "2022-01-19 02:23:53,320 | Backward\n",
      "2022-01-19 02:24:14,713 | Epoch: [5][ 7/48]\tTime  33.24 ( 33.76)\tData 0.0137 (0.1941)\tLoss (L1) 10.175 (10.827)\n",
      "2022-01-19 02:24:14,726 | ===> Batch : 8\n",
      "2022-01-19 02:24:14,727 | FDS disable\n",
      "2022-01-19 02:24:26,560 | Calculate Loss\n",
      "2022-01-19 02:24:26,561 | Update Loss\n",
      "2022-01-19 02:24:26,565 | Backward\n",
      "2022-01-19 02:24:48,138 | Epoch: [5][ 8/48]\tTime  33.43 ( 33.72)\tData 0.0130 (0.1714)\tLoss (L1) 10.430 (10.778)\n",
      "2022-01-19 02:24:48,157 | ===> Batch : 9\n",
      "2022-01-19 02:24:48,157 | FDS disable\n",
      "2022-01-19 02:24:59,993 | Calculate Loss\n",
      "2022-01-19 02:24:59,994 | Update Loss\n",
      "2022-01-19 02:24:59,998 | Backward\n",
      "2022-01-19 02:25:21,099 | Epoch: [5][ 9/48]\tTime  32.96 ( 33.63)\tData 0.0186 (0.1544)\tLoss (L1) 11.243 (10.829)\n",
      "2022-01-19 02:25:21,111 | ===> Batch : 10\n",
      "2022-01-19 02:25:21,112 | FDS disable\n",
      "2022-01-19 02:25:32,942 | Calculate Loss\n",
      "2022-01-19 02:25:32,943 | Update Loss\n",
      "2022-01-19 02:25:32,946 | Backward\n",
      "2022-01-19 02:25:54,049 | Epoch: [5][10/48]\tTime  32.95 ( 33.56)\tData 0.0127 (0.1403)\tLoss (L1) 11.161 (10.863)\n",
      "2022-01-19 02:25:54,062 | ===> Batch : 11\n",
      "2022-01-19 02:25:54,063 | FDS disable\n",
      "2022-01-19 02:26:05,896 | Calculate Loss\n",
      "2022-01-19 02:26:05,897 | Update Loss\n",
      "2022-01-19 02:26:05,901 | Backward\n",
      "2022-01-19 02:26:26,922 | Epoch: [5][11/48]\tTime  32.87 ( 33.50)\tData 0.0131 (0.1287)\tLoss (L1) 10.971 (10.872)\n",
      "2022-01-19 02:26:26,938 | ===> Batch : 12\n",
      "2022-01-19 02:26:26,938 | FDS disable\n",
      "2022-01-19 02:26:38,779 | Calculate Loss\n",
      "2022-01-19 02:26:38,780 | Update Loss\n",
      "2022-01-19 02:26:38,784 | Backward\n",
      "2022-01-19 02:27:00,168 | Epoch: [5][12/48]\tTime  33.25 ( 33.48)\tData 0.0161 (0.1193)\tLoss (L1) 11.151 (10.896)\n",
      "2022-01-19 02:27:00,181 | ===> Batch : 13\n",
      "2022-01-19 02:27:00,182 | FDS disable\n",
      "2022-01-19 02:27:12,017 | Calculate Loss\n",
      "2022-01-19 02:27:12,019 | Update Loss\n",
      "2022-01-19 02:27:12,023 | Backward\n",
      "2022-01-19 02:27:32,926 | Epoch: [5][13/48]\tTime  32.76 ( 33.43)\tData 0.0129 (0.1111)\tLoss (L1) 9.484 (10.787)\n",
      "2022-01-19 02:27:32,939 | ===> Batch : 14\n",
      "2022-01-19 02:27:32,939 | FDS disable\n",
      "2022-01-19 02:27:44,791 | Calculate Loss\n",
      "2022-01-19 02:27:44,792 | Update Loss\n",
      "2022-01-19 02:27:44,796 | Backward\n",
      "2022-01-19 02:28:05,995 | Epoch: [5][14/48]\tTime  33.07 ( 33.40)\tData 0.0132 (0.1042)\tLoss (L1) 10.260 (10.749)\n",
      "2022-01-19 02:28:06,008 | ===> Batch : 15\n",
      "2022-01-19 02:28:06,008 | FDS disable\n",
      "2022-01-19 02:28:17,813 | Calculate Loss\n",
      "2022-01-19 02:28:17,814 | Update Loss\n",
      "2022-01-19 02:28:17,818 | Backward\n",
      "2022-01-19 02:28:39,180 | Epoch: [5][15/48]\tTime  33.18 ( 33.39)\tData 0.0127 (0.0981)\tLoss (L1) 10.792 (10.752)\n",
      "2022-01-19 02:28:39,193 | ===> Batch : 16\n",
      "2022-01-19 02:28:39,193 | FDS disable\n",
      "2022-01-19 02:28:51,025 | Calculate Loss\n",
      "2022-01-19 02:28:51,026 | Update Loss\n",
      "2022-01-19 02:28:51,030 | Backward\n",
      "2022-01-19 02:29:11,733 | Epoch: [5][16/48]\tTime  32.55 ( 33.33)\tData 0.0131 (0.0927)\tLoss (L1) 11.323 (10.788)\n",
      "2022-01-19 02:29:11,748 | ===> Batch : 17\n",
      "2022-01-19 02:29:11,748 | FDS disable\n",
      "2022-01-19 02:29:23,575 | Calculate Loss\n",
      "2022-01-19 02:29:23,576 | Update Loss\n",
      "2022-01-19 02:29:23,580 | Backward\n",
      "2022-01-19 02:29:45,336 | Epoch: [5][17/48]\tTime  33.60 ( 33.35)\tData 0.0147 (0.0882)\tLoss (L1) 11.842 (10.850)\n",
      "2022-01-19 02:29:45,350 | ===> Batch : 18\n",
      "2022-01-19 02:29:45,351 | FDS disable\n",
      "2022-01-19 02:29:57,137 | Calculate Loss\n",
      "2022-01-19 02:29:57,138 | Update Loss\n",
      "2022-01-19 02:29:57,142 | Backward\n",
      "2022-01-19 02:30:18,316 | Epoch: [5][18/48]\tTime  32.98 ( 33.33)\tData 0.0140 (0.0840)\tLoss (L1) 10.420 (10.826)\n",
      "2022-01-19 02:30:18,331 | ===> Batch : 19\n",
      "2022-01-19 02:30:18,332 | FDS disable\n",
      "2022-01-19 02:30:30,126 | Calculate Loss\n",
      "2022-01-19 02:30:30,128 | Update Loss\n",
      "2022-01-19 02:30:30,131 | Backward\n",
      "2022-01-19 02:30:51,570 | Epoch: [5][19/48]\tTime  33.25 ( 33.32)\tData 0.0148 (0.0804)\tLoss (L1) 10.693 (10.819)\n",
      "2022-01-19 02:30:51,585 | ===> Batch : 20\n",
      "2022-01-19 02:30:51,585 | FDS disable\n",
      "2022-01-19 02:31:03,427 | Calculate Loss\n",
      "2022-01-19 02:31:03,428 | Update Loss\n",
      "2022-01-19 02:31:03,431 | Backward\n",
      "2022-01-19 02:31:24,423 | Epoch: [5][20/48]\tTime  32.85 ( 33.30)\tData 0.0142 (0.0771)\tLoss (L1) 11.097 (10.833)\n",
      "2022-01-19 02:31:24,437 | ===> Batch : 21\n",
      "2022-01-19 02:31:24,437 | FDS disable\n",
      "2022-01-19 02:31:36,286 | Calculate Loss\n",
      "2022-01-19 02:31:36,289 | Update Loss\n",
      "2022-01-19 02:31:36,293 | Backward\n",
      "2022-01-19 02:31:57,334 | Epoch: [5][21/48]\tTime  32.91 ( 33.28)\tData 0.0141 (0.0741)\tLoss (L1) 10.689 (10.826)\n",
      "2022-01-19 02:31:57,351 | ===> Batch : 22\n",
      "2022-01-19 02:31:57,351 | FDS disable\n",
      "2022-01-19 02:32:09,239 | Calculate Loss\n",
      "2022-01-19 02:32:09,241 | Update Loss\n",
      "2022-01-19 02:32:09,244 | Backward\n",
      "2022-01-19 02:32:30,826 | Epoch: [5][22/48]\tTime  33.49 ( 33.29)\tData 0.0162 (0.0715)\tLoss (L1) 10.738 (10.822)\n",
      "2022-01-19 02:32:30,840 | ===> Batch : 23\n",
      "2022-01-19 02:32:30,840 | FDS disable\n",
      "2022-01-19 02:32:42,574 | Calculate Loss\n",
      "2022-01-19 02:32:42,575 | Update Loss\n",
      "2022-01-19 02:32:42,578 | Backward\n",
      "2022-01-19 02:33:03,463 | Epoch: [5][23/48]\tTime  32.64 ( 33.26)\tData 0.0135 (0.0689)\tLoss (L1) 9.422 (10.761)\n",
      "2022-01-19 02:33:03,478 | ===> Batch : 24\n",
      "2022-01-19 02:33:03,479 | FDS disable\n",
      "2022-01-19 02:33:15,283 | Calculate Loss\n",
      "2022-01-19 02:33:15,284 | Update Loss\n",
      "2022-01-19 02:33:15,288 | Backward\n",
      "2022-01-19 02:33:36,460 | Epoch: [5][24/48]\tTime  33.00 ( 33.25)\tData 0.0152 (0.0667)\tLoss (L1) 9.697 (10.717)\n",
      "2022-01-19 02:33:36,476 | ===> Batch : 25\n",
      "2022-01-19 02:33:36,476 | FDS disable\n",
      "2022-01-19 02:33:48,333 | Calculate Loss\n",
      "2022-01-19 02:33:48,334 | Update Loss\n",
      "2022-01-19 02:33:48,338 | Backward\n",
      "2022-01-19 02:34:09,521 | Epoch: [5][25/48]\tTime  33.06 ( 33.24)\tData 0.0158 (0.0647)\tLoss (L1) 10.244 (10.698)\n",
      "2022-01-19 02:34:09,536 | ===> Batch : 26\n",
      "2022-01-19 02:34:09,536 | FDS disable\n",
      "2022-01-19 02:34:21,367 | Calculate Loss\n",
      "2022-01-19 02:34:21,369 | Update Loss\n",
      "2022-01-19 02:34:21,372 | Backward\n",
      "2022-01-19 02:34:42,982 | Epoch: [5][26/48]\tTime  33.46 ( 33.25)\tData 0.0147 (0.0627)\tLoss (L1) 10.529 (10.691)\n",
      "2022-01-19 02:34:42,995 | ===> Batch : 27\n",
      "2022-01-19 02:34:42,996 | FDS disable\n",
      "2022-01-19 02:34:54,849 | Calculate Loss\n",
      "2022-01-19 02:34:54,850 | Update Loss\n",
      "2022-01-19 02:34:54,854 | Backward\n",
      "2022-01-19 02:35:15,911 | Epoch: [5][27/48]\tTime  32.93 ( 33.24)\tData 0.0140 (0.0609)\tLoss (L1) 10.430 (10.682)\n",
      "2022-01-19 02:35:15,925 | ===> Batch : 28\n",
      "2022-01-19 02:35:15,926 | FDS disable\n",
      "2022-01-19 02:35:27,832 | Calculate Loss\n",
      "2022-01-19 02:35:27,833 | Update Loss\n",
      "2022-01-19 02:35:27,837 | Backward\n",
      "2022-01-19 02:35:49,138 | Epoch: [5][28/48]\tTime  33.23 ( 33.24)\tData 0.0139 (0.0593)\tLoss (L1) 10.550 (10.677)\n",
      "2022-01-19 02:35:49,155 | ===> Batch : 29\n",
      "2022-01-19 02:35:49,156 | FDS disable\n",
      "2022-01-19 02:36:00,986 | Calculate Loss\n",
      "2022-01-19 02:36:00,988 | Update Loss\n",
      "2022-01-19 02:36:00,992 | Backward\n",
      "2022-01-19 02:36:22,695 | Epoch: [5][29/48]\tTime  33.56 ( 33.25)\tData 0.0177 (0.0578)\tLoss (L1) 10.974 (10.687)\n",
      "2022-01-19 02:36:22,710 | ===> Batch : 30\n",
      "2022-01-19 02:36:22,711 | FDS disable\n",
      "2022-01-19 02:36:34,624 | Calculate Loss\n",
      "2022-01-19 02:36:34,625 | Update Loss\n",
      "2022-01-19 02:36:34,629 | Backward\n",
      "2022-01-19 02:36:55,257 | Epoch: [5][30/48]\tTime  32.56 ( 33.23)\tData 0.0150 (0.0564)\tLoss (L1) 11.214 (10.705)\n",
      "2022-01-19 02:36:55,272 | ===> Batch : 31\n",
      "2022-01-19 02:36:55,273 | FDS disable\n",
      "2022-01-19 02:37:07,122 | Calculate Loss\n",
      "2022-01-19 02:37:07,124 | Update Loss\n",
      "2022-01-19 02:37:07,128 | Backward\n",
      "2022-01-19 02:37:28,699 | Epoch: [5][31/48]\tTime  33.44 ( 33.24)\tData 0.0158 (0.0551)\tLoss (L1) 10.440 (10.696)\n",
      "2022-01-19 02:37:28,713 | ===> Batch : 32\n",
      "2022-01-19 02:37:28,714 | FDS disable\n",
      "2022-01-19 02:37:40,485 | Calculate Loss\n",
      "2022-01-19 02:37:40,486 | Update Loss\n",
      "2022-01-19 02:37:40,489 | Backward\n",
      "2022-01-19 02:38:02,060 | Epoch: [5][32/48]\tTime  33.36 ( 33.24)\tData 0.0143 (0.0538)\tLoss (L1) 11.359 (10.717)\n",
      "2022-01-19 02:38:02,076 | ===> Batch : 33\n",
      "2022-01-19 02:38:02,076 | FDS disable\n",
      "2022-01-19 02:38:13,773 | Calculate Loss\n",
      "2022-01-19 02:38:13,774 | Update Loss\n",
      "2022-01-19 02:38:13,777 | Backward\n",
      "2022-01-19 02:38:34,941 | Epoch: [5][33/48]\tTime  32.88 ( 33.23)\tData 0.0153 (0.0526)\tLoss (L1) 10.588 (10.713)\n",
      "2022-01-19 02:38:34,957 | ===> Batch : 34\n",
      "2022-01-19 02:38:34,958 | FDS disable\n",
      "2022-01-19 02:38:46,667 | Calculate Loss\n",
      "2022-01-19 02:38:46,668 | Update Loss\n",
      "2022-01-19 02:38:46,672 | Backward\n",
      "2022-01-19 02:39:07,804 | Epoch: [5][34/48]\tTime  32.86 ( 33.22)\tData 0.0156 (0.0515)\tLoss (L1) 10.857 (10.717)\n",
      "2022-01-19 02:39:07,818 | ===> Batch : 35\n",
      "2022-01-19 02:39:07,818 | FDS disable\n",
      "2022-01-19 02:39:19,504 | Calculate Loss\n",
      "2022-01-19 02:39:19,505 | Update Loss\n",
      "2022-01-19 02:39:19,509 | Backward\n",
      "2022-01-19 02:39:40,618 | Epoch: [5][35/48]\tTime  32.81 ( 33.21)\tData 0.0137 (0.0505)\tLoss (L1) 10.679 (10.716)\n",
      "2022-01-19 02:39:40,633 | ===> Batch : 36\n",
      "2022-01-19 02:39:40,633 | FDS disable\n",
      "2022-01-19 02:39:52,407 | Calculate Loss\n",
      "2022-01-19 02:39:52,408 | Update Loss\n",
      "2022-01-19 02:39:52,412 | Backward\n",
      "2022-01-19 02:40:14,069 | Epoch: [5][36/48]\tTime  33.45 ( 33.21)\tData 0.0150 (0.0495)\tLoss (L1) 10.797 (10.718)\n",
      "2022-01-19 02:40:14,084 | ===> Batch : 37\n",
      "2022-01-19 02:40:14,084 | FDS disable\n",
      "2022-01-19 02:40:25,842 | Calculate Loss\n",
      "2022-01-19 02:40:25,843 | Update Loss\n",
      "2022-01-19 02:40:25,847 | Backward\n",
      "2022-01-19 02:40:46,914 | Epoch: [5][37/48]\tTime  32.84 ( 33.20)\tData 0.0150 (0.0486)\tLoss (L1) 10.119 (10.702)\n",
      "2022-01-19 02:40:46,928 | ===> Batch : 38\n",
      "2022-01-19 02:40:46,928 | FDS disable\n",
      "2022-01-19 02:40:58,670 | Calculate Loss\n",
      "2022-01-19 02:40:58,672 | Update Loss\n",
      "2022-01-19 02:40:58,676 | Backward\n",
      "2022-01-19 02:41:19,743 | Epoch: [5][38/48]\tTime  32.83 ( 33.19)\tData 0.0141 (0.0476)\tLoss (L1) 10.150 (10.688)\n",
      "2022-01-19 02:41:19,757 | ===> Batch : 39\n",
      "2022-01-19 02:41:19,757 | FDS disable\n",
      "2022-01-19 02:41:31,479 | Calculate Loss\n",
      "2022-01-19 02:41:31,480 | Update Loss\n",
      "2022-01-19 02:41:31,485 | Backward\n",
      "2022-01-19 02:41:53,053 | Epoch: [5][39/48]\tTime  33.31 ( 33.20)\tData 0.0136 (0.0468)\tLoss (L1) 10.912 (10.693)\n",
      "2022-01-19 02:41:53,066 | ===> Batch : 40\n",
      "2022-01-19 02:41:53,067 | FDS disable\n",
      "2022-01-19 02:42:04,783 | Calculate Loss\n",
      "2022-01-19 02:42:04,784 | Update Loss\n",
      "2022-01-19 02:42:04,787 | Backward\n",
      "2022-01-19 02:42:25,697 | Epoch: [5][40/48]\tTime  32.64 ( 33.18)\tData 0.0138 (0.0459)\tLoss (L1) 10.358 (10.685)\n",
      "2022-01-19 02:42:25,712 | ===> Batch : 41\n",
      "2022-01-19 02:42:25,712 | FDS disable\n",
      "2022-01-19 02:42:37,464 | Calculate Loss\n",
      "2022-01-19 02:42:37,466 | Update Loss\n",
      "2022-01-19 02:42:37,470 | Backward\n",
      "2022-01-19 02:42:58,554 | Epoch: [5][41/48]\tTime  32.86 ( 33.17)\tData 0.0141 (0.0452)\tLoss (L1) 9.868 (10.665)\n",
      "2022-01-19 02:42:58,568 | ===> Batch : 42\n",
      "2022-01-19 02:42:58,569 | FDS disable\n",
      "2022-01-19 02:43:10,286 | Calculate Loss\n",
      "2022-01-19 02:43:10,288 | Update Loss\n",
      "2022-01-19 02:43:10,291 | Backward\n",
      "2022-01-19 02:43:31,449 | Epoch: [5][42/48]\tTime  32.89 ( 33.17)\tData 0.0138 (0.0444)\tLoss (L1) 10.339 (10.657)\n",
      "2022-01-19 02:43:31,463 | ===> Batch : 43\n",
      "2022-01-19 02:43:31,464 | FDS disable\n",
      "2022-01-19 02:43:43,129 | Calculate Loss\n",
      "2022-01-19 02:43:43,130 | Update Loss\n",
      "2022-01-19 02:43:43,134 | Backward\n",
      "2022-01-19 02:44:04,635 | Epoch: [5][43/48]\tTime  33.19 ( 33.17)\tData 0.0144 (0.0437)\tLoss (L1) 10.772 (10.660)\n",
      "2022-01-19 02:44:04,649 | ===> Batch : 44\n",
      "2022-01-19 02:44:04,649 | FDS disable\n",
      "2022-01-19 02:44:16,383 | Calculate Loss\n",
      "2022-01-19 02:44:16,384 | Update Loss\n",
      "2022-01-19 02:44:16,388 | Backward\n",
      "2022-01-19 02:44:37,883 | Epoch: [5][44/48]\tTime  33.25 ( 33.17)\tData 0.0139 (0.0431)\tLoss (L1) 10.849 (10.664)\n",
      "2022-01-19 02:44:37,897 | ===> Batch : 45\n",
      "2022-01-19 02:44:37,898 | FDS disable\n",
      "2022-01-19 02:44:49,607 | Calculate Loss\n",
      "2022-01-19 02:44:49,608 | Update Loss\n",
      "2022-01-19 02:44:49,613 | Backward\n",
      "2022-01-19 02:45:11,199 | Epoch: [5][45/48]\tTime  33.32 ( 33.17)\tData 0.0142 (0.0424)\tLoss (L1) 10.740 (10.666)\n",
      "2022-01-19 02:45:11,215 | ===> Batch : 46\n",
      "2022-01-19 02:45:11,216 | FDS disable\n",
      "2022-01-19 02:45:22,962 | Calculate Loss\n",
      "2022-01-19 02:45:22,963 | Update Loss\n",
      "2022-01-19 02:45:22,967 | Backward\n",
      "2022-01-19 02:45:44,141 | Epoch: [5][46/48]\tTime  32.94 ( 33.17)\tData 0.0162 (0.0418)\tLoss (L1) 10.632 (10.665)\n",
      "2022-01-19 02:45:44,155 | ===> Batch : 47\n",
      "2022-01-19 02:45:44,155 | FDS disable\n",
      "2022-01-19 02:45:55,901 | Calculate Loss\n",
      "2022-01-19 02:45:55,902 | Update Loss\n",
      "2022-01-19 02:45:55,906 | Backward\n",
      "2022-01-19 02:46:17,545 | Epoch: [5][47/48]\tTime  33.40 ( 33.17)\tData 0.0139 (0.0412)\tLoss (L1) 10.240 (10.656)\n",
      "2022-01-19 02:46:17,560 | ===> Batch : 48\n",
      "2022-01-19 02:46:17,560 | FDS disable\n",
      "2022-01-19 02:46:26,206 | Calculate Loss\n",
      "2022-01-19 02:46:26,207 | Update Loss\n",
      "2022-01-19 02:46:26,211 | Backward\n",
      "2022-01-19 02:46:40,535 | Epoch: [5][48/48]\tTime  22.99 ( 32.96)\tData 0.0146 (0.0407)\tLoss (L1) 11.082 (10.662)\n",
      "2022-01-19 02:46:52,428 | Val: [0/9]\tTime 11.730 (11.730)\tLoss (MSE) 326.997 (326.997)\tLoss (L1) 14.683 (14.683)\n",
      "2022-01-19 02:47:01,951 | Val: [1/9]\tTime  9.523 (10.626)\tLoss (MSE) 309.496 (318.246)\tLoss (L1) 14.447 (14.565)\n",
      "2022-01-19 02:47:11,574 | Val: [2/9]\tTime  9.623 (10.292)\tLoss (MSE) 368.614 (335.036)\tLoss (L1) 16.163 (15.097)\n",
      "2022-01-19 02:47:21,127 | Val: [3/9]\tTime  9.553 (10.107)\tLoss (MSE) 332.576 (334.421)\tLoss (L1) 15.127 (15.105)\n",
      "2022-01-19 02:47:30,731 | Val: [4/9]\tTime  9.604 (10.006)\tLoss (MSE) 328.739 (333.284)\tLoss (L1) 15.393 (15.163)\n",
      "2022-01-19 02:47:40,341 | Val: [5/9]\tTime  9.610 ( 9.940)\tLoss (MSE) 359.480 (337.650)\tLoss (L1) 15.748 (15.260)\n",
      "2022-01-19 02:47:49,962 | Val: [6/9]\tTime  9.621 ( 9.895)\tLoss (MSE) 329.283 (336.455)\tLoss (L1) 15.239 (15.257)\n",
      "2022-01-19 02:47:59,583 | Val: [7/9]\tTime  9.621 ( 9.861)\tLoss (MSE) 348.548 (337.967)\tLoss (L1) 15.366 (15.271)\n",
      "2022-01-19 02:48:02,883 | Val: [8/9]\tTime  3.300 ( 9.132)\tLoss (MSE) 312.602 (336.876)\tLoss (L1) 14.363 (15.232)\n",
      "2022-01-19 02:48:03,001 |  * Overall: MSE 336.876\tL1 15.232\tG-Mean 10.692\n",
      "2022-01-19 02:48:03,002 |  * Many: MSE 173.740\tL1 10.929\tG-Mean 7.670\n",
      "2022-01-19 02:48:03,002 |  * Median: MSE 671.832\tL1 24.673\tG-Mean 23.272\n",
      "2022-01-19 02:48:03,002 |  * Low: MSE 951.309\tL1 29.765\tG-Mean 28.673\n",
      "2022-01-19 02:48:03,007 | Best L1 Loss: 13.533\n",
      "2022-01-19 02:48:04,277 | Epoch #5: Train loss [10.6624]; Val loss: MSE [336.8761], L1 [15.2317], G-Mean [10.6922]\n",
      "2022-01-19 02:48:04,279 | Training...\n",
      "2022-01-19 02:48:04,280 | Load train loader\n",
      "2022-01-19 02:48:05,443 | ===> Batch : 1\n",
      "2022-01-19 02:48:05,445 | FDS disable\n",
      "2022-01-19 02:48:19,592 | Calculate Loss\n",
      "2022-01-19 02:48:19,593 | Update Loss\n",
      "2022-01-19 02:48:19,718 | Backward\n",
      "2022-01-19 02:48:41,738 | Epoch: [6][ 1/48]\tTime  37.46 ( 37.46)\tData 1.1637 (1.1637)\tLoss (L1) 10.807 (10.807)\n",
      "2022-01-19 02:48:41,754 | ===> Batch : 2\n",
      "2022-01-19 02:48:41,754 | FDS disable\n",
      "2022-01-19 02:48:53,595 | Calculate Loss\n",
      "2022-01-19 02:48:53,597 | Update Loss\n",
      "2022-01-19 02:48:53,600 | Backward\n",
      "2022-01-19 02:49:15,238 | Epoch: [6][ 2/48]\tTime  33.50 ( 35.48)\tData 0.0158 (0.5898)\tLoss (L1) 11.323 (11.065)\n",
      "2022-01-19 02:49:15,252 | ===> Batch : 3\n",
      "2022-01-19 02:49:15,252 | FDS disable\n",
      "2022-01-19 02:49:27,093 | Calculate Loss\n",
      "2022-01-19 02:49:27,094 | Update Loss\n",
      "2022-01-19 02:49:27,098 | Backward\n",
      "2022-01-19 02:49:47,955 | Epoch: [6][ 3/48]\tTime  32.72 ( 34.56)\tData 0.0134 (0.3976)\tLoss (L1) 10.802 (10.977)\n",
      "2022-01-19 02:49:47,973 | ===> Batch : 4\n",
      "2022-01-19 02:49:47,974 | FDS disable\n",
      "2022-01-19 02:49:59,799 | Calculate Loss\n",
      "2022-01-19 02:49:59,801 | Update Loss\n",
      "2022-01-19 02:49:59,804 | Backward\n",
      "2022-01-19 02:50:21,337 | Epoch: [6][ 4/48]\tTime  33.38 ( 34.26)\tData 0.0183 (0.3028)\tLoss (L1) 11.285 (11.054)\n",
      "2022-01-19 02:50:21,351 | ===> Batch : 5\n",
      "2022-01-19 02:50:21,352 | FDS disable\n",
      "2022-01-19 02:50:33,173 | Calculate Loss\n",
      "2022-01-19 02:50:33,174 | Update Loss\n",
      "2022-01-19 02:50:33,178 | Backward\n",
      "2022-01-19 02:50:54,019 | Epoch: [6][ 5/48]\tTime  32.68 ( 33.95)\tData 0.0138 (0.2450)\tLoss (L1) 10.686 (10.981)\n",
      "2022-01-19 02:50:54,032 | ===> Batch : 6\n",
      "2022-01-19 02:50:54,033 | FDS disable\n",
      "2022-01-19 02:51:05,811 | Calculate Loss\n",
      "2022-01-19 02:51:05,812 | Update Loss\n",
      "2022-01-19 02:51:05,816 | Backward\n",
      "2022-01-19 02:51:27,091 | Epoch: [6][ 6/48]\tTime  33.07 ( 33.80)\tData 0.0135 (0.2064)\tLoss (L1) 9.868 (10.795)\n",
      "2022-01-19 02:51:27,104 | ===> Batch : 7\n",
      "2022-01-19 02:51:27,105 | FDS disable\n",
      "2022-01-19 02:51:38,978 | Calculate Loss\n",
      "2022-01-19 02:51:38,979 | Update Loss\n",
      "2022-01-19 02:51:38,982 | Backward\n",
      "2022-01-19 02:52:00,532 | Epoch: [6][ 7/48]\tTime  33.44 ( 33.75)\tData 0.0130 (0.1788)\tLoss (L1) 10.259 (10.719)\n",
      "2022-01-19 02:52:00,544 | ===> Batch : 8\n",
      "2022-01-19 02:52:00,545 | FDS disable\n",
      "2022-01-19 02:52:12,393 | Calculate Loss\n",
      "2022-01-19 02:52:12,394 | Update Loss\n",
      "2022-01-19 02:52:12,398 | Backward\n",
      "2022-01-19 02:52:33,924 | Epoch: [6][ 8/48]\tTime  33.39 ( 33.71)\tData 0.0129 (0.1580)\tLoss (L1) 10.509 (10.692)\n",
      "2022-01-19 02:52:33,942 | ===> Batch : 9\n",
      "2022-01-19 02:52:33,942 | FDS disable\n",
      "2022-01-19 02:52:45,799 | Calculate Loss\n",
      "2022-01-19 02:52:45,801 | Update Loss\n",
      "2022-01-19 02:52:45,804 | Backward\n",
      "2022-01-19 02:53:07,011 | Epoch: [6][ 9/48]\tTime  33.09 ( 33.64)\tData 0.0179 (0.1425)\tLoss (L1) 10.028 (10.619)\n",
      "2022-01-19 02:53:07,024 | ===> Batch : 10\n",
      "2022-01-19 02:53:07,025 | FDS disable\n",
      "2022-01-19 02:53:18,835 | Calculate Loss\n",
      "2022-01-19 02:53:18,836 | Update Loss\n",
      "2022-01-19 02:53:18,840 | Backward\n",
      "2022-01-19 02:53:40,379 | Epoch: [6][10/48]\tTime  33.37 ( 33.61)\tData 0.0132 (0.1295)\tLoss (L1) 9.788 (10.535)\n",
      "2022-01-19 02:53:40,393 | ===> Batch : 11\n",
      "2022-01-19 02:53:40,394 | FDS disable\n",
      "2022-01-19 02:53:52,206 | Calculate Loss\n",
      "2022-01-19 02:53:52,208 | Update Loss\n",
      "2022-01-19 02:53:52,212 | Backward\n",
      "2022-01-19 02:54:12,773 | Epoch: [6][11/48]\tTime  32.39 ( 33.50)\tData 0.0146 (0.1191)\tLoss (L1) 9.993 (10.486)\n",
      "2022-01-19 02:54:12,786 | ===> Batch : 12\n",
      "2022-01-19 02:54:12,787 | FDS disable\n",
      "2022-01-19 02:54:24,585 | Calculate Loss\n",
      "2022-01-19 02:54:24,588 | Update Loss\n",
      "2022-01-19 02:54:24,592 | Backward\n",
      "2022-01-19 02:54:46,253 | Epoch: [6][12/48]\tTime  33.48 ( 33.50)\tData 0.0131 (0.1103)\tLoss (L1) 9.668 (10.418)\n",
      "2022-01-19 02:54:46,266 | ===> Batch : 13\n",
      "2022-01-19 02:54:46,267 | FDS disable\n",
      "2022-01-19 02:54:58,152 | Calculate Loss\n",
      "2022-01-19 02:54:58,153 | Update Loss\n",
      "2022-01-19 02:54:58,157 | Backward\n",
      "2022-01-19 02:55:19,265 | Epoch: [6][13/48]\tTime  33.01 ( 33.46)\tData 0.0128 (0.1028)\tLoss (L1) 10.865 (10.452)\n",
      "2022-01-19 02:55:19,280 | ===> Batch : 14\n",
      "2022-01-19 02:55:19,281 | FDS disable\n",
      "2022-01-19 02:55:31,164 | Calculate Loss\n",
      "2022-01-19 02:55:31,165 | Update Loss\n",
      "2022-01-19 02:55:31,169 | Backward\n",
      "2022-01-19 02:55:52,813 | Epoch: [6][14/48]\tTime  33.55 ( 33.47)\tData 0.0155 (0.0965)\tLoss (L1) 10.560 (10.460)\n",
      "2022-01-19 02:55:52,826 | ===> Batch : 15\n",
      "2022-01-19 02:55:52,827 | FDS disable\n",
      "2022-01-19 02:56:04,670 | Calculate Loss\n",
      "2022-01-19 02:56:04,671 | Update Loss\n",
      "2022-01-19 02:56:04,675 | Backward\n",
      "2022-01-19 02:56:25,699 | Epoch: [6][15/48]\tTime  32.89 ( 33.43)\tData 0.0128 (0.0910)\tLoss (L1) 10.191 (10.442)\n",
      "2022-01-19 02:56:25,714 | ===> Batch : 16\n",
      "2022-01-19 02:56:25,714 | FDS disable\n",
      "2022-01-19 02:56:37,496 | Calculate Loss\n",
      "2022-01-19 02:56:37,497 | Update Loss\n",
      "2022-01-19 02:56:37,501 | Backward\n",
      "2022-01-19 02:56:58,813 | Epoch: [6][16/48]\tTime  33.11 ( 33.41)\tData 0.0147 (0.0862)\tLoss (L1) 9.935 (10.410)\n",
      "2022-01-19 02:56:58,827 | ===> Batch : 17\n",
      "2022-01-19 02:56:58,828 | FDS disable\n",
      "2022-01-19 02:57:10,744 | Calculate Loss\n",
      "2022-01-19 02:57:10,745 | Update Loss\n",
      "2022-01-19 02:57:10,748 | Backward\n",
      "2022-01-19 02:57:32,311 | Epoch: [6][17/48]\tTime  33.50 ( 33.41)\tData 0.0141 (0.0819)\tLoss (L1) 10.397 (10.410)\n",
      "2022-01-19 02:57:32,325 | ===> Batch : 18\n",
      "2022-01-19 02:57:32,325 | FDS disable\n",
      "2022-01-19 02:57:44,138 | Calculate Loss\n",
      "2022-01-19 02:57:44,139 | Update Loss\n",
      "2022-01-19 02:57:44,143 | Backward\n",
      "2022-01-19 02:58:05,383 | Epoch: [6][18/48]\tTime  33.07 ( 33.39)\tData 0.0139 (0.0782)\tLoss (L1) 10.785 (10.431)\n",
      "2022-01-19 02:58:05,397 | ===> Batch : 19\n",
      "2022-01-19 02:58:05,398 | FDS disable\n",
      "2022-01-19 02:58:17,210 | Calculate Loss\n",
      "2022-01-19 02:58:17,211 | Update Loss\n",
      "2022-01-19 02:58:17,214 | Backward\n",
      "2022-01-19 02:58:38,296 | Epoch: [6][19/48]\tTime  32.91 ( 33.37)\tData 0.0145 (0.0748)\tLoss (L1) 9.250 (10.368)\n",
      "2022-01-19 02:58:38,309 | ===> Batch : 20\n",
      "2022-01-19 02:58:38,310 | FDS disable\n",
      "2022-01-19 02:58:50,130 | Calculate Loss\n",
      "2022-01-19 02:58:50,131 | Update Loss\n",
      "2022-01-19 02:58:50,135 | Backward\n",
      "2022-01-19 02:59:11,291 | Epoch: [6][20/48]\tTime  33.00 ( 33.35)\tData 0.0135 (0.0718)\tLoss (L1) 9.746 (10.337)\n",
      "2022-01-19 02:59:11,308 | ===> Batch : 21\n",
      "2022-01-19 02:59:11,308 | FDS disable\n",
      "2022-01-19 02:59:23,152 | Calculate Loss\n",
      "2022-01-19 02:59:23,153 | Update Loss\n",
      "2022-01-19 02:59:23,157 | Backward\n",
      "2022-01-19 02:59:44,516 | Epoch: [6][21/48]\tTime  33.23 ( 33.34)\tData 0.0165 (0.0691)\tLoss (L1) 10.375 (10.339)\n",
      "2022-01-19 02:59:44,531 | ===> Batch : 22\n",
      "2022-01-19 02:59:44,531 | FDS disable\n",
      "2022-01-19 02:59:56,405 | Calculate Loss\n",
      "2022-01-19 02:59:56,406 | Update Loss\n",
      "2022-01-19 02:59:56,410 | Backward\n",
      "2022-01-19 03:00:17,984 | Epoch: [6][22/48]\tTime  33.47 ( 33.35)\tData 0.0144 (0.0666)\tLoss (L1) 9.803 (10.315)\n",
      "2022-01-19 03:00:17,999 | ===> Batch : 23\n",
      "2022-01-19 03:00:17,999 | FDS disable\n",
      "2022-01-19 03:00:29,824 | Calculate Loss\n",
      "2022-01-19 03:00:29,826 | Update Loss\n",
      "2022-01-19 03:00:29,830 | Backward\n",
      "2022-01-19 03:00:51,502 | Epoch: [6][23/48]\tTime  33.52 ( 33.36)\tData 0.0150 (0.0644)\tLoss (L1) 9.974 (10.300)\n",
      "2022-01-19 03:00:51,517 | ===> Batch : 24\n",
      "2022-01-19 03:00:51,518 | FDS disable\n",
      "2022-01-19 03:01:03,377 | Calculate Loss\n",
      "2022-01-19 03:01:03,378 | Update Loss\n",
      "2022-01-19 03:01:03,382 | Backward\n",
      "2022-01-19 03:01:24,807 | Epoch: [6][24/48]\tTime  33.31 ( 33.36)\tData 0.0152 (0.0623)\tLoss (L1) 9.926 (10.284)\n",
      "2022-01-19 03:01:24,822 | ===> Batch : 25\n",
      "2022-01-19 03:01:24,823 | FDS disable\n",
      "2022-01-19 03:01:36,609 | Calculate Loss\n",
      "2022-01-19 03:01:36,610 | Update Loss\n",
      "2022-01-19 03:01:36,613 | Backward\n",
      "2022-01-19 03:01:58,045 | Epoch: [6][25/48]\tTime  33.24 ( 33.35)\tData 0.0149 (0.0604)\tLoss (L1) 9.717 (10.262)\n",
      "2022-01-19 03:01:58,060 | ===> Batch : 26\n",
      "2022-01-19 03:01:58,061 | FDS disable\n",
      "2022-01-19 03:02:09,844 | Calculate Loss\n",
      "2022-01-19 03:02:09,845 | Update Loss\n",
      "2022-01-19 03:02:09,849 | Backward\n",
      "2022-01-19 03:02:30,916 | Epoch: [6][26/48]\tTime  32.87 ( 33.33)\tData 0.0148 (0.0587)\tLoss (L1) 10.152 (10.257)\n",
      "2022-01-19 03:02:30,931 | ===> Batch : 27\n",
      "2022-01-19 03:02:30,931 | FDS disable\n",
      "2022-01-19 03:02:42,812 | Calculate Loss\n",
      "2022-01-19 03:02:42,814 | Update Loss\n",
      "2022-01-19 03:02:42,817 | Backward\n",
      "2022-01-19 03:03:04,460 | Epoch: [6][27/48]\tTime  33.54 ( 33.34)\tData 0.0149 (0.0571)\tLoss (L1) 9.735 (10.238)\n",
      "2022-01-19 03:03:04,475 | ===> Batch : 28\n",
      "2022-01-19 03:03:04,475 | FDS disable\n",
      "2022-01-19 03:03:16,279 | Calculate Loss\n",
      "2022-01-19 03:03:16,281 | Update Loss\n",
      "2022-01-19 03:03:16,285 | Backward\n",
      "2022-01-19 03:03:37,718 | Epoch: [6][28/48]\tTime  33.26 ( 33.34)\tData 0.0144 (0.0555)\tLoss (L1) 10.299 (10.240)\n",
      "2022-01-19 03:03:37,733 | ===> Batch : 29\n",
      "2022-01-19 03:03:37,733 | FDS disable\n",
      "2022-01-19 03:03:49,524 | Calculate Loss\n",
      "2022-01-19 03:03:49,525 | Update Loss\n",
      "2022-01-19 03:03:49,529 | Backward\n",
      "2022-01-19 03:04:10,788 | Epoch: [6][29/48]\tTime  33.07 ( 33.33)\tData 0.0146 (0.0541)\tLoss (L1) 10.383 (10.245)\n",
      "2022-01-19 03:04:10,802 | ===> Batch : 30\n",
      "2022-01-19 03:04:10,803 | FDS disable\n",
      "2022-01-19 03:04:22,654 | Calculate Loss\n",
      "2022-01-19 03:04:22,655 | Update Loss\n",
      "2022-01-19 03:04:22,659 | Backward\n",
      "2022-01-19 03:04:44,063 | Epoch: [6][30/48]\tTime  33.27 ( 33.33)\tData 0.0139 (0.0528)\tLoss (L1) 10.744 (10.262)\n",
      "2022-01-19 03:04:44,077 | ===> Batch : 31\n",
      "2022-01-19 03:04:44,077 | FDS disable\n",
      "2022-01-19 03:04:55,890 | Calculate Loss\n",
      "2022-01-19 03:04:55,892 | Update Loss\n",
      "2022-01-19 03:04:55,895 | Backward\n",
      "2022-01-19 03:05:17,277 | Epoch: [6][31/48]\tTime  33.21 ( 33.32)\tData 0.0139 (0.0515)\tLoss (L1) 9.685 (10.243)\n",
      "2022-01-19 03:05:17,291 | ===> Batch : 32\n",
      "2022-01-19 03:05:17,292 | FDS disable\n",
      "2022-01-19 03:05:29,022 | Calculate Loss\n",
      "2022-01-19 03:05:29,023 | Update Loss\n",
      "2022-01-19 03:05:29,027 | Backward\n",
      "2022-01-19 03:05:50,213 | Epoch: [6][32/48]\tTime  32.94 ( 33.31)\tData 0.0141 (0.0504)\tLoss (L1) 11.159 (10.272)\n",
      "2022-01-19 03:05:50,227 | ===> Batch : 33\n",
      "2022-01-19 03:05:50,228 | FDS disable\n",
      "2022-01-19 03:06:01,959 | Calculate Loss\n",
      "2022-01-19 03:06:01,960 | Update Loss\n",
      "2022-01-19 03:06:01,964 | Backward\n",
      "2022-01-19 03:06:23,512 | Epoch: [6][33/48]\tTime  33.30 ( 33.31)\tData 0.0141 (0.0493)\tLoss (L1) 9.908 (10.261)\n",
      "2022-01-19 03:06:23,526 | ===> Batch : 34\n",
      "2022-01-19 03:06:23,526 | FDS disable\n",
      "2022-01-19 03:06:35,339 | Calculate Loss\n",
      "2022-01-19 03:06:35,340 | Update Loss\n",
      "2022-01-19 03:06:35,344 | Backward\n",
      "2022-01-19 03:06:55,882 | Epoch: [6][34/48]\tTime  32.37 ( 33.28)\tData 0.0135 (0.0482)\tLoss (L1) 9.343 (10.234)\n",
      "2022-01-19 03:06:55,896 | ===> Batch : 35\n",
      "2022-01-19 03:06:55,897 | FDS disable\n",
      "2022-01-19 03:07:07,591 | Calculate Loss\n",
      "2022-01-19 03:07:07,592 | Update Loss\n",
      "2022-01-19 03:07:07,596 | Backward\n",
      "2022-01-19 03:07:29,119 | Epoch: [6][35/48]\tTime  33.24 ( 33.28)\tData 0.0138 (0.0472)\tLoss (L1) 9.677 (10.218)\n",
      "2022-01-19 03:07:29,133 | ===> Batch : 36\n",
      "2022-01-19 03:07:29,133 | FDS disable\n",
      "2022-01-19 03:07:40,846 | Calculate Loss\n",
      "2022-01-19 03:07:40,847 | Update Loss\n",
      "2022-01-19 03:07:40,851 | Backward\n",
      "2022-01-19 03:08:02,339 | Epoch: [6][36/48]\tTime  33.22 ( 33.28)\tData 0.0143 (0.0463)\tLoss (L1) 9.735 (10.204)\n",
      "2022-01-19 03:08:02,353 | ===> Batch : 37\n",
      "2022-01-19 03:08:02,354 | FDS disable\n",
      "2022-01-19 03:08:14,042 | Calculate Loss\n",
      "2022-01-19 03:08:14,043 | Update Loss\n",
      "2022-01-19 03:08:14,047 | Backward\n",
      "2022-01-19 03:08:35,141 | Epoch: [6][37/48]\tTime  32.80 ( 33.27)\tData 0.0140 (0.0454)\tLoss (L1) 11.379 (10.236)\n",
      "2022-01-19 03:08:35,156 | ===> Batch : 38\n",
      "2022-01-19 03:08:35,157 | FDS disable\n",
      "2022-01-19 03:08:46,835 | Calculate Loss\n",
      "2022-01-19 03:08:46,837 | Update Loss\n",
      "2022-01-19 03:08:46,840 | Backward\n",
      "2022-01-19 03:09:08,038 | Epoch: [6][38/48]\tTime  32.90 ( 33.26)\tData 0.0147 (0.0446)\tLoss (L1) 9.617 (10.220)\n",
      "2022-01-19 03:09:08,053 | ===> Batch : 39\n",
      "2022-01-19 03:09:08,053 | FDS disable\n",
      "2022-01-19 03:09:19,759 | Calculate Loss\n",
      "2022-01-19 03:09:19,761 | Update Loss\n",
      "2022-01-19 03:09:19,765 | Backward\n",
      "2022-01-19 03:09:41,281 | Epoch: [6][39/48]\tTime  33.24 ( 33.26)\tData 0.0144 (0.0439)\tLoss (L1) 10.334 (10.223)\n",
      "2022-01-19 03:09:41,297 | ===> Batch : 40\n",
      "2022-01-19 03:09:41,298 | FDS disable\n",
      "2022-01-19 03:09:52,980 | Calculate Loss\n",
      "2022-01-19 03:09:52,983 | Update Loss\n",
      "2022-01-19 03:09:52,987 | Backward\n",
      "2022-01-19 03:10:14,272 | Epoch: [6][40/48]\tTime  32.99 ( 33.25)\tData 0.0157 (0.0432)\tLoss (L1) 9.796 (10.212)\n",
      "2022-01-19 03:10:14,288 | ===> Batch : 41\n",
      "2022-01-19 03:10:14,288 | FDS disable\n",
      "2022-01-19 03:10:26,024 | Calculate Loss\n",
      "2022-01-19 03:10:26,026 | Update Loss\n",
      "2022-01-19 03:10:26,029 | Backward\n",
      "2022-01-19 03:10:47,062 | Epoch: [6][41/48]\tTime  32.79 ( 33.24)\tData 0.0156 (0.0425)\tLoss (L1) 10.448 (10.218)\n",
      "2022-01-19 03:10:47,077 | ===> Batch : 42\n",
      "2022-01-19 03:10:47,077 | FDS disable\n",
      "2022-01-19 03:10:58,779 | Calculate Loss\n",
      "2022-01-19 03:10:58,780 | Update Loss\n",
      "2022-01-19 03:10:58,784 | Backward\n",
      "2022-01-19 03:11:20,355 | Epoch: [6][42/48]\tTime  33.29 ( 33.24)\tData 0.0143 (0.0418)\tLoss (L1) 10.462 (10.224)\n",
      "2022-01-19 03:11:20,368 | ===> Batch : 43\n",
      "2022-01-19 03:11:20,369 | FDS disable\n",
      "2022-01-19 03:11:32,109 | Calculate Loss\n",
      "2022-01-19 03:11:32,111 | Update Loss\n",
      "2022-01-19 03:11:32,115 | Backward\n",
      "2022-01-19 03:11:53,197 | Epoch: [6][43/48]\tTime  32.84 ( 33.23)\tData 0.0137 (0.0412)\tLoss (L1) 9.968 (10.218)\n",
      "2022-01-19 03:11:53,211 | ===> Batch : 44\n",
      "2022-01-19 03:11:53,212 | FDS disable\n",
      "2022-01-19 03:12:04,959 | Calculate Loss\n",
      "2022-01-19 03:12:04,960 | Update Loss\n",
      "2022-01-19 03:12:04,964 | Backward\n",
      "2022-01-19 03:12:26,023 | Epoch: [6][44/48]\tTime  32.83 ( 33.22)\tData 0.0144 (0.0405)\tLoss (L1) 10.763 (10.230)\n",
      "2022-01-19 03:12:26,037 | ===> Batch : 45\n",
      "2022-01-19 03:12:26,038 | FDS disable\n",
      "2022-01-19 03:12:37,722 | Calculate Loss\n",
      "2022-01-19 03:12:37,723 | Update Loss\n",
      "2022-01-19 03:12:37,726 | Backward\n",
      "2022-01-19 03:12:58,815 | Epoch: [6][45/48]\tTime  32.79 ( 33.21)\tData 0.0141 (0.0400)\tLoss (L1) 10.305 (10.232)\n",
      "2022-01-19 03:12:58,830 | ===> Batch : 46\n",
      "2022-01-19 03:12:58,831 | FDS disable\n",
      "2022-01-19 03:13:10,521 | Calculate Loss\n",
      "2022-01-19 03:13:10,523 | Update Loss\n",
      "2022-01-19 03:13:10,527 | Backward\n",
      "2022-01-19 03:13:31,960 | Epoch: [6][46/48]\tTime  33.14 ( 33.21)\tData 0.0155 (0.0394)\tLoss (L1) 10.360 (10.235)\n",
      "2022-01-19 03:13:31,974 | ===> Batch : 47\n",
      "2022-01-19 03:13:31,974 | FDS disable\n",
      "2022-01-19 03:13:43,712 | Calculate Loss\n",
      "2022-01-19 03:13:43,713 | Update Loss\n",
      "2022-01-19 03:13:43,717 | Backward\n",
      "2022-01-19 03:14:05,335 | Epoch: [6][47/48]\tTime  33.37 ( 33.21)\tData 0.0139 (0.0389)\tLoss (L1) 10.163 (10.233)\n",
      "2022-01-19 03:14:05,348 | ===> Batch : 48\n",
      "2022-01-19 03:14:05,349 | FDS disable\n",
      "2022-01-19 03:14:13,955 | Calculate Loss\n",
      "2022-01-19 03:14:13,957 | Update Loss\n",
      "2022-01-19 03:14:13,960 | Backward\n",
      "2022-01-19 03:14:28,456 | Epoch: [6][48/48]\tTime  23.12 ( 33.00)\tData 0.0139 (0.0384)\tLoss (L1) 10.287 (10.234)\n",
      "2022-01-19 03:14:40,468 | Val: [0/9]\tTime 11.853 (11.853)\tLoss (MSE) 294.873 (294.873)\tLoss (L1) 13.933 (13.933)\n",
      "2022-01-19 03:14:50,063 | Val: [1/9]\tTime  9.595 (10.724)\tLoss (MSE) 277.118 (285.995)\tLoss (L1) 13.180 (13.557)\n",
      "2022-01-19 03:14:59,693 | Val: [2/9]\tTime  9.630 (10.359)\tLoss (MSE) 303.913 (291.968)\tLoss (L1) 14.398 (13.837)\n",
      "2022-01-19 03:15:09,325 | Val: [3/9]\tTime  9.632 (10.178)\tLoss (MSE) 291.597 (291.875)\tLoss (L1) 13.627 (13.785)\n",
      "2022-01-19 03:15:18,968 | Val: [4/9]\tTime  9.643 (10.071)\tLoss (MSE) 323.835 (298.267)\tLoss (L1) 14.483 (13.924)\n",
      "2022-01-19 03:15:28,591 | Val: [5/9]\tTime  9.623 ( 9.996)\tLoss (MSE) 367.821 (309.860)\tLoss (L1) 15.575 (14.199)\n",
      "2022-01-19 03:15:38,261 | Val: [6/9]\tTime  9.670 ( 9.949)\tLoss (MSE) 301.419 (308.654)\tLoss (L1) 14.338 (14.219)\n",
      "2022-01-19 03:15:47,900 | Val: [7/9]\tTime  9.640 ( 9.911)\tLoss (MSE) 317.162 (309.717)\tLoss (L1) 14.240 (14.222)\n",
      "2022-01-19 03:15:51,223 | Val: [8/9]\tTime  3.322 ( 9.179)\tLoss (MSE) 319.183 (310.124)\tLoss (L1) 14.668 (14.241)\n",
      "2022-01-19 03:15:51,333 |  * Overall: MSE 310.124\tL1 14.241\tG-Mean 9.662\n",
      "2022-01-19 03:15:51,334 |  * Many: MSE 175.994\tL1 10.630\tG-Mean 7.126\n",
      "2022-01-19 03:15:51,334 |  * Median: MSE 579.657\tL1 21.976\tG-Mean 19.453\n",
      "2022-01-19 03:15:51,335 |  * Low: MSE 831.533\tL1 26.957\tG-Mean 24.718\n",
      "2022-01-19 03:15:51,340 | Best L1 Loss: 13.533\n",
      "2022-01-19 03:15:52,601 | Epoch #6: Train loss [10.2339]; Val loss: MSE [310.1241], L1 [14.2410], G-Mean [9.6617]\n",
      "2022-01-19 03:15:52,603 | Training...\n",
      "2022-01-19 03:15:52,604 | Load train loader\n",
      "2022-01-19 03:15:53,810 | ===> Batch : 1\n",
      "2022-01-19 03:15:53,814 | FDS disable\n",
      "2022-01-19 03:16:07,892 | Calculate Loss\n",
      "2022-01-19 03:16:07,893 | Update Loss\n",
      "2022-01-19 03:16:08,017 | Backward\n",
      "2022-01-19 03:16:30,055 | Epoch: [7][ 1/48]\tTime  37.45 ( 37.45)\tData 1.2064 (1.2064)\tLoss (L1) 10.180 (10.180)\n",
      "2022-01-19 03:16:30,070 | ===> Batch : 2\n",
      "2022-01-19 03:16:30,070 | FDS disable\n",
      "2022-01-19 03:16:41,933 | Calculate Loss\n",
      "2022-01-19 03:16:41,935 | Update Loss\n",
      "2022-01-19 03:16:41,938 | Backward\n",
      "2022-01-19 03:17:03,607 | Epoch: [7][ 2/48]\tTime  33.55 ( 35.50)\tData 0.0148 (0.6106)\tLoss (L1) 10.077 (10.128)\n",
      "2022-01-19 03:17:03,623 | ===> Batch : 3\n",
      "2022-01-19 03:17:03,623 | FDS disable\n",
      "2022-01-19 03:17:15,442 | Calculate Loss\n",
      "2022-01-19 03:17:15,444 | Update Loss\n",
      "2022-01-19 03:17:15,448 | Backward\n",
      "2022-01-19 03:17:36,962 | Epoch: [7][ 3/48]\tTime  33.36 ( 34.79)\tData 0.0163 (0.4125)\tLoss (L1) 10.123 (10.127)\n",
      "2022-01-19 03:17:36,978 | ===> Batch : 4\n",
      "2022-01-19 03:17:36,979 | FDS disable\n",
      "2022-01-19 03:17:48,828 | Calculate Loss\n",
      "2022-01-19 03:17:48,831 | Update Loss\n",
      "2022-01-19 03:17:48,835 | Backward\n",
      "2022-01-19 03:18:10,476 | Epoch: [7][ 4/48]\tTime  33.51 ( 34.47)\tData 0.0162 (0.3134)\tLoss (L1) 9.482 (9.966)\n",
      "2022-01-19 03:18:10,488 | ===> Batch : 5\n",
      "2022-01-19 03:18:10,489 | FDS disable\n",
      "2022-01-19 03:18:22,299 | Calculate Loss\n",
      "2022-01-19 03:18:22,301 | Update Loss\n",
      "2022-01-19 03:18:22,305 | Backward\n",
      "2022-01-19 03:18:42,873 | Epoch: [7][ 5/48]\tTime  32.40 ( 34.05)\tData 0.0126 (0.2533)\tLoss (L1) 10.047 (9.982)\n",
      "2022-01-19 03:18:42,886 | ===> Batch : 6\n",
      "2022-01-19 03:18:42,887 | FDS disable\n",
      "2022-01-19 03:18:54,731 | Calculate Loss\n",
      "2022-01-19 03:18:54,733 | Update Loss\n",
      "2022-01-19 03:18:54,736 | Backward\n",
      "2022-01-19 03:19:16,360 | Epoch: [7][ 6/48]\tTime  33.49 ( 33.96)\tData 0.0133 (0.2133)\tLoss (L1) 9.817 (9.954)\n",
      "2022-01-19 03:19:16,374 | ===> Batch : 7\n",
      "2022-01-19 03:19:16,374 | FDS disable\n",
      "2022-01-19 03:19:28,258 | Calculate Loss\n",
      "2022-01-19 03:19:28,259 | Update Loss\n",
      "2022-01-19 03:19:28,263 | Backward\n",
      "2022-01-19 03:19:49,678 | Epoch: [7][ 7/48]\tTime  33.32 ( 33.87)\tData 0.0134 (0.1847)\tLoss (L1) 9.419 (9.878)\n",
      "2022-01-19 03:19:49,691 | ===> Batch : 8\n",
      "2022-01-19 03:19:49,692 | FDS disable\n",
      "2022-01-19 03:20:01,478 | Calculate Loss\n",
      "2022-01-19 03:20:01,479 | Update Loss\n",
      "2022-01-19 03:20:01,482 | Backward\n",
      "2022-01-19 03:20:22,541 | Epoch: [7][ 8/48]\tTime  32.86 ( 33.74)\tData 0.0133 (0.1633)\tLoss (L1) 10.331 (9.935)\n",
      "2022-01-19 03:20:22,560 | ===> Batch : 9\n",
      "2022-01-19 03:20:22,561 | FDS disable\n",
      "2022-01-19 03:20:34,461 | Calculate Loss\n",
      "2022-01-19 03:20:34,462 | Update Loss\n",
      "2022-01-19 03:20:34,466 | Backward\n",
      "2022-01-19 03:20:55,923 | Epoch: [7][ 9/48]\tTime  33.38 ( 33.70)\tData 0.0190 (0.1472)\tLoss (L1) 10.330 (9.978)\n",
      "2022-01-19 03:20:55,937 | ===> Batch : 10\n",
      "2022-01-19 03:20:55,938 | FDS disable\n",
      "2022-01-19 03:21:07,835 | Calculate Loss\n",
      "2022-01-19 03:21:07,836 | Update Loss\n",
      "2022-01-19 03:21:07,840 | Backward\n",
      "2022-01-19 03:21:29,099 | Epoch: [7][10/48]\tTime  33.18 ( 33.65)\tData 0.0148 (0.1340)\tLoss (L1) 10.219 (10.003)\n",
      "2022-01-19 03:21:29,111 | ===> Batch : 11\n",
      "2022-01-19 03:21:29,112 | FDS disable\n",
      "2022-01-19 03:21:40,936 | Calculate Loss\n",
      "2022-01-19 03:21:40,938 | Update Loss\n",
      "2022-01-19 03:21:40,942 | Backward\n",
      "2022-01-19 03:22:02,044 | Epoch: [7][11/48]\tTime  32.95 ( 33.59)\tData 0.0129 (0.1230)\tLoss (L1) 11.037 (10.097)\n",
      "2022-01-19 03:22:02,057 | ===> Batch : 12\n",
      "2022-01-19 03:22:02,058 | FDS disable\n",
      "2022-01-19 03:22:13,897 | Calculate Loss\n",
      "2022-01-19 03:22:13,900 | Update Loss\n",
      "2022-01-19 03:22:13,904 | Backward\n",
      "2022-01-19 03:22:35,545 | Epoch: [7][12/48]\tTime  33.50 ( 33.58)\tData 0.0130 (0.1138)\tLoss (L1) 9.815 (10.073)\n",
      "2022-01-19 03:22:35,558 | ===> Batch : 13\n",
      "2022-01-19 03:22:35,559 | FDS disable\n",
      "2022-01-19 03:22:47,464 | Calculate Loss\n",
      "2022-01-19 03:22:47,466 | Update Loss\n",
      "2022-01-19 03:22:47,470 | Backward\n",
      "2022-01-19 03:23:08,923 | Epoch: [7][13/48]\tTime  33.38 ( 33.56)\tData 0.0131 (0.1061)\tLoss (L1) 9.731 (10.047)\n",
      "2022-01-19 03:23:08,936 | ===> Batch : 14\n",
      "2022-01-19 03:23:08,937 | FDS disable\n",
      "2022-01-19 03:23:20,797 | Calculate Loss\n",
      "2022-01-19 03:23:20,798 | Update Loss\n",
      "2022-01-19 03:23:20,802 | Backward\n",
      "2022-01-19 03:23:41,941 | Epoch: [7][14/48]\tTime  33.02 ( 33.52)\tData 0.0131 (0.0994)\tLoss (L1) 9.853 (10.033)\n",
      "2022-01-19 03:23:41,957 | ===> Batch : 15\n",
      "2022-01-19 03:23:41,958 | FDS disable\n",
      "2022-01-19 03:23:53,832 | Calculate Loss\n",
      "2022-01-19 03:23:53,833 | Update Loss\n",
      "2022-01-19 03:23:53,837 | Backward\n",
      "2022-01-19 03:24:15,107 | Epoch: [7][15/48]\tTime  33.17 ( 33.50)\tData 0.0157 (0.0939)\tLoss (L1) 9.647 (10.007)\n",
      "2022-01-19 03:24:15,120 | ===> Batch : 16\n",
      "2022-01-19 03:24:15,121 | FDS disable\n",
      "2022-01-19 03:24:26,921 | Calculate Loss\n",
      "2022-01-19 03:24:26,923 | Update Loss\n",
      "2022-01-19 03:24:26,926 | Backward\n",
      "2022-01-19 03:24:48,385 | Epoch: [7][16/48]\tTime  33.28 ( 33.49)\tData 0.0130 (0.0888)\tLoss (L1) 9.392 (9.969)\n",
      "2022-01-19 03:24:48,400 | ===> Batch : 17\n",
      "2022-01-19 03:24:48,400 | FDS disable\n",
      "2022-01-19 03:25:00,253 | Calculate Loss\n",
      "2022-01-19 03:25:00,255 | Update Loss\n",
      "2022-01-19 03:25:00,258 | Backward\n",
      "2022-01-19 03:25:21,946 | Epoch: [7][17/48]\tTime  33.56 ( 33.49)\tData 0.0146 (0.0844)\tLoss (L1) 9.981 (9.969)\n",
      "2022-01-19 03:25:21,961 | ===> Batch : 18\n",
      "2022-01-19 03:25:21,962 | FDS disable\n",
      "2022-01-19 03:25:33,881 | Calculate Loss\n",
      "2022-01-19 03:25:33,882 | Update Loss\n",
      "2022-01-19 03:25:33,886 | Backward\n",
      "2022-01-19 03:25:55,085 | Epoch: [7][18/48]\tTime  33.14 ( 33.47)\tData 0.0149 (0.0806)\tLoss (L1) 10.329 (9.989)\n",
      "2022-01-19 03:25:55,106 | ===> Batch : 19\n",
      "2022-01-19 03:25:55,107 | FDS disable\n",
      "2022-01-19 03:26:07,004 | Calculate Loss\n",
      "2022-01-19 03:26:07,006 | Update Loss\n",
      "2022-01-19 03:26:07,009 | Backward\n",
      "2022-01-19 03:26:28,434 | Epoch: [7][19/48]\tTime  33.35 ( 33.46)\tData 0.0210 (0.0774)\tLoss (L1) 9.653 (9.972)\n",
      "2022-01-19 03:26:28,448 | ===> Batch : 20\n",
      "2022-01-19 03:26:28,449 | FDS disable\n",
      "2022-01-19 03:26:40,295 | Calculate Loss\n",
      "2022-01-19 03:26:40,296 | Update Loss\n",
      "2022-01-19 03:26:40,300 | Backward\n",
      "2022-01-19 03:27:01,250 | Epoch: [7][20/48]\tTime  32.82 ( 33.43)\tData 0.0136 (0.0743)\tLoss (L1) 9.133 (9.930)\n",
      "2022-01-19 03:27:01,264 | ===> Batch : 21\n",
      "2022-01-19 03:27:01,264 | FDS disable\n",
      "2022-01-19 03:27:13,121 | Calculate Loss\n",
      "2022-01-19 03:27:13,123 | Update Loss\n",
      "2022-01-19 03:27:13,127 | Backward\n",
      "2022-01-19 03:27:34,213 | Epoch: [7][21/48]\tTime  32.96 ( 33.41)\tData 0.0137 (0.0714)\tLoss (L1) 10.618 (9.963)\n",
      "2022-01-19 03:27:34,227 | ===> Batch : 22\n",
      "2022-01-19 03:27:34,228 | FDS disable\n",
      "2022-01-19 03:27:46,057 | Calculate Loss\n",
      "2022-01-19 03:27:46,058 | Update Loss\n",
      "2022-01-19 03:27:46,061 | Backward\n",
      "2022-01-19 03:28:07,713 | Epoch: [7][22/48]\tTime  33.50 ( 33.41)\tData 0.0138 (0.0688)\tLoss (L1) 10.585 (9.991)\n",
      "2022-01-19 03:28:07,735 | ===> Batch : 23\n",
      "2022-01-19 03:28:07,736 | FDS disable\n",
      "2022-01-19 03:28:19,561 | Calculate Loss\n",
      "2022-01-19 03:28:19,563 | Update Loss\n",
      "2022-01-19 03:28:19,566 | Backward\n",
      "2022-01-19 03:28:40,539 | Epoch: [7][23/48]\tTime  32.83 ( 33.39)\tData 0.0223 (0.0667)\tLoss (L1) 10.443 (10.010)\n",
      "2022-01-19 03:28:40,554 | ===> Batch : 24\n",
      "2022-01-19 03:28:40,554 | FDS disable\n",
      "2022-01-19 03:28:52,362 | Calculate Loss\n",
      "2022-01-19 03:28:52,364 | Update Loss\n",
      "2022-01-19 03:28:52,367 | Backward\n",
      "2022-01-19 03:29:13,045 | Epoch: [7][24/48]\tTime  32.51 ( 33.35)\tData 0.0143 (0.0645)\tLoss (L1) 10.289 (10.022)\n",
      "2022-01-19 03:29:13,061 | ===> Batch : 25\n",
      "2022-01-19 03:29:13,061 | FDS disable\n",
      "2022-01-19 03:29:24,893 | Calculate Loss\n",
      "2022-01-19 03:29:24,895 | Update Loss\n",
      "2022-01-19 03:29:24,898 | Backward\n",
      "2022-01-19 03:29:46,553 | Epoch: [7][25/48]\tTime  33.51 ( 33.36)\tData 0.0154 (0.0626)\tLoss (L1) 10.774 (10.052)\n",
      "2022-01-19 03:29:46,568 | ===> Batch : 26\n",
      "2022-01-19 03:29:46,569 | FDS disable\n",
      "2022-01-19 03:29:58,428 | Calculate Loss\n",
      "2022-01-19 03:29:58,429 | Update Loss\n",
      "2022-01-19 03:29:58,433 | Backward\n",
      "2022-01-19 03:30:19,254 | Epoch: [7][26/48]\tTime  32.70 ( 33.33)\tData 0.0150 (0.0608)\tLoss (L1) 9.996 (10.050)\n",
      "2022-01-19 03:30:19,268 | ===> Batch : 27\n",
      "2022-01-19 03:30:19,268 | FDS disable\n",
      "2022-01-19 03:30:31,064 | Calculate Loss\n",
      "2022-01-19 03:30:31,065 | Update Loss\n",
      "2022-01-19 03:30:31,068 | Backward\n",
      "2022-01-19 03:30:52,467 | Epoch: [7][27/48]\tTime  33.21 ( 33.33)\tData 0.0140 (0.0590)\tLoss (L1) 10.454 (10.065)\n",
      "2022-01-19 03:30:52,481 | ===> Batch : 28\n",
      "2022-01-19 03:30:52,481 | FDS disable\n",
      "2022-01-19 03:31:04,294 | Calculate Loss\n",
      "2022-01-19 03:31:04,296 | Update Loss\n",
      "2022-01-19 03:31:04,300 | Backward\n",
      "2022-01-19 03:31:25,884 | Epoch: [7][28/48]\tTime  33.42 ( 33.33)\tData 0.0142 (0.0574)\tLoss (L1) 11.231 (10.107)\n",
      "2022-01-19 03:31:25,898 | ===> Batch : 29\n",
      "2022-01-19 03:31:25,898 | FDS disable\n",
      "2022-01-19 03:31:37,743 | Calculate Loss\n",
      "2022-01-19 03:31:37,745 | Update Loss\n",
      "2022-01-19 03:31:37,748 | Backward\n",
      "2022-01-19 03:31:59,216 | Epoch: [7][29/48]\tTime  33.33 ( 33.33)\tData 0.0142 (0.0559)\tLoss (L1) 9.930 (10.100)\n",
      "2022-01-19 03:31:59,230 | ===> Batch : 30\n",
      "2022-01-19 03:31:59,231 | FDS disable\n",
      "2022-01-19 03:32:11,150 | Calculate Loss\n",
      "2022-01-19 03:32:11,151 | Update Loss\n",
      "2022-01-19 03:32:11,155 | Backward\n",
      "2022-01-19 03:32:31,947 | Epoch: [7][30/48]\tTime  32.73 ( 33.31)\tData 0.0142 (0.0545)\tLoss (L1) 10.206 (10.104)\n",
      "2022-01-19 03:32:31,962 | ===> Batch : 31\n",
      "2022-01-19 03:32:31,962 | FDS disable\n",
      "2022-01-19 03:32:43,715 | Calculate Loss\n",
      "2022-01-19 03:32:43,716 | Update Loss\n",
      "2022-01-19 03:32:43,720 | Backward\n",
      "2022-01-19 03:33:05,315 | Epoch: [7][31/48]\tTime  33.37 ( 33.31)\tData 0.0147 (0.0533)\tLoss (L1) 10.762 (10.125)\n",
      "2022-01-19 03:33:05,329 | ===> Batch : 32\n",
      "2022-01-19 03:33:05,330 | FDS disable\n",
      "2022-01-19 03:33:17,159 | Calculate Loss\n",
      "2022-01-19 03:33:17,160 | Update Loss\n",
      "2022-01-19 03:33:17,164 | Backward\n",
      "2022-01-19 03:33:38,786 | Epoch: [7][32/48]\tTime  33.47 ( 33.32)\tData 0.0142 (0.0520)\tLoss (L1) 10.083 (10.124)\n",
      "2022-01-19 03:33:38,800 | ===> Batch : 33\n",
      "2022-01-19 03:33:38,801 | FDS disable\n",
      "2022-01-19 03:33:50,520 | Calculate Loss\n",
      "2022-01-19 03:33:50,523 | Update Loss\n",
      "2022-01-19 03:33:50,527 | Backward\n",
      "2022-01-19 03:34:11,424 | Epoch: [7][33/48]\tTime  32.64 ( 33.30)\tData 0.0143 (0.0509)\tLoss (L1) 10.824 (10.145)\n",
      "2022-01-19 03:34:11,438 | ===> Batch : 34\n",
      "2022-01-19 03:34:11,438 | FDS disable\n",
      "2022-01-19 03:34:23,145 | Calculate Loss\n",
      "2022-01-19 03:34:23,146 | Update Loss\n",
      "2022-01-19 03:34:23,151 | Backward\n",
      "2022-01-19 03:34:44,559 | Epoch: [7][34/48]\tTime  33.14 ( 33.29)\tData 0.0143 (0.0498)\tLoss (L1) 10.418 (10.153)\n",
      "2022-01-19 03:34:44,574 | ===> Batch : 35\n",
      "2022-01-19 03:34:44,575 | FDS disable\n",
      "2022-01-19 03:34:56,291 | Calculate Loss\n",
      "2022-01-19 03:34:56,292 | Update Loss\n",
      "2022-01-19 03:34:56,296 | Backward\n",
      "2022-01-19 03:35:17,391 | Epoch: [7][35/48]\tTime  32.83 ( 33.28)\tData 0.0150 (0.0488)\tLoss (L1) 10.220 (10.155)\n",
      "2022-01-19 03:35:17,404 | ===> Batch : 36\n",
      "2022-01-19 03:35:17,405 | FDS disable\n",
      "2022-01-19 03:35:29,163 | Calculate Loss\n",
      "2022-01-19 03:35:29,164 | Update Loss\n",
      "2022-01-19 03:35:29,168 | Backward\n",
      "2022-01-19 03:35:50,792 | Epoch: [7][36/48]\tTime  33.40 ( 33.28)\tData 0.0139 (0.0479)\tLoss (L1) 10.648 (10.169)\n",
      "2022-01-19 03:35:50,806 | ===> Batch : 37\n",
      "2022-01-19 03:35:50,807 | FDS disable\n",
      "2022-01-19 03:36:02,560 | Calculate Loss\n",
      "2022-01-19 03:36:02,561 | Update Loss\n",
      "2022-01-19 03:36:02,565 | Backward\n",
      "2022-01-19 03:36:23,589 | Epoch: [7][37/48]\tTime  32.80 ( 33.27)\tData 0.0139 (0.0469)\tLoss (L1) 10.418 (10.175)\n",
      "2022-01-19 03:36:23,603 | ===> Batch : 38\n",
      "2022-01-19 03:36:23,604 | FDS disable\n",
      "2022-01-19 03:36:35,234 | Calculate Loss\n",
      "2022-01-19 03:36:35,235 | Update Loss\n",
      "2022-01-19 03:36:35,239 | Backward\n",
      "2022-01-19 03:36:56,443 | Epoch: [7][38/48]\tTime  32.85 ( 33.26)\tData 0.0144 (0.0461)\tLoss (L1) 9.389 (10.155)\n",
      "2022-01-19 03:36:56,457 | ===> Batch : 39\n",
      "2022-01-19 03:36:56,457 | FDS disable\n",
      "2022-01-19 03:37:08,180 | Calculate Loss\n",
      "2022-01-19 03:37:08,181 | Update Loss\n",
      "2022-01-19 03:37:08,185 | Backward\n",
      "2022-01-19 03:37:29,172 | Epoch: [7][39/48]\tTime  32.73 ( 33.25)\tData 0.0139 (0.0453)\tLoss (L1) 9.707 (10.143)\n",
      "2022-01-19 03:37:29,189 | ===> Batch : 40\n",
      "2022-01-19 03:37:29,190 | FDS disable\n",
      "2022-01-19 03:37:40,888 | Calculate Loss\n",
      "2022-01-19 03:37:40,890 | Update Loss\n",
      "2022-01-19 03:37:40,893 | Backward\n",
      "2022-01-19 03:38:02,045 | Epoch: [7][40/48]\tTime  32.87 ( 33.24)\tData 0.0173 (0.0446)\tLoss (L1) 9.750 (10.133)\n",
      "2022-01-19 03:38:02,058 | ===> Batch : 41\n",
      "2022-01-19 03:38:02,059 | FDS disable\n",
      "2022-01-19 03:38:13,748 | Calculate Loss\n",
      "2022-01-19 03:38:13,750 | Update Loss\n",
      "2022-01-19 03:38:13,753 | Backward\n",
      "2022-01-19 03:38:35,278 | Epoch: [7][41/48]\tTime  33.23 ( 33.24)\tData 0.0137 (0.0438)\tLoss (L1) 9.717 (10.123)\n",
      "2022-01-19 03:38:35,293 | ===> Batch : 42\n",
      "2022-01-19 03:38:35,294 | FDS disable\n",
      "2022-01-19 03:38:46,994 | Calculate Loss\n",
      "2022-01-19 03:38:46,996 | Update Loss\n",
      "2022-01-19 03:38:47,000 | Backward\n",
      "2022-01-19 03:39:08,572 | Epoch: [7][42/48]\tTime  33.29 ( 33.24)\tData 0.0151 (0.0431)\tLoss (L1) 9.848 (10.117)\n",
      "2022-01-19 03:39:08,588 | ===> Batch : 43\n",
      "2022-01-19 03:39:08,589 | FDS disable\n",
      "2022-01-19 03:39:20,346 | Calculate Loss\n",
      "2022-01-19 03:39:20,349 | Update Loss\n",
      "2022-01-19 03:39:20,354 | Backward\n",
      "2022-01-19 03:39:41,698 | Epoch: [7][43/48]\tTime  33.13 ( 33.23)\tData 0.0159 (0.0425)\tLoss (L1) 9.501 (10.102)\n",
      "2022-01-19 03:39:41,712 | ===> Batch : 44\n",
      "2022-01-19 03:39:41,713 | FDS disable\n",
      "2022-01-19 03:39:53,398 | Calculate Loss\n",
      "2022-01-19 03:39:53,400 | Update Loss\n",
      "2022-01-19 03:39:53,403 | Backward\n",
      "2022-01-19 03:40:14,687 | Epoch: [7][44/48]\tTime  32.99 ( 33.23)\tData 0.0142 (0.0418)\tLoss (L1) 9.990 (10.100)\n",
      "2022-01-19 03:40:14,701 | ===> Batch : 45\n",
      "2022-01-19 03:40:14,702 | FDS disable\n",
      "2022-01-19 03:40:26,459 | Calculate Loss\n",
      "2022-01-19 03:40:26,460 | Update Loss\n",
      "2022-01-19 03:40:26,464 | Backward\n",
      "2022-01-19 03:40:47,240 | Epoch: [7][45/48]\tTime  32.55 ( 33.21)\tData 0.0144 (0.0412)\tLoss (L1) 9.497 (10.086)\n",
      "2022-01-19 03:40:47,254 | ===> Batch : 46\n",
      "2022-01-19 03:40:47,254 | FDS disable\n",
      "2022-01-19 03:40:58,954 | Calculate Loss\n",
      "2022-01-19 03:40:58,956 | Update Loss\n",
      "2022-01-19 03:40:58,959 | Backward\n",
      "2022-01-19 03:41:20,394 | Epoch: [7][46/48]\tTime  33.15 ( 33.21)\tData 0.0139 (0.0406)\tLoss (L1) 9.561 (10.075)\n",
      "2022-01-19 03:41:20,408 | ===> Batch : 47\n",
      "2022-01-19 03:41:20,408 | FDS disable\n",
      "2022-01-19 03:41:32,128 | Calculate Loss\n",
      "2022-01-19 03:41:32,129 | Update Loss\n",
      "2022-01-19 03:41:32,133 | Backward\n",
      "2022-01-19 03:41:53,166 | Epoch: [7][47/48]\tTime  32.77 ( 33.20)\tData 0.0139 (0.0401)\tLoss (L1) 10.664 (10.088)\n",
      "2022-01-19 03:41:53,180 | ===> Batch : 48\n",
      "2022-01-19 03:41:53,180 | FDS disable\n",
      "2022-01-19 03:42:01,850 | Calculate Loss\n",
      "2022-01-19 03:42:01,851 | Update Loss\n",
      "2022-01-19 03:42:01,855 | Backward\n",
      "2022-01-19 03:42:16,517 | Epoch: [7][48/48]\tTime  23.35 ( 33.00)\tData 0.0141 (0.0395)\tLoss (L1) 9.787 (10.083)\n",
      "2022-01-19 03:42:28,359 | Val: [0/9]\tTime 11.687 (11.687)\tLoss (MSE) 248.357 (248.357)\tLoss (L1) 12.640 (12.640)\n",
      "2022-01-19 03:42:37,946 | Val: [1/9]\tTime  9.587 (10.637)\tLoss (MSE) 226.403 (237.380)\tLoss (L1) 12.069 (12.355)\n",
      "2022-01-19 03:42:47,688 | Val: [2/9]\tTime  9.742 (10.339)\tLoss (MSE) 253.435 (242.732)\tLoss (L1) 12.834 (12.514)\n",
      "2022-01-19 03:42:57,326 | Val: [3/9]\tTime  9.638 (10.164)\tLoss (MSE) 241.270 (242.366)\tLoss (L1) 12.246 (12.447)\n",
      "2022-01-19 03:43:06,947 | Val: [4/9]\tTime  9.622 (10.055)\tLoss (MSE) 278.267 (249.546)\tLoss (L1) 13.553 (12.668)\n",
      "2022-01-19 03:43:16,556 | Val: [5/9]\tTime  9.608 ( 9.981)\tLoss (MSE) 294.775 (257.085)\tLoss (L1) 13.673 (12.836)\n",
      "2022-01-19 03:43:26,223 | Val: [6/9]\tTime  9.667 ( 9.936)\tLoss (MSE) 251.696 (256.315)\tLoss (L1) 12.570 (12.798)\n",
      "2022-01-19 03:43:35,786 | Val: [7/9]\tTime  9.563 ( 9.889)\tLoss (MSE) 283.271 (259.684)\tLoss (L1) 13.157 (12.843)\n",
      "2022-01-19 03:43:39,059 | Val: [8/9]\tTime  3.274 ( 9.154)\tLoss (MSE) 264.037 (259.871)\tLoss (L1) 12.123 (12.812)\n",
      "2022-01-19 03:43:39,171 |  * Overall: MSE 259.871\tL1 12.812\tG-Mean 8.557\n",
      "2022-01-19 03:43:39,172 |  * Many: MSE 174.315\tL1 10.340\tG-Mean 6.771\n",
      "2022-01-19 03:43:39,172 |  * Median: MSE 420.206\tL1 17.469\tG-Mean 13.617\n",
      "2022-01-19 03:43:39,173 |  * Low: MSE 624.507\tL1 23.280\tG-Mean 21.621\n",
      "2022-01-19 03:43:39,178 | Best L1 Loss: 12.812\n",
      "2022-01-19 03:43:40,427 | ===> Saving current best checkpoint...\n",
      "2022-01-19 03:43:41,608 | Epoch #7: Train loss [10.0833]; Val loss: MSE [259.8714], L1 [12.8117], G-Mean [8.5571]\n",
      "2022-01-19 03:43:41,609 | Training...\n",
      "2022-01-19 03:43:41,610 | Load train loader\n",
      "2022-01-19 03:43:42,759 | ===> Batch : 1\n",
      "2022-01-19 03:43:42,761 | FDS disable\n",
      "2022-01-19 03:43:56,913 | Calculate Loss\n",
      "2022-01-19 03:43:56,915 | Update Loss\n",
      "2022-01-19 03:43:57,044 | Backward\n",
      "2022-01-19 03:44:19,056 | Epoch: [8][ 1/48]\tTime  37.45 ( 37.45)\tData 1.1486 (1.1486)\tLoss (L1) 10.709 (10.709)\n",
      "2022-01-19 03:44:19,074 | ===> Batch : 2\n",
      "2022-01-19 03:44:19,074 | FDS disable\n",
      "2022-01-19 03:44:30,915 | Calculate Loss\n",
      "2022-01-19 03:44:30,917 | Update Loss\n",
      "2022-01-19 03:44:30,921 | Backward\n",
      "2022-01-19 03:44:52,537 | Epoch: [8][ 2/48]\tTime  33.48 ( 35.46)\tData 0.0181 (0.5833)\tLoss (L1) 9.248 (9.979)\n",
      "2022-01-19 03:44:52,550 | ===> Batch : 3\n",
      "2022-01-19 03:44:52,551 | FDS disable\n",
      "2022-01-19 03:45:04,445 | Calculate Loss\n",
      "2022-01-19 03:45:04,447 | Update Loss\n",
      "2022-01-19 03:45:04,451 | Backward\n",
      "2022-01-19 03:45:25,553 | Epoch: [8][ 3/48]\tTime  33.02 ( 34.65)\tData 0.0136 (0.3934)\tLoss (L1) 9.770 (9.909)\n",
      "2022-01-19 03:45:25,566 | ===> Batch : 4\n",
      "2022-01-19 03:45:25,567 | FDS disable\n",
      "2022-01-19 03:45:37,448 | Calculate Loss\n",
      "2022-01-19 03:45:37,449 | Update Loss\n",
      "2022-01-19 03:45:37,453 | Backward\n",
      "2022-01-19 03:45:58,566 | Epoch: [8][ 4/48]\tTime  33.01 ( 34.24)\tData 0.0139 (0.2985)\tLoss (L1) 9.869 (9.899)\n",
      "2022-01-19 03:45:58,580 | ===> Batch : 5\n",
      "2022-01-19 03:45:58,580 | FDS disable\n",
      "2022-01-19 03:46:10,436 | Calculate Loss\n",
      "2022-01-19 03:46:10,438 | Update Loss\n",
      "2022-01-19 03:46:10,441 | Backward\n",
      "2022-01-19 03:46:32,077 | Epoch: [8][ 5/48]\tTime  33.51 ( 34.09)\tData 0.0139 (0.2416)\tLoss (L1) 9.774 (9.874)\n",
      "2022-01-19 03:46:32,093 | ===> Batch : 6\n",
      "2022-01-19 03:46:32,093 | FDS disable\n",
      "2022-01-19 03:46:43,928 | Calculate Loss\n",
      "2022-01-19 03:46:43,930 | Update Loss\n",
      "2022-01-19 03:46:43,933 | Backward\n",
      "2022-01-19 03:47:04,808 | Epoch: [8][ 6/48]\tTime  32.73 ( 33.87)\tData 0.0158 (0.2040)\tLoss (L1) 9.837 (9.868)\n",
      "2022-01-19 03:47:04,821 | ===> Batch : 7\n",
      "2022-01-19 03:47:04,822 | FDS disable\n",
      "2022-01-19 03:47:16,673 | Calculate Loss\n",
      "2022-01-19 03:47:16,674 | Update Loss\n",
      "2022-01-19 03:47:16,678 | Backward\n",
      "2022-01-19 03:47:38,256 | Epoch: [8][ 7/48]\tTime  33.45 ( 33.81)\tData 0.0129 (0.1767)\tLoss (L1) 10.099 (9.901)\n",
      "2022-01-19 03:47:38,270 | ===> Batch : 8\n",
      "2022-01-19 03:47:38,271 | FDS disable\n",
      "2022-01-19 03:47:50,161 | Calculate Loss\n",
      "2022-01-19 03:47:50,163 | Update Loss\n",
      "2022-01-19 03:47:50,166 | Backward\n",
      "2022-01-19 03:48:11,758 | Epoch: [8][ 8/48]\tTime  33.50 ( 33.77)\tData 0.0144 (0.1564)\tLoss (L1) 9.666 (9.872)\n",
      "2022-01-19 03:48:11,773 | ===> Batch : 9\n",
      "2022-01-19 03:48:11,774 | FDS disable\n",
      "2022-01-19 03:48:23,593 | Calculate Loss\n",
      "2022-01-19 03:48:23,595 | Update Loss\n",
      "2022-01-19 03:48:23,599 | Backward\n",
      "2022-01-19 03:48:44,999 | Epoch: [8][ 9/48]\tTime  33.24 ( 33.71)\tData 0.0154 (0.1407)\tLoss (L1) 9.330 (9.811)\n",
      "2022-01-19 03:48:45,015 | ===> Batch : 10\n",
      "2022-01-19 03:48:45,016 | FDS disable\n",
      "2022-01-19 03:48:56,866 | Calculate Loss\n",
      "2022-01-19 03:48:56,867 | Update Loss\n",
      "2022-01-19 03:48:56,871 | Backward\n",
      "2022-01-19 03:49:18,088 | Epoch: [8][10/48]\tTime  33.09 ( 33.65)\tData 0.0164 (0.1283)\tLoss (L1) 9.986 (9.829)\n",
      "2022-01-19 03:49:18,106 | ===> Batch : 11\n",
      "2022-01-19 03:49:18,106 | FDS disable\n",
      "2022-01-19 03:49:29,895 | Calculate Loss\n",
      "2022-01-19 03:49:29,897 | Update Loss\n",
      "2022-01-19 03:49:29,901 | Backward\n",
      "2022-01-19 03:49:51,511 | Epoch: [8][11/48]\tTime  33.42 ( 33.63)\tData 0.0175 (0.1182)\tLoss (L1) 10.514 (9.891)\n",
      "2022-01-19 03:49:51,526 | ===> Batch : 12\n",
      "2022-01-19 03:49:51,526 | FDS disable\n",
      "2022-01-19 03:50:03,364 | Calculate Loss\n",
      "2022-01-19 03:50:03,365 | Update Loss\n",
      "2022-01-19 03:50:03,369 | Backward\n",
      "2022-01-19 03:50:23,920 | Epoch: [8][12/48]\tTime  32.41 ( 33.53)\tData 0.0146 (0.1096)\tLoss (L1) 9.902 (9.892)\n",
      "2022-01-19 03:50:23,933 | ===> Batch : 13\n",
      "2022-01-19 03:50:23,934 | FDS disable\n",
      "2022-01-19 03:50:35,877 | Calculate Loss\n",
      "2022-01-19 03:50:35,879 | Update Loss\n",
      "2022-01-19 03:50:35,882 | Backward\n",
      "2022-01-19 03:50:56,969 | Epoch: [8][13/48]\tTime  33.05 ( 33.49)\tData 0.0128 (0.1021)\tLoss (L1) 10.292 (9.923)\n",
      "2022-01-19 03:50:56,982 | ===> Batch : 14\n",
      "2022-01-19 03:50:56,983 | FDS disable\n",
      "2022-01-19 03:51:08,789 | Calculate Loss\n",
      "2022-01-19 03:51:08,790 | Update Loss\n",
      "2022-01-19 03:51:08,794 | Backward\n",
      "2022-01-19 03:51:30,360 | Epoch: [8][14/48]\tTime  33.39 ( 33.48)\tData 0.0127 (0.0958)\tLoss (L1) 9.239 (9.874)\n",
      "2022-01-19 03:51:30,375 | ===> Batch : 15\n",
      "2022-01-19 03:51:30,376 | FDS disable\n",
      "2022-01-19 03:51:42,221 | Calculate Loss\n",
      "2022-01-19 03:51:42,222 | Update Loss\n",
      "2022-01-19 03:51:42,226 | Backward\n",
      "2022-01-19 03:52:03,334 | Epoch: [8][15/48]\tTime  32.97 ( 33.45)\tData 0.0154 (0.0904)\tLoss (L1) 10.101 (9.889)\n",
      "2022-01-19 03:52:03,347 | ===> Batch : 16\n",
      "2022-01-19 03:52:03,348 | FDS disable\n",
      "2022-01-19 03:52:15,253 | Calculate Loss\n",
      "2022-01-19 03:52:15,254 | Update Loss\n",
      "2022-01-19 03:52:15,258 | Backward\n",
      "2022-01-19 03:52:36,348 | Epoch: [8][16/48]\tTime  33.01 ( 33.42)\tData 0.0130 (0.0856)\tLoss (L1) 10.143 (9.905)\n",
      "2022-01-19 03:52:36,369 | ===> Batch : 17\n",
      "2022-01-19 03:52:36,370 | FDS disable\n",
      "2022-01-19 03:52:48,208 | Calculate Loss\n",
      "2022-01-19 03:52:48,210 | Update Loss\n",
      "2022-01-19 03:52:48,214 | Backward\n",
      "2022-01-19 03:53:09,273 | Epoch: [8][17/48]\tTime  32.93 ( 33.39)\tData 0.0216 (0.0818)\tLoss (L1) 9.404 (9.876)\n",
      "2022-01-19 03:53:09,288 | ===> Batch : 18\n",
      "2022-01-19 03:53:09,288 | FDS disable\n",
      "2022-01-19 03:53:21,100 | Calculate Loss\n",
      "2022-01-19 03:53:21,101 | Update Loss\n",
      "2022-01-19 03:53:21,105 | Backward\n",
      "2022-01-19 03:53:42,361 | Epoch: [8][18/48]\tTime  33.09 ( 33.38)\tData 0.0150 (0.0781)\tLoss (L1) 10.439 (9.907)\n",
      "2022-01-19 03:53:42,375 | ===> Batch : 19\n",
      "2022-01-19 03:53:42,375 | FDS disable\n",
      "2022-01-19 03:53:54,247 | Calculate Loss\n",
      "2022-01-19 03:53:54,248 | Update Loss\n",
      "2022-01-19 03:53:54,252 | Backward\n",
      "2022-01-19 03:54:15,816 | Epoch: [8][19/48]\tTime  33.45 ( 33.38)\tData 0.0139 (0.0747)\tLoss (L1) 9.356 (9.878)\n",
      "2022-01-19 03:54:15,830 | ===> Batch : 20\n",
      "2022-01-19 03:54:15,831 | FDS disable\n",
      "2022-01-19 03:54:27,667 | Calculate Loss\n",
      "2022-01-19 03:54:27,668 | Update Loss\n",
      "2022-01-19 03:54:27,672 | Backward\n",
      "2022-01-19 03:54:48,457 | Epoch: [8][20/48]\tTime  32.64 ( 33.34)\tData 0.0143 (0.0717)\tLoss (L1) 9.754 (9.872)\n",
      "2022-01-19 03:54:48,471 | ===> Batch : 21\n",
      "2022-01-19 03:54:48,471 | FDS disable\n",
      "2022-01-19 03:55:00,297 | Calculate Loss\n",
      "2022-01-19 03:55:00,298 | Update Loss\n",
      "2022-01-19 03:55:00,302 | Backward\n",
      "2022-01-19 03:55:21,884 | Epoch: [8][21/48]\tTime  33.43 ( 33.35)\tData 0.0140 (0.0689)\tLoss (L1) 10.068 (9.881)\n",
      "2022-01-19 03:55:21,898 | ===> Batch : 22\n",
      "2022-01-19 03:55:21,898 | FDS disable\n",
      "2022-01-19 03:55:33,776 | Calculate Loss\n",
      "2022-01-19 03:55:33,777 | Update Loss\n",
      "2022-01-19 03:55:33,781 | Backward\n",
      "2022-01-19 03:55:54,859 | Epoch: [8][22/48]\tTime  32.98 ( 33.33)\tData 0.0141 (0.0665)\tLoss (L1) 10.151 (9.893)\n",
      "2022-01-19 03:55:54,873 | ===> Batch : 23\n",
      "2022-01-19 03:55:54,874 | FDS disable\n",
      "2022-01-19 03:56:06,724 | Calculate Loss\n",
      "2022-01-19 03:56:06,725 | Update Loss\n",
      "2022-01-19 03:56:06,729 | Backward\n",
      "2022-01-19 03:56:27,759 | Epoch: [8][23/48]\tTime  32.90 ( 33.31)\tData 0.0140 (0.0642)\tLoss (L1) 8.834 (9.847)\n",
      "2022-01-19 03:56:27,774 | ===> Batch : 24\n",
      "2022-01-19 03:56:27,774 | FDS disable\n",
      "2022-01-19 03:56:39,561 | Calculate Loss\n",
      "2022-01-19 03:56:39,563 | Update Loss\n",
      "2022-01-19 03:56:39,567 | Backward\n",
      "2022-01-19 03:57:01,239 | Epoch: [8][24/48]\tTime  33.48 ( 33.32)\tData 0.0146 (0.0621)\tLoss (L1) 9.748 (9.843)\n",
      "2022-01-19 03:57:01,253 | ===> Batch : 25\n",
      "2022-01-19 03:57:01,253 | FDS disable\n",
      "2022-01-19 03:57:13,044 | Calculate Loss\n",
      "2022-01-19 03:57:13,045 | Update Loss\n",
      "2022-01-19 03:57:13,048 | Backward\n",
      "2022-01-19 03:57:34,667 | Epoch: [8][25/48]\tTime  33.43 ( 33.32)\tData 0.0139 (0.0602)\tLoss (L1) 9.883 (9.845)\n",
      "2022-01-19 03:57:34,682 | ===> Batch : 26\n",
      "2022-01-19 03:57:34,683 | FDS disable\n",
      "2022-01-19 03:57:46,557 | Calculate Loss\n",
      "2022-01-19 03:57:46,559 | Update Loss\n",
      "2022-01-19 03:57:46,563 | Backward\n",
      "2022-01-19 03:58:07,366 | Epoch: [8][26/48]\tTime  32.70 ( 33.30)\tData 0.0151 (0.0584)\tLoss (L1) 9.877 (9.846)\n",
      "2022-01-19 03:58:07,380 | ===> Batch : 27\n",
      "2022-01-19 03:58:07,380 | FDS disable\n",
      "2022-01-19 03:58:19,198 | Calculate Loss\n",
      "2022-01-19 03:58:19,199 | Update Loss\n",
      "2022-01-19 03:58:19,203 | Backward\n",
      "2022-01-19 03:58:40,675 | Epoch: [8][27/48]\tTime  33.31 ( 33.30)\tData 0.0142 (0.0568)\tLoss (L1) 10.009 (9.852)\n",
      "2022-01-19 03:58:40,689 | ===> Batch : 28\n",
      "2022-01-19 03:58:40,690 | FDS disable\n",
      "2022-01-19 03:58:52,524 | Calculate Loss\n",
      "2022-01-19 03:58:52,525 | Update Loss\n",
      "2022-01-19 03:58:52,529 | Backward\n",
      "2022-01-19 03:59:13,663 | Epoch: [8][28/48]\tTime  32.99 ( 33.29)\tData 0.0145 (0.0553)\tLoss (L1) 9.734 (9.848)\n",
      "2022-01-19 03:59:13,677 | ===> Batch : 29\n",
      "2022-01-19 03:59:13,677 | FDS disable\n",
      "2022-01-19 03:59:25,506 | Calculate Loss\n",
      "2022-01-19 03:59:25,507 | Update Loss\n",
      "2022-01-19 03:59:25,511 | Backward\n",
      "2022-01-19 03:59:46,800 | Epoch: [8][29/48]\tTime  33.14 ( 33.28)\tData 0.0141 (0.0539)\tLoss (L1) 9.751 (9.844)\n",
      "2022-01-19 03:59:46,815 | ===> Batch : 30\n",
      "2022-01-19 03:59:46,816 | FDS disable\n",
      "2022-01-19 03:59:58,665 | Calculate Loss\n",
      "2022-01-19 03:59:58,666 | Update Loss\n",
      "2022-01-19 03:59:58,670 | Backward\n",
      "2022-01-19 04:00:19,597 | Epoch: [8][30/48]\tTime  32.80 ( 33.27)\tData 0.0150 (0.0526)\tLoss (L1) 9.366 (9.829)\n",
      "2022-01-19 04:00:19,611 | ===> Batch : 31\n",
      "2022-01-19 04:00:19,612 | FDS disable\n",
      "2022-01-19 04:00:31,399 | Calculate Loss\n",
      "2022-01-19 04:00:31,401 | Update Loss\n",
      "2022-01-19 04:00:31,405 | Backward\n",
      "2022-01-19 04:00:52,991 | Epoch: [8][31/48]\tTime  33.39 ( 33.27)\tData 0.0144 (0.0514)\tLoss (L1) 9.867 (9.830)\n",
      "2022-01-19 04:00:53,007 | ===> Batch : 32\n",
      "2022-01-19 04:00:53,008 | FDS disable\n",
      "2022-01-19 04:01:04,803 | Calculate Loss\n",
      "2022-01-19 04:01:04,805 | Update Loss\n",
      "2022-01-19 04:01:04,808 | Backward\n",
      "2022-01-19 04:01:26,424 | Epoch: [8][32/48]\tTime  33.43 ( 33.28)\tData 0.0160 (0.0502)\tLoss (L1) 9.029 (9.805)\n",
      "2022-01-19 04:01:26,439 | ===> Batch : 33\n",
      "2022-01-19 04:01:26,439 | FDS disable\n",
      "2022-01-19 04:01:38,163 | Calculate Loss\n",
      "2022-01-19 04:01:38,166 | Update Loss\n",
      "2022-01-19 04:01:38,170 | Backward\n",
      "2022-01-19 04:01:59,600 | Epoch: [8][33/48]\tTime  33.18 ( 33.27)\tData 0.0148 (0.0492)\tLoss (L1) 9.372 (9.792)\n",
      "2022-01-19 04:01:59,614 | ===> Batch : 34\n",
      "2022-01-19 04:01:59,615 | FDS disable\n",
      "2022-01-19 04:02:11,305 | Calculate Loss\n",
      "2022-01-19 04:02:11,307 | Update Loss\n",
      "2022-01-19 04:02:11,310 | Backward\n",
      "2022-01-19 04:02:32,158 | Epoch: [8][34/48]\tTime  32.56 ( 33.25)\tData 0.0141 (0.0481)\tLoss (L1) 9.514 (9.783)\n",
      "2022-01-19 04:02:32,172 | ===> Batch : 35\n",
      "2022-01-19 04:02:32,172 | FDS disable\n",
      "2022-01-19 04:02:43,889 | Calculate Loss\n",
      "2022-01-19 04:02:43,890 | Update Loss\n",
      "2022-01-19 04:02:43,894 | Backward\n",
      "2022-01-19 04:03:05,264 | Epoch: [8][35/48]\tTime  33.11 ( 33.25)\tData 0.0139 (0.0472)\tLoss (L1) 9.795 (9.784)\n",
      "2022-01-19 04:03:05,279 | ===> Batch : 36\n",
      "2022-01-19 04:03:05,279 | FDS disable\n",
      "2022-01-19 04:03:16,958 | Calculate Loss\n",
      "2022-01-19 04:03:16,960 | Update Loss\n",
      "2022-01-19 04:03:16,963 | Backward\n",
      "2022-01-19 04:03:38,055 | Epoch: [8][36/48]\tTime  32.79 ( 33.23)\tData 0.0150 (0.0463)\tLoss (L1) 9.007 (9.762)\n",
      "2022-01-19 04:03:38,070 | ===> Batch : 37\n",
      "2022-01-19 04:03:38,070 | FDS disable\n",
      "2022-01-19 04:03:49,815 | Calculate Loss\n",
      "2022-01-19 04:03:49,816 | Update Loss\n",
      "2022-01-19 04:03:49,820 | Backward\n",
      "2022-01-19 04:04:11,370 | Epoch: [8][37/48]\tTime  33.32 ( 33.24)\tData 0.0151 (0.0454)\tLoss (L1) 9.741 (9.762)\n",
      "2022-01-19 04:04:11,385 | ===> Batch : 38\n",
      "2022-01-19 04:04:11,386 | FDS disable\n",
      "2022-01-19 04:04:23,081 | Calculate Loss\n",
      "2022-01-19 04:04:23,083 | Update Loss\n",
      "2022-01-19 04:04:23,086 | Backward\n",
      "2022-01-19 04:04:44,109 | Epoch: [8][38/48]\tTime  32.74 ( 33.22)\tData 0.0148 (0.0446)\tLoss (L1) 10.545 (9.782)\n",
      "2022-01-19 04:04:44,124 | ===> Batch : 39\n",
      "2022-01-19 04:04:44,124 | FDS disable\n",
      "2022-01-19 04:04:55,872 | Calculate Loss\n",
      "2022-01-19 04:04:55,873 | Update Loss\n",
      "2022-01-19 04:04:55,877 | Backward\n",
      "2022-01-19 04:05:17,488 | Epoch: [8][39/48]\tTime  33.38 ( 33.23)\tData 0.0144 (0.0438)\tLoss (L1) 10.873 (9.810)\n",
      "2022-01-19 04:05:17,503 | ===> Batch : 40\n",
      "2022-01-19 04:05:17,503 | FDS disable\n",
      "2022-01-19 04:05:29,187 | Calculate Loss\n",
      "2022-01-19 04:05:29,189 | Update Loss\n",
      "2022-01-19 04:05:29,192 | Backward\n",
      "2022-01-19 04:05:50,295 | Epoch: [8][40/48]\tTime  32.81 ( 33.22)\tData 0.0144 (0.0431)\tLoss (L1) 10.236 (9.821)\n",
      "2022-01-19 04:05:50,310 | ===> Batch : 41\n",
      "2022-01-19 04:05:50,311 | FDS disable\n",
      "2022-01-19 04:06:02,031 | Calculate Loss\n",
      "2022-01-19 04:06:02,032 | Update Loss\n",
      "2022-01-19 04:06:02,036 | Backward\n",
      "2022-01-19 04:06:23,385 | Epoch: [8][41/48]\tTime  33.09 ( 33.21)\tData 0.0151 (0.0424)\tLoss (L1) 9.108 (9.803)\n",
      "2022-01-19 04:06:23,400 | ===> Batch : 42\n",
      "2022-01-19 04:06:23,401 | FDS disable\n",
      "2022-01-19 04:06:35,120 | Calculate Loss\n",
      "2022-01-19 04:06:35,121 | Update Loss\n",
      "2022-01-19 04:06:35,125 | Backward\n",
      "2022-01-19 04:06:56,883 | Epoch: [8][42/48]\tTime  33.50 ( 33.22)\tData 0.0151 (0.0418)\tLoss (L1) 9.731 (9.802)\n",
      "2022-01-19 04:06:56,897 | ===> Batch : 43\n",
      "2022-01-19 04:06:56,897 | FDS disable\n",
      "2022-01-19 04:07:08,615 | Calculate Loss\n",
      "2022-01-19 04:07:08,616 | Update Loss\n",
      "2022-01-19 04:07:08,620 | Backward\n",
      "2022-01-19 04:07:30,121 | Epoch: [8][43/48]\tTime  33.24 ( 33.22)\tData 0.0140 (0.0411)\tLoss (L1) 9.916 (9.804)\n",
      "2022-01-19 04:07:30,136 | ===> Batch : 44\n",
      "2022-01-19 04:07:30,136 | FDS disable\n",
      "2022-01-19 04:07:41,860 | Calculate Loss\n",
      "2022-01-19 04:07:41,862 | Update Loss\n",
      "2022-01-19 04:07:41,866 | Backward\n",
      "2022-01-19 04:08:03,483 | Epoch: [8][44/48]\tTime  33.36 ( 33.22)\tData 0.0144 (0.0405)\tLoss (L1) 10.429 (9.819)\n",
      "2022-01-19 04:08:03,497 | ===> Batch : 45\n",
      "2022-01-19 04:08:03,497 | FDS disable\n",
      "2022-01-19 04:08:15,196 | Calculate Loss\n",
      "2022-01-19 04:08:15,197 | Update Loss\n",
      "2022-01-19 04:08:15,201 | Backward\n",
      "2022-01-19 04:08:36,108 | Epoch: [8][45/48]\tTime  32.63 ( 33.21)\tData 0.0142 (0.0399)\tLoss (L1) 8.675 (9.793)\n",
      "2022-01-19 04:08:36,122 | ===> Batch : 46\n",
      "2022-01-19 04:08:36,123 | FDS disable\n",
      "2022-01-19 04:08:47,807 | Calculate Loss\n",
      "2022-01-19 04:08:47,809 | Update Loss\n",
      "2022-01-19 04:08:47,813 | Backward\n",
      "2022-01-19 04:09:09,119 | Epoch: [8][46/48]\tTime  33.01 ( 33.21)\tData 0.0140 (0.0394)\tLoss (L1) 9.657 (9.790)\n",
      "2022-01-19 04:09:09,136 | ===> Batch : 47\n",
      "2022-01-19 04:09:09,136 | FDS disable\n",
      "2022-01-19 04:09:20,851 | Calculate Loss\n",
      "2022-01-19 04:09:20,853 | Update Loss\n",
      "2022-01-19 04:09:20,856 | Backward\n",
      "2022-01-19 04:09:42,123 | Epoch: [8][47/48]\tTime  33.00 ( 33.20)\tData 0.0170 (0.0389)\tLoss (L1) 10.107 (9.797)\n",
      "2022-01-19 04:09:42,137 | ===> Batch : 48\n",
      "2022-01-19 04:09:42,138 | FDS disable\n",
      "2022-01-19 04:09:50,759 | Calculate Loss\n",
      "2022-01-19 04:09:50,760 | Update Loss\n",
      "2022-01-19 04:09:50,764 | Backward\n",
      "2022-01-19 04:10:05,278 | Epoch: [8][48/48]\tTime  23.15 ( 32.99)\tData 0.0144 (0.0384)\tLoss (L1) 10.328 (9.805)\n",
      "2022-01-19 04:10:17,204 | Val: [0/9]\tTime 11.755 (11.755)\tLoss (MSE) 248.509 (248.509)\tLoss (L1) 12.123 (12.123)\n",
      "2022-01-19 04:10:26,727 | Val: [1/9]\tTime  9.523 (10.639)\tLoss (MSE) 215.525 (232.017)\tLoss (L1) 11.023 (11.573)\n",
      "2022-01-19 04:10:36,304 | Val: [2/9]\tTime  9.577 (10.285)\tLoss (MSE) 264.966 (243.000)\tLoss (L1) 12.675 (11.940)\n",
      "2022-01-19 04:10:45,830 | Val: [3/9]\tTime  9.526 (10.095)\tLoss (MSE) 256.701 (246.425)\tLoss (L1) 12.562 (12.096)\n",
      "2022-01-19 04:10:55,439 | Val: [4/9]\tTime  9.608 ( 9.998)\tLoss (MSE) 286.938 (254.528)\tLoss (L1) 13.439 (12.364)\n",
      "2022-01-19 04:11:05,040 | Val: [5/9]\tTime  9.601 ( 9.932)\tLoss (MSE) 299.532 (262.028)\tLoss (L1) 13.418 (12.540)\n",
      "2022-01-19 04:11:14,646 | Val: [6/9]\tTime  9.606 ( 9.885)\tLoss (MSE) 260.255 (261.775)\tLoss (L1) 12.575 (12.545)\n",
      "2022-01-19 04:11:24,220 | Val: [7/9]\tTime  9.573 ( 9.846)\tLoss (MSE) 287.546 (264.996)\tLoss (L1) 12.828 (12.580)\n",
      "2022-01-19 04:11:27,490 | Val: [8/9]\tTime  3.271 ( 9.116)\tLoss (MSE) 273.053 (265.343)\tLoss (L1) 12.192 (12.564)\n",
      "2022-01-19 04:11:27,603 |  * Overall: MSE 265.343\tL1 12.564\tG-Mean 7.948\n",
      "2022-01-19 04:11:27,604 |  * Many: MSE 191.549\tL1 10.689\tG-Mean 6.748\n",
      "2022-01-19 04:11:27,604 |  * Median: MSE 449.484\tL1 17.043\tG-Mean 11.514\n",
      "2022-01-19 04:11:27,605 |  * Low: MSE 453.052\tL1 17.886\tG-Mean 13.377\n",
      "2022-01-19 04:11:27,610 | Best L1 Loss: 12.564\n",
      "2022-01-19 04:11:28,867 | ===> Saving current best checkpoint...\n",
      "2022-01-19 04:11:30,006 | Epoch #8: Train loss [9.8046]; Val loss: MSE [265.3427], L1 [12.5637], G-Mean [7.9475]\n",
      "2022-01-19 04:11:30,007 | Training...\n",
      "2022-01-19 04:11:30,009 | Load train loader\n",
      "2022-01-19 04:11:31,247 | ===> Batch : 1\n",
      "2022-01-19 04:11:31,249 | FDS disable\n",
      "2022-01-19 04:11:45,370 | Calculate Loss\n",
      "2022-01-19 04:11:45,371 | Update Loss\n",
      "2022-01-19 04:11:45,497 | Backward\n",
      "2022-01-19 04:12:07,512 | Epoch: [9][ 1/48]\tTime  37.50 ( 37.50)\tData 1.2387 (1.2387)\tLoss (L1) 9.530 (9.530)\n",
      "2022-01-19 04:12:07,525 | ===> Batch : 2\n",
      "2022-01-19 04:12:07,526 | FDS disable\n",
      "2022-01-19 04:12:19,357 | Calculate Loss\n",
      "2022-01-19 04:12:19,358 | Update Loss\n",
      "2022-01-19 04:12:19,362 | Backward\n",
      "2022-01-19 04:12:40,965 | Epoch: [9][ 2/48]\tTime  33.45 ( 35.48)\tData 0.0134 (0.6260)\tLoss (L1) 9.626 (9.578)\n",
      "2022-01-19 04:12:40,978 | ===> Batch : 3\n",
      "2022-01-19 04:12:40,979 | FDS disable\n",
      "2022-01-19 04:12:52,780 | Calculate Loss\n",
      "2022-01-19 04:12:52,782 | Update Loss\n",
      "2022-01-19 04:12:52,786 | Backward\n",
      "2022-01-19 04:13:14,452 | Epoch: [9][ 3/48]\tTime  33.49 ( 34.81)\tData 0.0133 (0.4218)\tLoss (L1) 8.897 (9.351)\n",
      "2022-01-19 04:13:14,467 | ===> Batch : 4\n",
      "2022-01-19 04:13:14,467 | FDS disable\n",
      "2022-01-19 04:13:26,304 | Calculate Loss\n",
      "2022-01-19 04:13:26,305 | Update Loss\n",
      "2022-01-19 04:13:26,309 | Backward\n",
      "2022-01-19 04:13:47,829 | Epoch: [9][ 4/48]\tTime  33.38 ( 34.46)\tData 0.0146 (0.3200)\tLoss (L1) 10.427 (9.620)\n",
      "2022-01-19 04:13:47,842 | ===> Batch : 5\n",
      "2022-01-19 04:13:47,843 | FDS disable\n",
      "2022-01-19 04:13:59,695 | Calculate Loss\n",
      "2022-01-19 04:13:59,696 | Update Loss\n",
      "2022-01-19 04:13:59,700 | Backward\n",
      "2022-01-19 04:14:21,283 | Epoch: [9][ 5/48]\tTime  33.45 ( 34.25)\tData 0.0132 (0.2586)\tLoss (L1) 10.367 (9.769)\n",
      "2022-01-19 04:14:21,299 | ===> Batch : 6\n",
      "2022-01-19 04:14:21,300 | FDS disable\n",
      "2022-01-19 04:14:33,128 | Calculate Loss\n",
      "2022-01-19 04:14:33,129 | Update Loss\n",
      "2022-01-19 04:14:33,133 | Backward\n",
      "2022-01-19 04:14:54,069 | Epoch: [9][ 6/48]\tTime  32.79 ( 34.01)\tData 0.0164 (0.2183)\tLoss (L1) 9.245 (9.682)\n",
      "2022-01-19 04:14:54,083 | ===> Batch : 7\n",
      "2022-01-19 04:14:54,083 | FDS disable\n",
      "2022-01-19 04:15:05,891 | Calculate Loss\n",
      "2022-01-19 04:15:05,893 | Update Loss\n",
      "2022-01-19 04:15:05,897 | Backward\n",
      "2022-01-19 04:15:27,460 | Epoch: [9][ 7/48]\tTime  33.39 ( 33.92)\tData 0.0138 (0.1891)\tLoss (L1) 9.470 (9.652)\n",
      "2022-01-19 04:15:27,475 | ===> Batch : 8\n",
      "2022-01-19 04:15:27,475 | FDS disable\n",
      "2022-01-19 04:15:39,292 | Calculate Loss\n",
      "2022-01-19 04:15:39,293 | Update Loss\n",
      "2022-01-19 04:15:39,297 | Backward\n",
      "2022-01-19 04:16:00,772 | Epoch: [9][ 8/48]\tTime  33.31 ( 33.85)\tData 0.0144 (0.1672)\tLoss (L1) 9.721 (9.660)\n",
      "2022-01-19 04:16:00,791 | ===> Batch : 9\n",
      "2022-01-19 04:16:00,792 | FDS disable\n",
      "2022-01-19 04:16:12,639 | Calculate Loss\n",
      "2022-01-19 04:16:12,641 | Update Loss\n",
      "2022-01-19 04:16:12,644 | Backward\n",
      "2022-01-19 04:16:34,154 | Epoch: [9][ 9/48]\tTime  33.38 ( 33.79)\tData 0.0194 (0.1508)\tLoss (L1) 9.732 (9.668)\n",
      "2022-01-19 04:16:34,167 | ===> Batch : 10\n",
      "2022-01-19 04:16:34,168 | FDS disable\n",
      "2022-01-19 04:16:46,020 | Calculate Loss\n",
      "2022-01-19 04:16:46,021 | Update Loss\n",
      "2022-01-19 04:16:46,025 | Backward\n",
      "2022-01-19 04:17:06,673 | Epoch: [9][10/48]\tTime  32.52 ( 33.67)\tData 0.0131 (0.1370)\tLoss (L1) 9.112 (9.613)\n",
      "2022-01-19 04:17:06,688 | ===> Batch : 11\n",
      "2022-01-19 04:17:06,688 | FDS disable\n",
      "2022-01-19 04:17:18,494 | Calculate Loss\n",
      "2022-01-19 04:17:18,496 | Update Loss\n",
      "2022-01-19 04:17:18,499 | Backward\n",
      "2022-01-19 04:17:40,043 | Epoch: [9][11/48]\tTime  33.37 ( 33.64)\tData 0.0151 (0.1260)\tLoss (L1) 10.003 (9.648)\n",
      "2022-01-19 04:17:40,056 | ===> Batch : 12\n",
      "2022-01-19 04:17:40,057 | FDS disable\n",
      "2022-01-19 04:17:51,877 | Calculate Loss\n",
      "2022-01-19 04:17:51,878 | Update Loss\n",
      "2022-01-19 04:17:51,881 | Backward\n",
      "2022-01-19 04:18:13,522 | Epoch: [9][12/48]\tTime  33.48 ( 33.63)\tData 0.0128 (0.1165)\tLoss (L1) 10.047 (9.682)\n",
      "2022-01-19 04:18:13,536 | ===> Batch : 13\n",
      "2022-01-19 04:18:13,536 | FDS disable\n",
      "2022-01-19 04:18:25,380 | Calculate Loss\n",
      "2022-01-19 04:18:25,383 | Update Loss\n",
      "2022-01-19 04:18:25,388 | Backward\n",
      "2022-01-19 04:18:46,293 | Epoch: [9][13/48]\tTime  32.77 ( 33.56)\tData 0.0133 (0.1086)\tLoss (L1) 8.907 (9.622)\n",
      "2022-01-19 04:18:46,306 | ===> Batch : 14\n",
      "2022-01-19 04:18:46,306 | FDS disable\n",
      "2022-01-19 04:18:58,236 | Calculate Loss\n",
      "2022-01-19 04:18:58,237 | Update Loss\n",
      "2022-01-19 04:18:58,241 | Backward\n",
      "2022-01-19 04:19:19,897 | Epoch: [9][14/48]\tTime  33.60 ( 33.56)\tData 0.0132 (0.1018)\tLoss (L1) 9.209 (9.592)\n",
      "2022-01-19 04:19:19,910 | ===> Batch : 15\n",
      "2022-01-19 04:19:19,911 | FDS disable\n",
      "2022-01-19 04:19:31,767 | Calculate Loss\n",
      "2022-01-19 04:19:31,768 | Update Loss\n",
      "2022-01-19 04:19:31,772 | Backward\n",
      "2022-01-19 04:19:53,158 | Epoch: [9][15/48]\tTime  33.26 ( 33.54)\tData 0.0129 (0.0958)\tLoss (L1) 10.223 (9.635)\n",
      "2022-01-19 04:19:53,173 | ===> Batch : 16\n",
      "2022-01-19 04:19:53,173 | FDS disable\n",
      "2022-01-19 04:20:05,031 | Calculate Loss\n",
      "2022-01-19 04:20:05,032 | Update Loss\n",
      "2022-01-19 04:20:05,036 | Backward\n",
      "2022-01-19 04:20:26,514 | Epoch: [9][16/48]\tTime  33.36 ( 33.53)\tData 0.0143 (0.0907)\tLoss (L1) 9.340 (9.616)\n",
      "2022-01-19 04:20:26,528 | ===> Batch : 17\n",
      "2022-01-19 04:20:26,528 | FDS disable\n",
      "2022-01-19 04:20:38,419 | Calculate Loss\n",
      "2022-01-19 04:20:38,420 | Update Loss\n",
      "2022-01-19 04:20:38,424 | Backward\n",
      "2022-01-19 04:20:59,775 | Epoch: [9][17/48]\tTime  33.26 ( 33.52)\tData 0.0145 (0.0863)\tLoss (L1) 9.392 (9.603)\n",
      "2022-01-19 04:20:59,789 | ===> Batch : 18\n",
      "2022-01-19 04:20:59,789 | FDS disable\n",
      "2022-01-19 04:21:11,665 | Calculate Loss\n",
      "2022-01-19 04:21:11,667 | Update Loss\n",
      "2022-01-19 04:21:11,671 | Backward\n",
      "2022-01-19 04:21:32,882 | Epoch: [9][18/48]\tTime  33.11 ( 33.49)\tData 0.0141 (0.0823)\tLoss (L1) 10.573 (9.657)\n",
      "2022-01-19 04:21:32,896 | ===> Batch : 19\n",
      "2022-01-19 04:21:32,897 | FDS disable\n",
      "2022-01-19 04:21:44,717 | Calculate Loss\n",
      "2022-01-19 04:21:44,719 | Update Loss\n",
      "2022-01-19 04:21:44,722 | Backward\n",
      "2022-01-19 04:22:06,237 | Epoch: [9][19/48]\tTime  33.36 ( 33.49)\tData 0.0143 (0.0787)\tLoss (L1) 8.477 (9.595)\n",
      "2022-01-19 04:22:06,251 | ===> Batch : 20\n",
      "2022-01-19 04:22:06,251 | FDS disable\n",
      "2022-01-19 04:22:18,061 | Calculate Loss\n",
      "2022-01-19 04:22:18,063 | Update Loss\n",
      "2022-01-19 04:22:18,067 | Backward\n",
      "2022-01-19 04:22:39,151 | Epoch: [9][20/48]\tTime  32.91 ( 33.46)\tData 0.0139 (0.0754)\tLoss (L1) 9.222 (9.576)\n",
      "2022-01-19 04:22:39,167 | ===> Batch : 21\n",
      "2022-01-19 04:22:39,167 | FDS disable\n",
      "2022-01-19 04:22:51,021 | Calculate Loss\n",
      "2022-01-19 04:22:51,022 | Update Loss\n",
      "2022-01-19 04:22:51,026 | Backward\n",
      "2022-01-19 04:23:11,923 | Epoch: [9][21/48]\tTime  32.77 ( 33.42)\tData 0.0158 (0.0726)\tLoss (L1) 10.341 (9.612)\n",
      "2022-01-19 04:23:11,938 | ===> Batch : 22\n",
      "2022-01-19 04:23:11,939 | FDS disable\n",
      "2022-01-19 04:23:23,822 | Calculate Loss\n",
      "2022-01-19 04:23:23,823 | Update Loss\n",
      "2022-01-19 04:23:23,827 | Backward\n",
      "2022-01-19 04:23:45,504 | Epoch: [9][22/48]\tTime  33.58 ( 33.43)\tData 0.0149 (0.0700)\tLoss (L1) 9.790 (9.621)\n",
      "2022-01-19 04:23:45,518 | ===> Batch : 23\n",
      "2022-01-19 04:23:45,519 | FDS disable\n",
      "2022-01-19 04:23:57,313 | Calculate Loss\n",
      "2022-01-19 04:23:57,314 | Update Loss\n",
      "2022-01-19 04:23:57,317 | Backward\n",
      "2022-01-19 04:24:18,203 | Epoch: [9][23/48]\tTime  32.70 ( 33.40)\tData 0.0140 (0.0675)\tLoss (L1) 9.491 (9.615)\n",
      "2022-01-19 04:24:18,216 | ===> Batch : 24\n",
      "2022-01-19 04:24:18,217 | FDS disable\n",
      "2022-01-19 04:24:30,064 | Calculate Loss\n",
      "2022-01-19 04:24:30,066 | Update Loss\n",
      "2022-01-19 04:24:30,071 | Backward\n",
      "2022-01-19 04:24:51,814 | Epoch: [9][24/48]\tTime  33.61 ( 33.41)\tData 0.0136 (0.0653)\tLoss (L1) 9.155 (9.596)\n",
      "2022-01-19 04:24:51,829 | ===> Batch : 25\n",
      "2022-01-19 04:24:51,830 | FDS disable\n",
      "2022-01-19 04:25:03,693 | Calculate Loss\n",
      "2022-01-19 04:25:03,695 | Update Loss\n",
      "2022-01-19 04:25:03,699 | Backward\n",
      "2022-01-19 04:25:25,131 | Epoch: [9][25/48]\tTime  33.32 ( 33.40)\tData 0.0157 (0.0633)\tLoss (L1) 10.070 (9.615)\n",
      "2022-01-19 04:25:25,146 | ===> Batch : 26\n",
      "2022-01-19 04:25:25,147 | FDS disable\n",
      "2022-01-19 04:25:37,031 | Calculate Loss\n",
      "2022-01-19 04:25:37,033 | Update Loss\n",
      "2022-01-19 04:25:37,036 | Backward\n",
      "2022-01-19 04:25:58,242 | Epoch: [9][26/48]\tTime  33.11 ( 33.39)\tData 0.0150 (0.0614)\tLoss (L1) 8.795 (9.583)\n",
      "2022-01-19 04:25:58,257 | ===> Batch : 27\n",
      "2022-01-19 04:25:58,257 | FDS disable\n",
      "2022-01-19 04:26:10,123 | Calculate Loss\n",
      "2022-01-19 04:26:10,125 | Update Loss\n",
      "2022-01-19 04:26:10,128 | Backward\n",
      "2022-01-19 04:26:31,712 | Epoch: [9][27/48]\tTime  33.47 ( 33.40)\tData 0.0147 (0.0597)\tLoss (L1) 9.597 (9.584)\n",
      "2022-01-19 04:26:31,727 | ===> Batch : 28\n",
      "2022-01-19 04:26:31,728 | FDS disable\n",
      "2022-01-19 04:26:43,465 | Calculate Loss\n",
      "2022-01-19 04:26:43,467 | Update Loss\n",
      "2022-01-19 04:26:43,471 | Backward\n",
      "2022-01-19 04:27:04,604 | Epoch: [9][28/48]\tTime  32.89 ( 33.38)\tData 0.0150 (0.0581)\tLoss (L1) 10.124 (9.603)\n",
      "2022-01-19 04:27:04,618 | ===> Batch : 29\n",
      "2022-01-19 04:27:04,618 | FDS disable\n",
      "2022-01-19 04:27:16,492 | Calculate Loss\n",
      "2022-01-19 04:27:16,493 | Update Loss\n",
      "2022-01-19 04:27:16,497 | Backward\n",
      "2022-01-19 04:27:37,574 | Epoch: [9][29/48]\tTime  32.97 ( 33.36)\tData 0.0141 (0.0566)\tLoss (L1) 9.672 (9.605)\n",
      "2022-01-19 04:27:37,589 | ===> Batch : 30\n",
      "2022-01-19 04:27:37,590 | FDS disable\n",
      "2022-01-19 04:27:49,436 | Calculate Loss\n",
      "2022-01-19 04:27:49,437 | Update Loss\n",
      "2022-01-19 04:27:49,441 | Backward\n",
      "2022-01-19 04:28:11,018 | Epoch: [9][30/48]\tTime  33.44 ( 33.37)\tData 0.0151 (0.0552)\tLoss (L1) 8.583 (9.571)\n",
      "2022-01-19 04:28:11,033 | ===> Batch : 31\n",
      "2022-01-19 04:28:11,033 | FDS disable\n",
      "2022-01-19 04:28:22,826 | Calculate Loss\n",
      "2022-01-19 04:28:22,828 | Update Loss\n",
      "2022-01-19 04:28:22,831 | Backward\n",
      "2022-01-19 04:28:44,010 | Epoch: [9][31/48]\tTime  32.99 ( 33.35)\tData 0.0153 (0.0539)\tLoss (L1) 9.778 (9.578)\n",
      "2022-01-19 04:28:44,025 | ===> Batch : 32\n",
      "2022-01-19 04:28:44,025 | FDS disable\n",
      "2022-01-19 04:28:55,816 | Calculate Loss\n",
      "2022-01-19 04:28:55,817 | Update Loss\n",
      "2022-01-19 04:28:55,820 | Backward\n",
      "2022-01-19 04:29:16,946 | Epoch: [9][32/48]\tTime  32.94 ( 33.34)\tData 0.0152 (0.0527)\tLoss (L1) 9.383 (9.572)\n",
      "2022-01-19 04:29:16,960 | ===> Batch : 33\n",
      "2022-01-19 04:29:16,961 | FDS disable\n",
      "2022-01-19 04:29:28,705 | Calculate Loss\n",
      "2022-01-19 04:29:28,706 | Update Loss\n",
      "2022-01-19 04:29:28,709 | Backward\n",
      "2022-01-19 04:29:49,820 | Epoch: [9][33/48]\tTime  32.87 ( 33.33)\tData 0.0147 (0.0516)\tLoss (L1) 9.651 (9.574)\n",
      "2022-01-19 04:29:49,833 | ===> Batch : 34\n",
      "2022-01-19 04:29:49,834 | FDS disable\n",
      "2022-01-19 04:30:01,554 | Calculate Loss\n",
      "2022-01-19 04:30:01,555 | Update Loss\n",
      "2022-01-19 04:30:01,560 | Backward\n",
      "2022-01-19 04:30:22,450 | Epoch: [9][34/48]\tTime  32.63 ( 33.31)\tData 0.0135 (0.0504)\tLoss (L1) 9.313 (9.567)\n",
      "2022-01-19 04:30:22,464 | ===> Batch : 35\n",
      "2022-01-19 04:30:22,464 | FDS disable\n",
      "2022-01-19 04:30:34,130 | Calculate Loss\n",
      "2022-01-19 04:30:34,131 | Update Loss\n",
      "2022-01-19 04:30:34,135 | Backward\n",
      "2022-01-19 04:30:54,995 | Epoch: [9][35/48]\tTime  32.54 ( 33.29)\tData 0.0142 (0.0494)\tLoss (L1) 10.042 (9.580)\n",
      "2022-01-19 04:30:55,009 | ===> Batch : 36\n",
      "2022-01-19 04:30:55,009 | FDS disable\n",
      "2022-01-19 04:31:06,734 | Calculate Loss\n",
      "2022-01-19 04:31:06,735 | Update Loss\n",
      "2022-01-19 04:31:06,739 | Backward\n",
      "2022-01-19 04:31:28,341 | Epoch: [9][36/48]\tTime  33.35 ( 33.29)\tData 0.0143 (0.0484)\tLoss (L1) 9.632 (9.582)\n",
      "2022-01-19 04:31:28,356 | ===> Batch : 37\n",
      "2022-01-19 04:31:28,356 | FDS disable\n",
      "2022-01-19 04:31:40,079 | Calculate Loss\n",
      "2022-01-19 04:31:40,081 | Update Loss\n",
      "2022-01-19 04:31:40,085 | Backward\n",
      "2022-01-19 04:32:01,726 | Epoch: [9][37/48]\tTime  33.38 ( 33.29)\tData 0.0147 (0.0475)\tLoss (L1) 9.461 (9.578)\n",
      "2022-01-19 04:32:01,740 | ===> Batch : 38\n",
      "2022-01-19 04:32:01,741 | FDS disable\n",
      "2022-01-19 04:32:13,479 | Calculate Loss\n",
      "2022-01-19 04:32:13,480 | Update Loss\n",
      "2022-01-19 04:32:13,484 | Backward\n",
      "2022-01-19 04:32:34,076 | Epoch: [9][38/48]\tTime  32.35 ( 33.26)\tData 0.0147 (0.0467)\tLoss (L1) 9.132 (9.567)\n",
      "2022-01-19 04:32:34,090 | ===> Batch : 39\n",
      "2022-01-19 04:32:34,091 | FDS disable\n",
      "2022-01-19 04:32:45,833 | Calculate Loss\n",
      "2022-01-19 04:32:45,835 | Update Loss\n",
      "2022-01-19 04:32:45,839 | Backward\n",
      "2022-01-19 04:33:07,488 | Epoch: [9][39/48]\tTime  33.41 ( 33.27)\tData 0.0143 (0.0458)\tLoss (L1) 9.589 (9.567)\n",
      "2022-01-19 04:33:07,502 | ===> Batch : 40\n",
      "2022-01-19 04:33:07,502 | FDS disable\n",
      "2022-01-19 04:33:19,262 | Calculate Loss\n",
      "2022-01-19 04:33:19,263 | Update Loss\n",
      "2022-01-19 04:33:19,267 | Backward\n",
      "2022-01-19 04:33:40,985 | Epoch: [9][40/48]\tTime  33.50 ( 33.27)\tData 0.0142 (0.0450)\tLoss (L1) 9.862 (9.575)\n",
      "2022-01-19 04:33:40,999 | ===> Batch : 41\n",
      "2022-01-19 04:33:41,000 | FDS disable\n",
      "2022-01-19 04:33:52,712 | Calculate Loss\n",
      "2022-01-19 04:33:52,714 | Update Loss\n",
      "2022-01-19 04:33:52,718 | Backward\n",
      "2022-01-19 04:34:14,094 | Epoch: [9][41/48]\tTime  33.11 ( 33.27)\tData 0.0141 (0.0443)\tLoss (L1) 9.618 (9.576)\n",
      "2022-01-19 04:34:14,110 | ===> Batch : 42\n",
      "2022-01-19 04:34:14,111 | FDS disable\n",
      "2022-01-19 04:34:25,810 | Calculate Loss\n",
      "2022-01-19 04:34:25,811 | Update Loss\n",
      "2022-01-19 04:34:25,815 | Backward\n",
      "2022-01-19 04:34:46,502 | Epoch: [9][42/48]\tTime  32.41 ( 33.25)\tData 0.0161 (0.0436)\tLoss (L1) 9.391 (9.571)\n",
      "2022-01-19 04:34:46,517 | ===> Batch : 43\n",
      "2022-01-19 04:34:46,517 | FDS disable\n",
      "2022-01-19 04:34:58,210 | Calculate Loss\n",
      "2022-01-19 04:34:58,211 | Update Loss\n",
      "2022-01-19 04:34:58,214 | Backward\n",
      "2022-01-19 04:35:19,792 | Epoch: [9][43/48]\tTime  33.29 ( 33.25)\tData 0.0151 (0.0430)\tLoss (L1) 9.770 (9.576)\n",
      "2022-01-19 04:35:19,807 | ===> Batch : 44\n",
      "2022-01-19 04:35:19,807 | FDS disable\n",
      "2022-01-19 04:35:31,520 | Calculate Loss\n",
      "2022-01-19 04:35:31,521 | Update Loss\n",
      "2022-01-19 04:35:31,525 | Backward\n",
      "2022-01-19 04:35:53,079 | Epoch: [9][44/48]\tTime  33.29 ( 33.25)\tData 0.0151 (0.0423)\tLoss (L1) 10.571 (9.599)\n",
      "2022-01-19 04:35:53,094 | ===> Batch : 45\n",
      "2022-01-19 04:35:53,095 | FDS disable\n",
      "2022-01-19 04:36:04,805 | Calculate Loss\n",
      "2022-01-19 04:36:04,806 | Update Loss\n",
      "2022-01-19 04:36:04,810 | Backward\n",
      "2022-01-19 04:36:25,661 | Epoch: [9][45/48]\tTime  32.58 ( 33.24)\tData 0.0155 (0.0417)\tLoss (L1) 9.746 (9.602)\n",
      "2022-01-19 04:36:25,675 | ===> Batch : 46\n",
      "2022-01-19 04:36:25,675 | FDS disable\n",
      "2022-01-19 04:36:37,396 | Calculate Loss\n",
      "2022-01-19 04:36:37,398 | Update Loss\n",
      "2022-01-19 04:36:37,402 | Backward\n",
      "2022-01-19 04:36:59,132 | Epoch: [9][46/48]\tTime  33.47 ( 33.24)\tData 0.0142 (0.0411)\tLoss (L1) 8.953 (9.588)\n",
      "2022-01-19 04:36:59,148 | ===> Batch : 47\n",
      "2022-01-19 04:36:59,148 | FDS disable\n",
      "2022-01-19 04:37:10,877 | Calculate Loss\n",
      "2022-01-19 04:37:10,879 | Update Loss\n",
      "2022-01-19 04:37:10,883 | Backward\n",
      "2022-01-19 04:37:32,542 | Epoch: [9][47/48]\tTime  33.41 ( 33.25)\tData 0.0154 (0.0406)\tLoss (L1) 10.159 (9.600)\n",
      "2022-01-19 04:37:32,558 | ===> Batch : 48\n",
      "2022-01-19 04:37:32,558 | FDS disable\n",
      "2022-01-19 04:37:41,218 | Calculate Loss\n",
      "2022-01-19 04:37:41,219 | Update Loss\n",
      "2022-01-19 04:37:41,224 | Backward\n",
      "2022-01-19 04:37:55,616 | Epoch: [9][48/48]\tTime  23.07 ( 33.03)\tData 0.0162 (0.0401)\tLoss (L1) 9.500 (9.598)\n",
      "2022-01-19 04:38:07,485 | Val: [0/9]\tTime 11.708 (11.708)\tLoss (MSE) 311.740 (311.740)\tLoss (L1) 14.320 (14.320)\n",
      "2022-01-19 04:38:16,999 | Val: [1/9]\tTime  9.515 (10.611)\tLoss (MSE) 261.803 (286.771)\tLoss (L1) 12.949 (13.634)\n",
      "2022-01-19 04:38:26,594 | Val: [2/9]\tTime  9.595 (10.273)\tLoss (MSE) 290.186 (287.909)\tLoss (L1) 14.125 (13.798)\n",
      "2022-01-19 04:38:36,250 | Val: [3/9]\tTime  9.656 (10.118)\tLoss (MSE) 294.374 (289.526)\tLoss (L1) 14.149 (13.886)\n",
      "2022-01-19 04:38:45,830 | Val: [4/9]\tTime  9.580 (10.011)\tLoss (MSE) 296.584 (290.937)\tLoss (L1) 14.092 (13.927)\n",
      "2022-01-19 04:38:55,442 | Val: [5/9]\tTime  9.612 ( 9.944)\tLoss (MSE) 315.530 (295.036)\tLoss (L1) 14.369 (14.000)\n",
      "2022-01-19 04:39:05,049 | Val: [6/9]\tTime  9.607 ( 9.896)\tLoss (MSE) 280.755 (292.996)\tLoss (L1) 13.579 (13.940)\n",
      "2022-01-19 04:39:14,645 | Val: [7/9]\tTime  9.596 ( 9.859)\tLoss (MSE) 315.952 (295.866)\tLoss (L1) 14.121 (13.963)\n",
      "2022-01-19 04:39:17,974 | Val: [8/9]\tTime  3.329 ( 9.133)\tLoss (MSE) 276.771 (295.045)\tLoss (L1) 13.390 (13.938)\n",
      "2022-01-19 04:39:18,095 |  * Overall: MSE 295.045\tL1 13.938\tG-Mean 9.607\n",
      "2022-01-19 04:39:18,096 |  * Many: MSE 170.227\tL1 10.524\tG-Mean 7.217\n",
      "2022-01-19 04:39:18,096 |  * Median: MSE 560.151\tL1 21.351\tG-Mean 18.265\n",
      "2022-01-19 04:39:18,097 |  * Low: MSE 740.749\tL1 25.687\tG-Mean 24.212\n",
      "2022-01-19 04:39:18,102 | Best L1 Loss: 12.564\n",
      "2022-01-19 04:39:19,371 | Epoch #9: Train loss [9.5984]; Val loss: MSE [295.0447], L1 [13.9383], G-Mean [9.6067]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "        tb_logger.log_value('train_loss', train_loss, epoch)\n",
    "        tb_logger.log_value('val_loss_mse', val_loss_mse, epoch)\n",
    "        tb_logger.log_value('val_loss_l1', val_loss_l1, epoch)\n",
    "        tb_logger.log_value('val_loss_gmean', val_loss_gmean, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"checkpoints/agedb_resnet50_agedb_resnet50_agedb_resnet50_For Final Project_adam_l1_0.001_256_adam_l1_0.001_256_adam_l1_0.001_256/ckpt.best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 20:22:05,659 | ========================================================================================================================\n",
      "2022-01-19 20:22:05,660 | Test best model on testset...\n",
      "2022-01-19 20:22:05,682 | Loaded best model, epoch 9, best val loss 12.5637\n",
      "2022-01-19 20:22:18,106 | Test: [0/9]\tTime 12.422 (12.422)\tLoss (MSE) 263.122 (263.122)\tLoss (L1) 12.634 (12.634)\n",
      "2022-01-19 20:22:29,426 | Test: [1/9]\tTime 11.320 (11.871)\tLoss (MSE) 233.709 (248.415)\tLoss (L1) 11.731 (12.182)\n",
      "2022-01-19 20:22:40,744 | Test: [2/9]\tTime 11.318 (11.687)\tLoss (MSE) 234.572 (243.801)\tLoss (L1) 12.198 (12.188)\n",
      "2022-01-19 20:22:52,113 | Test: [3/9]\tTime 11.369 (11.607)\tLoss (MSE) 235.623 (241.757)\tLoss (L1) 11.930 (12.123)\n",
      "2022-01-19 20:23:03,428 | Test: [4/9]\tTime 11.315 (11.549)\tLoss (MSE) 278.305 (249.066)\tLoss (L1) 12.797 (12.258)\n",
      "2022-01-19 20:23:14,798 | Test: [5/9]\tTime 11.370 (11.519)\tLoss (MSE) 240.040 (247.562)\tLoss (L1) 12.161 (12.242)\n",
      "2022-01-19 20:23:26,175 | Test: [6/9]\tTime 11.377 (11.499)\tLoss (MSE) 249.471 (247.835)\tLoss (L1) 12.433 (12.269)\n",
      "2022-01-19 20:23:37,526 | Test: [7/9]\tTime 11.351 (11.480)\tLoss (MSE) 271.459 (250.788)\tLoss (L1) 12.995 (12.360)\n",
      "2022-01-19 20:23:41,532 | Test: [8/9]\tTime  4.006 (10.650)\tLoss (MSE) 282.368 (252.145)\tLoss (L1) 12.870 (12.382)\n",
      "2022-01-19 20:23:41,583 |  * Overall: MSE 252.145\tL1 12.382\tG-Mean 7.959\n",
      "2022-01-19 20:23:41,584 |  * Many: MSE 185.090\tL1 10.545\tG-Mean 6.686\n",
      "2022-01-19 20:23:41,584 |  * Median: MSE 409.613\tL1 16.601\tG-Mean 11.933\n",
      "2022-01-19 20:23:41,584 |  * Low: MSE 449.978\tL1 18.061\tG-Mean 13.470\n",
      "2022-01-19 20:23:41,589 | Test loss: MSE [252.1454], L1 [12.3816], G-Mean [7.9594]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# test with best checkpoint\n",
    "print(\"=\" * 120)\n",
    "print(\"Test best model on testset...\")\n",
    "#checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2/UlEQVR4nO3dd3xV9f348df73uxFgISwCVNkCAIiCFpA3LOuaq1Sa0uXdf6crVXrt1U7HK21da+qYN1SFBEShiIj7L1HEEiAhCRk535+f5xzT+4l6ybkEpPzfj4e95F7zj3j87lJzvt85hFjDEoppRSAp6UToJRS6rtDg4JSSimHBgWllFIODQpKKaUcGhSUUko5NCgopZRyaFBQqhYi8pmITGnpdCh1oomOU1BthYgUBSzGAWVAlb38c2PMWycoHTuBnxpjvjwR51OqOUW0dAKUai7GmAT/+/ouzCISYYypPJFpU6q10Ooj1eaJyAQRyRaRe0VkP/CqiLQXkRkikisiefb77gH7ZIrIT+33PxaRhSLyV3vbHSJyQRPSES0iT4vIt/braRGJtj9LsdOQLyKHRWSBiHjsz+4Vkb0iUigim0Tk7Gb6apSqQYOCcovOQAegFzAV62//VXu5J1ACPFvP/qcDm4AU4M/AyyIijUzDb4ExwHBgGDAa+J392V1ANpAKpAEPAEZETgJuAU4zxiQC5wE7G3lepUKmQUG5hQ94yBhTZowpMcYcMsa8b4wpNsYUAn8EvlfP/ruMMS8aY6qA14EuWBfvxrge+IMxJscYkws8Atxgf1ZhH7OXMabCGLPAWA1+VUA0MEhEIo0xO40x2xp5XqVCpkFBuUWuMabUvyAicSLyvIjsEpECYD6QLCLeOvbf739jjCm23ybUsW1dugK7ApZ32esA/gJsBb4Qke0icp99rq3A7cDDQI6ITBORrigVJhoUlFsc283uLuAk4HRjTBJwlr2+sVVCjfEtVnWVX097HcaYQmPMXcaYPsClwJ3+tgNjzNvGmPH2vgZ4IoxpVC6nQUG5VSJWO0K+iHQAHmrm40eKSEzAKwJ4B/idiKSKSArwe+A/ACJysYj0s9spjmBVG/lE5CQRmWQ3SJfaafY1c1qVcmhQUG71NBALHAS+AT5v5uPPxLqA+18PA/8HLANWA2uA5fY6gP7Al0ARsAh4zhiTgdWe8Lidzv1AJ+D+Zk6rUg4dvKaUUsqhJQWllFIODQpKKaUcGhSUUko5NCgopZRytIoJ8VJSUkx6enqT9j169Cjx8fHNm6BWws15B3fn3815B3fnPzDvWVlZB40xqY3Zv1UEhfT0dJYtW9akfTMzM5kwYULzJqiVcHPewd35d3Pewd35D8y7iOyqf+uatPpIKaWUQ4OCUkophwYFpZRSDg0KSimlHBoUlFJKOTQoKKWUcmhQUEop5WjTQWHOhgPM2F7e0slQSqlWo00Hhfmbc/lsR0VLJ0MppVqNNh0UYqK8lFW1dCqUUqr1CHtQEBGviKwQkRn28msiskNEVtqv4eE6d1xkBJU+qPLpg4SUUioUJ2Luo9uADUBSwLq7jTHvhfvEcVFeAIrLK0mMiQz36ZRSqtULa0lBRLoDFwEvhfM8dYmxg0JJhdYhKaVUKML6jGYReQ94DEgE/p8x5mIReQ0YC5QBc4D7jDFltew7FZgKkJaWNnLatGmNPv9Xeyt4cU05fz4rlk5xbbr5pFZFRUUkJCS0dDJajJvz7+a8g7vzH5j3iRMnZhljRjVm/7BVH4nIxUCOMSZLRCYEfHQ/sB+IAl4A7gX+cOz+xpgX7M8ZNWqUaco0uMVr9sGa5Qw9dRQnd0lqeIc2xs3TB4O78+/mvIO783+8eQ/n7fM44FIR2QlMAyaJyH+MMfuMpQx4FRgdrgTEavWRUko1StiCgjHmfmNMd2NMOnAtMNcY8yMR6QIgIgJcDqwNVxriIu2gUK5BQSmlQtEST157S0RSAQFWAr8I14lind5HGhSUUioUJyQoGGMygUz7/aQTcU6o7pKq1UdKKRWaNt0lJzbKinkl5ZUtnBKllGod2nZQiNTqI6WUaow2HRS0+kgppRqnTQeF6AgPgvY+UkqpULXpoCAiRHk1KCilVKjadFAA8ApU6iypSikVElcEBZ06WymlQtPmg4KIaElBKaVC1OaDglfAp0FBKaVC0uaDgkfbFJRSKmSuCApVPl9LJ0MppVqFNh8UtPeRUkqFrs0HBY8HfGF8upxSSrUlbT4oeEWorNKgoJRSoWjzQcGj4xSUUipkrggK2qaglFKhafNBwSvapqCUUqEKe1AQEa+IrBCRGfZybxFZLCJbRWS6iESF8/weQdsUlFIqRCeipHAbsCFg+QngKWNMPyAPuDmcJ9c2BaWUCl1Yg4KIdAcuAl6ylwWYBLxnb/I6cHk40+AVqNLqI6WUCkm4SwpPA/cA/iHFHYF8Y4z/ocnZQLdwJsCjE+IppVTIIsJ1YBG5GMgxxmSJyIQm7D8VmAqQlpZGZmZmk9JhqirJP1LQ5P1bs6KiIlfm28/N+Xdz3sHd+T/evIctKADjgEtF5EIgBkgCngGSRSTCLi10B/bWtrMx5gXgBYBRo0aZCRMmNCkRzyz/nNjoOCZMOKtJ+7dmmZmZNPV7awvcnH835x3cnf/jzXvYqo+MMfcbY7obY9KBa4G5xpjrgQzgKnuzKcDH4UoDaJdUpZRqjJYYp3AvcKeIbMVqY3g5nCfTwWtKKRW6cFYfOYwxmUCm/X47MPpEnBe0S6pSSjWGC0Y064R4SikVqjYfFDzapqCUUiFr80FBH7KjlFKha/NBQdsUlFIqdK4ICpVV+oxmpZQKRZsPCtY4hZZOhVJKtQ5tPih4PEKlT0sKSikVirYfFNA2BaWUClXbDwoeDQpKKRWqNh8U/G0KPg0MSinVoDYfFDxi/dQH7SilVMPcExS0pKCUUg1q80HBK1ZU0KCglFINa/NBwV9S0KkulFKqYa4JClpSUEqphrX5oOB1Sgo6gE0ppRrS5oOCv6SgMUEppRrW5oOClhSUUip0YQsKIhIjIktEZJWIrBORR+z1r4nIDhFZab+GhysNoG0KSinVGOF8RnMZMMkYUyQikcBCEfnM/uxuY8x7YTy3w6NdUpVSKmRhCwrGGAMU2YuR9uuEX5m9WlJQSqmQiQnj9A8i4gWygH7AP40x94rIa8BYrJLEHOA+Y0xZLftOBaYCpKWljZw2bVqT0rBwZxEvbRT+cEYMPZO8TctIK1VUVERCQkJLJ6PFuDn/bs47uDv/gXmfOHFiljFmVKMOYIwJ+wtIBjKAIUAXQIBo4HXg9w3tP3LkSNNUT02fbXrdO8Os3J3X5GO0VhkZGS2dhBbl5vy7Oe/GuDv/gXkHlplGXq9PSO8jY0y+HRTON8bss9NbBrwKjA7nuaPs+qOSiqpwnkYppdqEcPY+ShWRZPt9LHAOsFFEutjrBLgcWBuuNABE2TVGGhSUUqph4ex91AV43W5X8ADvGmNmiMhcEUnFqkJaCfwijGkg2i4plGlQUEqpBoWz99Fq4NRa1k8K1zlrE2WXhbSkoJRSDWvzI5qd6qNyHdGslFINcUFQ0IZmpZQKlQuCgvWzVIOCUko1qM0HhQix5j8qKdegoJRSDWnzQUFEiI30aklBKaVC0OaDAkBslFfbFJRSKgSuCArRERoUlFIqFK4ICrFRWn2klFKhcEdQiPRqQ7NSSoXANUGhtEIHrymlVENcERRitKFZKaVC4o6gEOHRNgWllAqBK4JCpNdDRZVWHymlVENcERS8HtFnNCulVAhcERQivEKlBgWllGqQO4KClhSUUiokrggKXo+HiioNCkop1ZBwPqM5RkSWiMgqEVknIo/Y63uLyGIR2Soi00UkKlxp8LNKCtrQrJRSDWkwKIhIvIh47PcDRORSEYkM4dhlwCRjzDBgOHC+iIwBngCeMsb0A/KAm5uc+hBpm4JSSoUmlJLCfCBGRLoBXwA3AK81tJOxFNmLkfbLAJOA9+z1rwOXNy7JjadtCkopFZqIELYRY0yxiNwMPGeM+bOIrAzl4CLiBbKAfsA/gW1AvjGm0t4kG+hWx75TgakAaWlpZGZmhnLKGoqKiti7t5zyyqomH6O1Kioqcl2eA7k5/27OO7g7/8eb95CCgoiMBa6nuqrHG8rBjTFVwHARSQY+BAaGmjBjzAvACwCjRo0yEyZMCHXXIJmZmfRJ74Jv51aaeozWKjMz03V5DuTm/Ls57+Du/B9v3kOpProduB/40BizTkT6ABmNOYkxJt/eZyyQLCL+YNQd2NuYYzWF1yP4DPi0CkkpperVYFAwxswzxlxqjHnCbnA+aIy5taH9RCTVLiEgIrHAOcAGrOBwlb3ZFODjpiY+VJFeAaDKaFBQSqn6hNL76G0RSRKReGAtsF5E7g7h2F2ADBFZDSwFZhtjZgD3AneKyFagI/By05MfGq/HyqY2NiulVP1CaVMYZIwpEJHrgc+A+7Aaj/9S307GmNXAqbWs3w6MbkJamyzCY5UUtFuqUkrVL5Q2hUh7XMLlwCfGmAqsrqWthtcfFHSmVKWUqlcoQeF5YCcQD8wXkV5AQTgT1dz8bQpaUlBKqfo1WH1kjPk78PeAVbtEZGL4ktT8tE1BKaVCE0pDczsReVJEltmvv2GVGloNbVNQSqnQhFJ99ApQCFxjvwqAV8OZqOambQpKKRWaUHof9TXGXBmw/Eio01x8V0Rom4JSSoUklJJCiYiM9y+IyDigJHxJan4R2qaglFIhCaWk8AvgDRFpZy/nYY1EbjWqq480KCilVH1C6X20ChgmIkn2coGI3A6sDnPamo2/oVlLCkopVb+Qn7xmjCkwxvjHJ9wZpvSEhdduU6jQp68ppVS9mvo4TmnWVISZlhSUUio0TQ0Krerq6m9o1jYFpZSqX51tCiJSSO0XfwFiw5aiMPB3SdWSglJK1a/OoGCMSTyRCQknf+8jbVNQSqn6NbX6qFVx2hS0+kgpperlkqBgtylo9ZFSStXLHUFB2xSUUiokrggKzohmbVNQSql6NSkoiMiaELbpISIZIrJeRNaJyG32+odFZK+IrLRfFzYlDY0RodNcKKVUSOrrknpFXR8BnUM4diVwlzFmuYgkAlkiMtv+7CljzF8bl9Sm8+rgNaWUCkl9cx9NB96i9rEKMQ0d2BizD9hnvy8UkQ1At6Yk8nhFerWhWSmlQiHG1H6hFJEsYIoxZm0tn+0xxvQI+SQi6cB8YAjWvEk/xnpYzzKs0kReLftMBaYCpKWljZw2bVqopwtSVFSELzKeWzOKuWFQFGf3jGzScVqjoqIiEhISWjoZLcbN+Xdz3sHd+Q/M+8SJE7OMMaMadQBjTK0v4EygZx2fjaprv1q2TQCygCvs5TTAi9We8UfglYaOMXLkSNNUGRkZJu9omel17wzzysLtTT5Oa5SRkdHSSWhRbs6/m/NujLvzH5h3YJkJ8Vrtf9XZ0GyMWWCM2V3Hx+PrWB9ERCKB94G3jDEf2Mc9YIypMsb4gBeB0aEc63jo8xSUUio0Te2S2uDU2SIiwMvABmPMkwHruwRs9n2gRvVUc/O3KZTrM5qVUqpeoTx5rTahTJ09DrgBWBPwTOcHgOtEZDhWA/ZO4OdNTEPIoiPsoFCpQUEpperT1KDQYD2MMWYhtQePmU08Z5OJCFERHkorq070qZVSqlVxxdTZYJUWyiq0pKCUUvVxxdTZADGRXsq0+kgpperlirmPwF9S0OojpZSqj7uCgpYUlFKqXq4JClb1kZYUlFKqPq4JClpSUEqphrkoKHgp1TYFpZSql3uCQqSWFJRSqiGuCQoxEV4dp6CUUg1wTVCIjtQRzUop1RD3BAUd0ayUUg1yUVDQLqlKKdUQ1wSFGG1oVkqpBrkmKPi7pJo6Hj+qlFLKVUHBg89ApU+DglJK1cU1QSEm0gugVUhKKVUP1wSF6EgrqzqqWSml6ha2oCAiPUQkQ0TWi8g6EbnNXt9BRGaLyBb7Z/twpSGQ/5GcWlJQSqm6hbOkUAncZYwZBIwBfi0ig4D7gDnGmP7AHHs57KIj7OojLSkopVSdwhYUjDH7jDHL7feFwAagG3AZ8Lq92evA5eFKQ6CYSC0pKKVUQ+REdNEUkXRgPjAE2G2MSbbXC5DnXz5mn6nAVIC0tLSR06ZNa9K5i4qKSEhIYFVuJU9llfHgmBj6JnubdKzWxp93t3Jz/t2cd3B3/gPzPnHixCxjzKhGHcAYE9YXkABkAVfYy/nHfJ7X0DFGjhxpmiojI8MYY8xXW3JNr3tnmEXbDjb5WK2NP+9u5eb8uznvxrg7/4F5B5aZRl6zw9r7SEQigfeBt4wxH9irD4hIF/vzLkBOONPgF63VR0op1aBw9j4S4GVggzHmyYCPPgGm2O+nAB+HKw2B/A3N2iVVKaXqFhHGY48DbgDWiMhKe90DwOPAuyJyM7ALuCaMaXBoQ7NSSjUsbEHBGLMQkDo+Pjtc562LdklVSqmGuW5Es5YUlFKqbu4JCtqmoJRSDXJRUNCSglJKNUSDglJKKYdrgoKI2M9p1uojpZSqi2uCAlilBS0pKKVU3dwVFCK9lFVqSUEpperiqqAQE+mhrEJLCkopVRdXBYXoCC+lWlJQSqk6uSwoaElBKaXq46qgEBvppbhcSwpKKVUXVwWF5Lgo8orLWzoZSin1neWqoNAhPlKDglJK1cNVQaF9fBR5Ryv8T3xTSil1DFcFhY7xUZRX+Sgqq2zppCil1HeSq4JC+7goAPKOVrRwSpRS6rvJVUGhQ7wVFA5ru4JSStUqnM9ofkVEckRkbcC6h0Vkr4istF8Xhuv8tXGCwtGyE3lapZRqNcJZUngNOL+W9U8ZY4bbr5lhPH8NnZJiANhzuOREnlYppVqNsAUFY8x84HC4jt8UXdvF0KtjHPM257Z0UpRS6jtJwtk9U0TSgRnGmCH28sPAj4ECYBlwlzEmr459pwJTAdLS0kZOmzatSWkoKioiISHBWX5zfRnzsyt5/pw4PCJNOmZrcWze3cbN+Xdz3sHd+Q/M+8SJE7OMMaMadQBjTNheQDqwNmA5DfBilVD+CLwSynFGjhxpmiojIyNo+fl5W02ve2eYwtKKJh+ztTg2727j5vy7Oe/GuDv/gXkHlplGXrdPaO8jY8wBY0yVMcYHvAiMPpHnB0iIjgSgqFTHKiil1LFOaFAQkS4Bi98H1ta1bbgkxEQA6AA2pZSqRUS4Diwi7wATgBQRyQYeAiaIyHDAADuBn4fr/HVJjNagoJRSdQlbUDDGXFfL6pfDdb5QOSUFrT5SSqkaXDWiGSA+yl9S0KkulFLqWK4LCol2SaFQSwpKKVWD64JCgrYpKKVUnVwXFOKja29T+GjFXj5bs68lkqSUUt8ZrgsKUREeoiM8FJVV4vMZsnZZA6pvn76SX761vIVTp5RSLct1QQGsdoWC0kpeXLCdK//1NX+dtcn5rLA0tAbo0ooqxj42h1nr9ocrmUopdcK5Mij0Tonnk5V7eS8rG4BnM7Y6n63JPhK0bUl5Va2P7/w2v4R9R0q5/4M1TUrDKwt3MGP1t03atzYb9xfwzJdb9FGjSqnj4sqg8OwPRxAfHcGWnKIan63fV+C8Lyqr5OTff84zc7bU2O7QUetBPXlNfGDPH2as55a3V/DFuv1sy62Zjsa66dWlPPXlZiddrcmX6w9w9b+/psqnAU2plubKoJCWFMN1o3sGrbt0WFeS4yKDLtA5BaUAPD9vOwCHj5bzty82cbSsktxC60E9x3tjPvXNLH7y2lJW7cnn6n9/TUl5lfPZ2r1HyA8IOmuyj1BZ5av1OCUV1n47Dh49vgTVIWvXYYY8NItDRc3/gKKfvrGMpTvzQq66U0qFjyuDAkCf1Pig5UcvH0L/Tgms/7aAL9bt5+9ztnDYvuv2X3CveX4R/5i7lQVbDjpBAYLbIfbml/DaVzvqvestragKWt51qJg73l3J0p15bDpQCMDOg0e5+B8LufyfXwGQnVfMJc8u5PefrKv1mP5Bedtziygpr+L9rGx8zViV9OzcrRSVVToN834VVb6gQHY8vitjRzbtLySnsLSlk6FUi3BvUEgJnmu9XWwkfVMTWJV9hKlvZvHk7M3sySt2Pi+v9LHVrm56a/Eu1u6tbnv4csMBHv9sI2uyjzDu8bk8/Ol6Fm49WOt5jTG8tGB7jfXbc607/JLyKv72xSYufXYhADsPFbP5QCH5xVbgeXvxborLqy+e8zbnkne0nAivOMe55/3V3PXfVWzLr71U0RhPzd7M52v3UWkHOa8n+BkUD32yjqEPz+KD5dlc9PcFLN5+qNbj+HymwbEhBWEsKeQUlLLncHHDGwLnPT2fCX/JrPPzFbvz+GjFXsAqPfrs7+a/y/awak9+rft8uCKb9d8W1PqZUt8lrg0KvY8pKQD84LQeQcsZG6uf0Pb1tuqL/IItB/mv3UgN8IdP1/Pvedu4xL6QA8zdcACwLvJ7863HfxpjmL3+AH/9YrOz3UlpiURFVP8a8orL+cfcrRQE3DV/m18SdBd9939XA3CoqIwpryzhl29lccCu6vpm+yE+XWU1YOeVBZcUDhaVsWFfAVU+w53TV7LSvoCVlFfx2Gcbaly0fT7DM3O28Iv/LKfCrrYK3GZrTiFvL95Npc9w57urWPdtAY/+bz21+esXmxjy0Cx+//Fa0u/7X9A5/BoqKVRW+fh660G25xbxu4/WUFnl48GP1vLywh21bm+MIXNTDj6fYfSf5nDmnzPqPX6Vz3nuB8V2BwP/9xro6n8v4vbpK9meW8SIR2fzzByrgf/u91ZzmV2yC1RSXsXd/13Na1/Xnk6lvkvCNiHed11CdAQ/O7M3Y/t2ZHTvjgCc2rM9Kx48h99+tIaZa/bzeUB30/98s7vGMbolx7I3v4S84uo73DF9OhAd4SVzcy4r9+Tz8sIdzkX65vG9a1zAOiVFM6hrEh/ad56rA3o/9U2NZ1vuUfKKy4mLqv5V/W/NPpb88UtG9EwG4Jvt1U89XRWw/+GS4KDwq/8sZ8nOw5wzKI3Z6w+QtTuPeXdP5L3l2Tw/bzsRHuHu8wY62+8LuCD6q9LyA/J648tLAPj59/o47S4J0RF8uupb5m7M4a9XD3NKFm8vsb6/NxbtAqwqtNzCMr4KKFHVN0nh2r1H+OsXm8jcVB2orz+9F+9lZRMf7WXK2F5EeIPvceZlV/LarKU8dsVQZ50xBqnjiXtjH5tD9/axzvJbi3fzu4/WMuv2szipcyKz1u1n5pp9TrXcm99Yefls7T5+Mq53nWlflZ1Ppc9wqKh1dAKoqPJhDEE3K8o9XP1b/+1Fg5g0MM2Z+gKgfXwUz10/klN7JlNe6SMuyktspJcvNxwgLspLx/goAP79oxG89dPTaxxzQFoiAzsnsutQMZf/8ysnIAC13tHGRHp54spTWHDPROu487YFHQvgjumrnLEUw7q3AyC3sIxZ6w4EHeuKEd0AOKNvR+KivBwura4+ytp1mCU7reAxe721365DxSzdWR1Q9uZVl2iMMewMaLTefMCqOvMHhfJKH98eKeWKEd24cWy6s9032w/zm3dW8OGKvfR9YCYLtxwka9fhGo8+3Ztfwpl/zuC+D9YQF+UFrAbnwCoWn33nvmjbIS7+x8KggABwwTMLKKmo4mBROV9vq1lttSrXausI7PobGNQClZRXkVNYxvLd+c66L+zvyd/O8/M3s/h45bckxlgPavJvu/lAES8GVAku3n6I7ICqR387zMF6eoYdKa7glYU7SL/vfyE35t/57kru/2ANH67IbnjjRpj85DyGPjyrWY95rI37C5wbDfXd4uqgUJ8p9oWuuLyKC4Z0Bqw7/U9+M54Pf3UG5w/pQnpKdRXU6b07AHbbRKfQnw2bkhBNVISHHh3ianzmn7wPcLrPntEvpdbjPHnNMH574cmcNSCVx64YSpd2MRwute5ofT7Dlf9aBMBN49KD9rv634s4YvdwmrMhh4VbDjLmsTmMeHQ2j86wqoL8gchKRyHvLtvjVKuc3rsDXdvF1Jm/H728mCv/tajGBWCLfaEFOLN/dZ5e/3onlVU+fvfRGvo8MJPbpq3kuhe/qfP4fh+t3MvRY6q/9hRaQfGrrdUBw1+Vd7Ssksc/2+g0KK/YU/NR4f7eZ/5g6XekxAos2wK6NAeOdfnBC98w/okM+j4wk0XbDjlB4VBRGf+Ys4UN+4LbFowxnPbHL/mD/X2va6DtobzSqjb7YPle3lmymzumrzru8SnnPjWPP83cAFg3C2WVjW+P2nekxPl+62OM4fynF3DtC4safQ4VfhoU6nDZ8K48dsVQnrl2OH+5ehhv//R0bj27P92SYzm1Z3tnu79cdQpP/2A4t08eAMBZA1LpmxocFF6eMopLh3UF4Jlrh7PkgbN54sqh/HpiX+47fyDHuusc61indE8OWi8CPdrXDB4Ag7u2o2NCNG/8ZDS9OsbTNTmWQ6UGn8+wOce6AA/rkcx9F9Q8n7+No7Cskh+9vJgDBWXkFVewcX8h3ZJjefDiQc62M1bv4573VjsD/7q0i61RHTOkW1KtaQzkL3kADOlaHXRE4IPle53quk9WNTzA78z+KXywfC+DH5rF6ux8Kqp8PPLpOg6W1LxQZtsX+IVbD/LvedsY/cc5/PzNZSwOqILz27jf+t625xYFtX34NdRwXuUzzNucy/Ldec65/zZ7Mze9ujRou6xdeZQHdDX+cMVeisoqMcbwwvxtLLNLc1tzCtlx8CjzNuc6VVd+3x4JbvvYcKiqUT2oNh8o4oX5NTtANMbYx+Yy7vG5DW633w62/r+BwtKKsHYyCFV+cTkzdf4z97YpNEREgsYy1HWHfvWo6sbp9X84j7ioiBoXi67JsTxy6WBuPbs//exSxA9OCx4nEeiXE/oycWAnBndN4ncfVT+xNCE6glPsu/bzB3fm83X7+evVw+jZIY6TOicGHaNvagILthyk729ncvXI7gD849pTiY7wMuM34/n+c19RURV8obvt7P7OQL2v7pvE9twiBnZOIiUhil9N6MtzmdVVW/7tuiZbpYQ3bx7NDXYbw9Bu7Vi7t/673U0BJYV+ASWraUv38Mmqb+kQH8Xgrkks2GK1OZw9sBNnn5zG6N4diI7wOI3Gg7smcePYdGe7a55fRGlFzbvcTonR5BSWMXfjASp9Ph6wR6IP75HMrHUHWLyjZlDw237waK0DHevzp+8P5YEP17Att4j84gp6p8Q7Y0j2F5Ty5qKd3GCXRv3VeX4frtjLhyv2ct3onkxfupvJJ6dRXunjhy8tJi7Kyxl9a/4trsnOp1uy1R7i8xmeWFrKMysz2PjoBdwxfSUd4qOCgrtfYWkFt01bWWsefD6DxyPszS/hV//J4p/Xj6B7HTcl9THG6nmWGBPJz95YFlRqnPLKEuZtziXK6yHj7gnERXppb1fRBtqaU8juw8XkHa3gSvvvubn9+NWlrNyTz1f3TXK+SzfSkkIz8jcGJ0RHsOj+Sc76Lu1iaB8fFXTxq02k3a00wuthSLd2Ne7Ak2IiGdKtHWsfOY/nrh/B+788g6tGdme0XXUVaFgPK3gYA+8uy2bCSan06GD9oQ/p1o7Pbz+Lm8al8+Wd33P2ucMuoQB0bRfDmf1TSU2MRkS45/yBjE63ztM3oOdWl3bWMc/sn8qAtATn+A353+rqO7KOCdFBnxWXVzE6vQP3BpSinr9hJD88vSf9OiUEVbX979Yzg6qfjg0IF51iPRb8aFklPTvE8e6ybG55e4XTu+vNm0cjYrU1+Ns2AnWMj2JbbhEPfbKWwF/H2D4d683fZcO7kpoYzRy7F9q5g9KCPn954Q6OllXy0MdreX7+dpJiat6fvbNkNz5jlSQ+Wml1RGgfF8WX9jE//vU4/v2jkYjA2r0FvLxwB3lHy51R9qUVPnYfKmbG6m9rjC/xe2PRLuZuzHGWA8fXFJVXsu7bIzzx2UZWZR9xOhPUJvBG6NiqrIxNOQx9+Auuf+kbq4NDQFrmbbbaicqrfIx7fC5n/jmj1qqwyU/O5yevLeOu/66q8VlpRRXPZW4NGi9TVmlqjAeqT2WVz+mNt87ubl7b+JuyyvqPuT23iAl/yXCqHpvir7M28fSXmxveMEzC+YzmV4CLgRxjzBB7XQdgOpCO9Yzma4wxtf+1tnIdAu522sVGhrTP4gcm1zvgzN8g7v85slf7OrcNrHq674KB3DQuPSjI9E1N4KFLBjv/gP5eN//84QgKSitq7aHz4MWD2JpbyKST0rjz3ZUYqqciB5x66MEB1UFO+8bRcsb3S2HX4WJe/WoHM9dYPbsuGda11nz06hjH4K5JPH7FUM4ckFqjZ1GgmEgvC+6ZSIRXGPtYcPXFhUO6UFhaydUju/PGop3sPmasQmJMJL07xrP94FHOH9yZD+xeYLdM7Mf6fQWM7dORP87cwDfbD3PnOQN4crb1zzqoaxKL6hiTgf29JMdGkltYRvu4SIbaJby7zzuJPYeLmbMxh2lL9/C63RvrkcsGc8f0VXRpF8PTPxjO3I05PG9X5xw6Ws7/Vu/j/MGdufnM3lz9b6sufliPZIb1sHrB+ds0vs0v4coR1XfSD368looqQ15xOY98uo5BXZKc0m1+cTkfLA9upC4oqa7G+enry1gSUIJ685td9EmNZ2zfjgzsXF1FuHj7If5ot0cA5BVX0CE+ioJSq/E8wu6BFti2U5eiskqW7crjtPTqG51jq+5KK6qIiawO4O8s2c2fP9/E8l15PH7lKaQkRPPLOcWkLslg8QOTGzwnwM5D1Z0q1u8rIDLCw02vLuWTW8Zx6Gg5lVWGId2SOOPxuZzVP5XXfzK61uO8uGA7Ow8V8/qinVT54K5zBxBZz99ubfy/S3+V9IkWzuqj14BngTcC1t0HzDHGPC4i99nL94YxDS0mOqL6j7auLpDH6lBLsfmjX4/j2blb+XLDAdrHhxZcAPqkxHNxn0huvWys04upNiLCl3d+zwlc/jvr2gzt3s65uL3849NqfJ6WGMOuQ8V0TrKqlAZ2TqwxnUinpBgGdEp0gsIjlw7G6xFevHEUS3Yc4sUFVg+tbu2ttoprR9dezfbQJYOCBtL5Sw8L7pkYNB6hQ3wUb9j/wL1T4nlpwXYeu+IUbpu2gjP6Wnf7PxnfmzkbDvCrif2coHDb5P5Eej1kBNxFnzs4jaNllQzp1q5GcAk0+eROgHVXD1ZX5/MHd+alG0cxaWAnnpy9mcNHy5m+dDc9OsTy7HUjGNYjmaHd2tEtOY7YKC/7jmkjOFpexbj+KYyw27MCG/d7p8Q7bSVbcoo4ENCW4L8T33+klFe/2glYVZ5frNvPn2ZuYOeh4HwEXhyX1FKl9sinVmP4daN7cN3ongxIS+TFBTuCulIfKCilQ3wUj83cyDtLdtO9fSyRXuGGMem88lXDYzW25hQFBYVj5/M6fLScrsmxbM0pxOvxOL3kvtyQw3lPzeeqUd3xGThQUMaWA1bHiNsnDwi6gal5zup8b9hX4FR/Lt2Z53S4eOdnYzDG+k7LKquc//HSiioOFJTyvb9kkmKXev+ZYVW1npbenlN7ticqwsNXWw/Sr1OC0+ZojMFnggeEBpZEXv1qB706xjFpYHApM9zCFhSMMfNFJP2Y1ZcBE+z3rwOZtNGg0FyG90jmH9edyrzNuYztW3+VRSAR4aoBUfUGBL+GqrVC9ewPrXR2bhfDovsnOV03j9Uurnq9v9rknEFpnDMoDRHhhfnbnWk76nJTHeMCOiUFV0UFBtIh3drx9LWnAvDCjaOc9T8a04sfjekFwLs/H8vcjTnO3Z1/OhSPWAMN77/wZACey6zubeQ3+eQ0UhOjuee8k4Dq3mMD0hKJ8HqYbFchpSZGU+UzbD5QxF3nDGBYj2QA+nWq/l2d1rsDHeOjOL1PBxZuOUhBaSXj+6Xg9Qhf3nkWSQHfbeANyPzNucy3A8F1o3vyjj0+JLA30R3TVzrjYgLbOgA2B7T1HGvW7Wfxp5kb2HXoKO8s2cM7S/YAVvXoyF7tGZCWyDtLdrP/SCknpSU6o9uz80pIS4rm95cMYmj3JO6YXrMK6KNfj2NvXgm/eWc539o9mHw+w4sLtgdNUglWL6foCA+Tn5wPWP8jfoeOlgdVc53zlLXNaekdiPR6+PvcLdx2dn/G90uhuKLK+R79c56dlt6efUdKnRJTYHff3ICuwjkFZfToEMdjMzfw/PztTD7Z+t0etLcRsaput+YUcfPry0hNjCa3sIzoCA+b/u8CAF5asIM/ztzgtEUWlFawP+BmwB+Adz5+UZ2/k3CQcE61bAeFGQHVR/nGmGT7vQB5/uVa9p0KTAVIS0sbOW3atCaloaioiISE5rnoNVbmngoSIoVRnVumPb8l896QH39uXYheOz94ZHlJpWHWzgou7hPpVDs09dgAT0+IJTmm6U1nxhg+31nJqDQvqXHVxzlU4uOzHRX0budh+qZyzuwWyYV9IomPrE7zY4tL2JTn4+YhUZzZvfoivmR/Jc+ttC4ed4+KYXBKzbaMQM8sLyW70Mefz6rZ0wvgvc3lzNhewc1Donh5bfVd9TMT43hiaQnto4V1h2rvYpoaK+QG9NI6tZOXFTnB9eYX9I6kTzsPp9l/xz5jeGZ5mTMOBODcXhFM7hXJPfOtC/qQFC/rDlbhP3KPRA+PjoulvMrw0dYKhqV6eTKrlAdOj6F9tIekaCtfd2YWExsBJ3Xw8vXeSkqbZ1otIj0Q2NyUEiscLDG8fG4cb6wvZ1VuFR6x0j0/u/ZeZeelRzBrp/XZr4ZFU1BueH9LOSWVkBQFBbUMuxiZ5iXrQHAmLuwdyeldvDyVVUZ+meG2EdH4DPxjRRnn9orgi13B53/1vLiQaxsg+P9+4sSJWcaYUQ3sEqTFgoK9nGeMqbti3DZq1CizbNmyJqUhMzOTCRMmNGnf1u67nHf/VBfhuAsKnEZj0/+dH3QnfSKd/bdMtuUe5f1fnhHUbrJ4+yF+8II19mL1w+cG3fXXJrewjNKKqlrHsoBV5bDlQBFDurXj2blbnC7G/u/2f6v38eu3racKThnbiw7x0TxlN2Q+c+1w3svKdnpvHeu560dw4dCaVYpvfrOLBwN6xj18ySCmnJHOXe+ucqrgwKoaqfIZxvXryFs/HVNvPgHOe2p+UM+0QN3bxzrVZL06xjEgLdHpufXnq05h3ubcoA4Mxzp7YCdG9+7AY59tdNY9dsVQ55ko4/ulMLJXe56Zs4VIrzi98wZ2TnS6J/tFeMSZD6w+oWx32fCubMstqrPH3qL7JzkdOkIR+H8vIo0OCif6FvaAiHQxxuwTkS5AToN7qDYp8/9NCNuzH9752RgOHy1n44Z1LRYQwKqu2pZ7lN4pwaWhlESriishOqLBgABWdVN9oiO8To+vWyb15wen9WRmRvU8XP4qtPgoL49cNgSAn53V2+ktd86gNHILy/iePQngazedxursI3z/1G51BqL+x1Q5JsVGIiI8+YPh/OmKoQx88HMAbhjTi9e+3kmEJ7TSmr8q6+2fnc6stfudhvhOidF8est4Tn10NgDz7p6Iz2fo88BMwOoGXdtYk7SkaA4UWKWyC4d24cqR3THA43ZgCHxIVt/UeDrbbTVd2sU67UaXDOvKxv2bgo4bSkDwT4PTkI9XWmNxUhKiOVhUxuSTO7F0Z54zSHLDvoJGBYXjdaK7pH4CTLHfTwE+PsHnV98R6Snx9faeOh5j+3bkolO6MDKtZYfhPHbFUGb8ZnyNDgTpHeP52Zm9+d+t48Ny3tTEaHolVQdDf/vMmIButIFzacVFRdCrYzzXje5JTKSHs/qncuvZ/esMCP5jTZ86hmW/m8wNY3px7uDOzmcxkV6e/eGpXDmiO+cOturaAxuw6/PUD4ZzxandGNunI7+/ZDBz7/oes+84i/n3TCQ5LjiAejzCtfYkln1TE5wuxbdM7MewVOv9I5cOcbbv2dHKz/dP7RZ0HH9X8L6dEki1G4o7JlT/zk7uksgnt4xzluOivKR3jHMCY13VnBcO7RzUfbtXx5rf5912+xPAv340gsknd+LHZ/R2biTuPGcAI3vV7HIeTuHskvoOVqNyiohkAw8BjwPvisjNwC7gmnCdX6mWFhcVUeuYDa9H+O1FNQeShcvQbu2474KBzgW0Ln/6/hAevWwwnhDbck63g8yjlw+p8dnFp3Tl4lO6OtO8nx1iD5qLTuni9IDzCvQ5ZnaAv1x1itMDzkrzUB646GSiIjzcde4AkmIjuPXs/oyK3lej6tQ/G0BaUgzb/3ShU8oY1y+FzE25dIyPdjoqfG9AKivsua26JccFXdAfvmQw/dISmLPhAFtyiuidEs+WnCJ+Or4395w/kOF/+ILicqu678fjejvVbL06xrPL7u01/+6JGAzd28exZMdh5m3OZUTP9rw0xerV98HybFbuyeeaUT1C7tLeXMLZ++i6Oj46O1znVErV5PEIv/he3wa3ExHnuRzNJS4qguUPnhM0j9fxCJxBAKy8+avgkuOigmb5PVangGo4j0e485wBDEhLpG9qPKUVVZw5IIWkmEg+/NUZDOuezNNfWqP2u7WPDRoXcY0dXId1T+aCIV14e8lutuQUcc1pPYiK8FBsD3rr1ymBsX06Eu31cM/7q4MGKHZKinaO+cKNIzlaVhXUNXV4z2Tmbc4lJaFmN/Vw02kulFJhVdv4mxPpxRtHsSY7v0YJ6Naz+zvvp00d67wPnNsMqgeLfnrLeOKiq4OD1yMMsUthlw3rWqP7d/9OiYgIZ/SrHg8zw24IDwwy0RHeGm1fPzq9F1eN7F7voM1w0aCglGrT/GNgGuuDX50R9LS+wGqrQEkxkU5VWiD/XX739nFOT7Az+nZk2c6GJ3HweCSo3edE0qCglFK1GNGzvTOCvDHe/unpZOeV1Dq24D83n074BgE0Dw0KSinVjOqaURkIuRG/JeksqUoppRwaFJRSSjk0KCillHJoUFBKKeXQoKCUUsqhQUEppZRDg4JSSimHBgWllFKOsD5kp7mISC7WrKpNkQLU/gSRts/NeQd359/NeQd35z8w772MMamN2blVBIXjISLLGvvkobbCzXkHd+ffzXkHd+f/ePOu1UdKKaUcGhSUUko53BAUXmjpBLQgN+cd3J1/N+cd3J3/48p7m29TUEopFTo3lBSUUkqFSIOCUkopR5sNCiJyvohsEpGtInJfS6cnHETkFRHJEZG1Aes6iMhsEdli/2xvrxcR+bv9fawWkREtl/LjJyI9RCRDRNaLyDoRuc1e75b8x4jIEhFZZef/EXt9bxFZbOdzuohE2euj7eWt9ufpLZqBZiAiXhFZISIz7GU35X2niKwRkZUissxe1yx/+20yKIiIF/gncAEwCLhORAa1bKrC4jXg/GPW3QfMMcb0B+bYy2B9F/3t11TgXycojeFSCdxljBkEjAF+bf+O3ZL/MmCSMWYYMBw4X0TGAE8ATxlj+gF5wM329jcDefb6p+ztWrvbgA0By27KO8BEY8zwgDEJzfO3b4xpcy9gLDArYPl+4P6WTleY8poOrA1Y3gR0sd93ATbZ758Hrqttu7bwAj4GznFj/oE4YDlwOtZI1gh7vfN/AMwCxtrvI+ztpKXTfhx57m5f+CYBMwBxS97tfOwEUo5Z1yx/+22ypAB0A/YELGfb69wgzRizz36/H0iz37fZ78SuDjgVWIyL8m9Xn6wEcoDZwDYg3xhTaW8SmEcn//bnR4COJzTBzetp4B7AZy93xD15BzDAFyKSJSJT7XXN8rcf0dwpVd8dxhgjIm26z7GIJADvA7cbYwpEqh+M3tbzb4ypAoaLSDLwITCwZVN0YojIxUCOMSZLRCa0cHJaynhjzF4R6QTMFpGNgR8ez99+Wy0p7AV6BCx3t9e5wQER6QJg/8yx17e570REIrECwlvGmA/s1a7Jv58xJh/IwKoySRYR/81eYB6d/NuftwMOndiUNptxwKUishOYhlWF9AzuyDsAxpi99s8crBuC0TTT335bDQpLgf52b4Qo4FrgkxZO04nyCTDFfj8Fq67dv/5GuyfCGOBIQFGz1RGrSPAysMEY82TAR27Jf6pdQkBEYrHaUzZgBYer7M2Ozb//e7kKmGvsCubWxhhzvzGmuzEmHet/e64x5npckHcAEYkXkUT/e+BcYC3N9bff0g0mYWyIuRDYjFXP+tuWTk+Y8vgOsA+owKonvBmrrnQOsAX4EuhgbytYPbK2AWuAUS2d/uPM+3isetXVwEr7daGL8n8KsMLO/1rg9/b6PsASYCvwXyDaXh9jL2+1P+/T0nlopu9hAjDDTXm387nKfq3zX9+a629fp7lQSinlaKvVR0oppZpAg4JSSimHBgWllFIODQpKKaUcGhSUUko5NCgoZRORKnvWyVUislxEzmhg+2QR+VUIx80UEVc+RF61PhoUlKpWYqxZJ4dhTaL4WAPbJwMNBgWlWhMNCkrVLglr+mVEJEFE5tilhzUicpm9zeNAX7t08Rd723vtbVaJyOMBx7vafv7BZhE588RmRanQ6YR4SlWLtWcdjcGaeniSvb4U+L6xJtxLAb4RkU+w5qsfYowZDiAiFwCXAacbY4pFpEPAsSOMMaNF5ELgIWDyCcmRUo2kQUGpaiUBF/ixwBsiMgRrmoA/ichZWFM1d6N6WuJAk4FXjTHFAMaYwwGf+Sfsy8J6BoZS30kaFJSqhTFmkV0qSMWaUykVGGmMqbBn54xp5CHL7J9V6P+d+g7TNgWlaiEiAwEv1hTL7bDm768QkYlAL3uzQiAxYLfZwE0iEmcfI7D6SKlWQe9YlKrmb1MAq8poijGmSkTeAj4VkTXAMmAjgDHmkIh8JSJrgc+MMXeLyHBgmYiUAzOBB054LpQ6DjpLqlJKKYdWHymllHJoUFBKKeXQoKCUUsqhQUEppZRDg4JSSimHBgWllFIODQpKKaUc/x//c19dguNqSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plotModel(vals,title,xtitle,ytitle,grid = False):\n",
    "    batch = np.arange(len(vals)) + 1\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xtitle)\n",
    "    plt.ylabel(ytitle)\n",
    "    plt.grid(grid)\n",
    "    plt.plot(batch,vals)\n",
    "    plt.show()\n",
    "    \n",
    "def getL1Losses(Lines, mod = \"train\"):\n",
    "    l1_values = []\n",
    "    for line in Lines:\n",
    "        if \"(L1)\" in line:\n",
    "            strr = re.sub(r\"\\s+\", \"\", line)\n",
    "            if mod == \"Val\":\n",
    "                if mod in strr:\n",
    "                    start = strr.find('(L1)')\n",
    "                    end = strr.rfind(\"(\")\n",
    "                    l1 = strr[start+4:end]\n",
    "                    l1_values.append(float(l1))\n",
    "            else:\n",
    "                if \"Val\" not in strr:\n",
    "                    start = strr.find('(L1)')\n",
    "                    end = strr.rfind(\"(\")\n",
    "                    l1 = strr[start+4:end]\n",
    "                    l1_values.append(float(l1))\n",
    "                \n",
    "    return l1_values\n",
    "\n",
    "def getLinesFromTextFile(path):\n",
    "    all_path = '/home/candoga01/' + path\n",
    "    file1 = open(all_path, 'r')\n",
    "    return file1.readlines()\n",
    "\n",
    "\n",
    "path = 'blg_561e_project/agedb-dir/checkpoints/agedb_resnet50_agedb_resnet50_agedb_resnet50_For Final Project_adam_l1_0.001_256_adam_l1_0.001_256_adam_l1_0.001_256/training.log'\n",
    "\n",
    "\n",
    "# Data for Val\n",
    "lines = getLinesFromTextFile(path)\n",
    "l1_values_val = getL1Losses(lines,mod=\"Val\")\n",
    "\n",
    "# Data for Train\n",
    "lines = getLinesFromTextFile(path)\n",
    "l1_values_train = getL1Losses(lines)\n",
    "\n",
    "plotModel(l1_values_val,\"Validation Loss\",\"Batch\",\"L1 Loss\",True)\n",
    "plotModel(l1_values_train,\"Train Loss\",\"Batch\",\"L1 Loss\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
