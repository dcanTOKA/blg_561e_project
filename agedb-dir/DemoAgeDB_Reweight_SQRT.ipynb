{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gmean\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboard_logger import Logger\n",
    "\n",
    "from resnet import resnet50\n",
    "from loss import *\n",
    "from datasets import AgeDB\n",
    "from utils import *\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "imbalanced_related = {\"dataset\": \"agedb\", \"start_epoch\": 0, \"epoch\": 10, \"best_loss\": 1e5, \"lr\": 1e-3,\n",
    "                      \"data_dir\": \"./data\", \"schedule\": [60, 80], \"loss\": \"l1\", \"print_freq\" : 1, \"evaluate\": \"\",\n",
    "                      \"fds\": False, \"bucket_num\": 100, \"bucket_start\": 3, \"start_update\": 0, \"resume\": \"\",\n",
    "                      \"start_smooth\": 1, \"fds_kernel\": \"gaussian\", \"fds_ks\": 9, \"fds_sigma\": 1, \"pretrained\": \"\",\n",
    "                      \"fds_mmt\": 0.9, \"retrain_fc\": False, \"img_size\": 224, \"reweight\": \"sqrt_inv\", \"optimizer\": \"adam\",\n",
    "                      \"lds\": False, \"lds_kernel\": \"gaussian\", \"lds_ks\": 9, \"lds_sigma\": 1, \"model\": \"resnet50\",\n",
    "                      \"batch_size\": 256, \"workers\": 8, \"store_root\": \"./checkpoints\", \"store_name\": \"Model_with_sqrt_inv\"\n",
    "                     }\n",
    "args = SimpleNamespace(**imbalanced_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 05:02:05,375 | Args: namespace(batch_size=256, best_loss=100000.0, bucket_num=100, bucket_start=3, data_dir='./data', dataset='agedb', epoch=10, evaluate='', fds=False, fds_kernel='gaussian', fds_ks=9, fds_mmt=0.9, fds_sigma=1, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, loss='l1', lr=0.001, model='resnet50', optimizer='adam', pretrained='', print_freq=1, resume='', retrain_fc=False, reweight='sqrt_inv', schedule=[60, 80], start_epoch=0, start_smooth=1, start_update=0, store_name='agedb_resnet50_agedb_resnet50_Model_with_sqrt_inv_sqrt_inv_adam_l1_0.001_256_sqrt_inv_adam_l1_0.001_256', store_root='./checkpoints', workers=8)\n",
      "2022-01-20 05:02:05,376 | Store name: agedb_resnet50_agedb_resnet50_Model_with_sqrt_inv_sqrt_inv_adam_l1_0.001_256_sqrt_inv_adam_l1_0.001_256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Creating folder: ./checkpoints/agedb_resnet50_agedb_resnet50_Model_with_sqrt_inv_sqrt_inv_adam_l1_0.001_256_sqrt_inv_adam_l1_0.001_256\n"
     ]
    }
   ],
   "source": [
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.fds:\n",
    "    args.store_name += f'_fds_{args.fds_kernel[:3]}_{args.fds_ks}'\n",
    "    if args.fds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.fds_sigma}'\n",
    "    args.store_name += f'_{args.start_update}_{args.start_smooth}_{args.fds_mmt}'\n",
    "if args.retrain_fc:\n",
    "    args.store_name += f'_retrain_fc'\n",
    "args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "logging.root.handlers = []\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(args.store_root, args.store_name, 'training.log')),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "print = logging.info\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "tb_logger = Logger(logdir=os.path.join(args.store_root, args.store_name), flush_secs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 05:02:12,908 | ===> Resume is disabled\n"
     ]
    }
   ],
   "source": [
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "        checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "            torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        args.best_loss = checkpoint['best_loss']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "    else:\n",
    "        print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "        \n",
    "else:\n",
    "    print(\"===> Resume is disabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 05:02:14,234 | =====> Preparing data...\n",
      "2022-01-20 05:02:14,235 | File (.csv): agedb.csv\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('=====> Preparing data...')\n",
    "print(f\"File (.csv): {args.dataset}.csv\")\n",
    "df = pd.read_csv(os.path.join(args.data_dir, f\"{args.dataset}.csv\"))\n",
    "df_train, df_val, df_test = df[df['split'] == 'train'], df[df['split'] == 'val'], df[df['split'] == 'test']\n",
    "train_labels = df_train['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 05:02:15,027 |    age                                path  split\n",
      "0   31   AgeDB/11706_OliviaHussey_31_f.jpg  train\n",
      "1   59   AgeDB/11684_MireilleDarc_59_f.jpg    val\n",
      "2   44   AgeDB/7955_GilbertRoland_44_m.jpg  train\n",
      "3   61  AgeDB/9352_GeorgesMarchal_61_m.jpg    val\n",
      "4   28     AgeDB/3888_TomasMilian_28_m.jpg    val\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 05:02:15,803 | Using re-weighting: [SQRT_INV]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AgeDB(data_dir=args.data_dir, df=df_train, img_size=args.img_size, split='train',reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "val_dataset = AgeDB(data_dir=args.data_dir, df=df_val, img_size=args.img_size, split='val')\n",
    "test_dataset = AgeDB(data_dir=args.data_dir, df=df_test, img_size=args.img_size, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                          num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                        num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                         num_workers=args.workers, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 05:02:17,993 | Training data size: 12208\n",
      "2022-01-20 05:02:17,994 | Validation data size: 2140\n",
      "2022-01-20 05:02:17,995 | Test data size: 2140\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data size: {len(train_dataset)}\")\n",
    "print(f\"Validation data size: {len(val_dataset)}\")\n",
    "print(f\"Test data size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist_of_age(bins,ages):\n",
    "    plt.style.use('ggplot')\n",
    "    fig=plt.figure(figsize=(16,6))\n",
    "\n",
    "    plt.hist(ages, bins = bins, edgecolor = 'black')\n",
    "\n",
    "    plt.xlabel(\"Age\")\n",
    "    plt.ylabel(\"Age Disribution\")\n",
    "    plt.title(\"Age Distribution Histogram\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train Data for Age\n",
    "get_hist_of_age(bins=100, ages = train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 05:02:24,257 | =====> Building model...\n",
      "2022-01-20 05:02:25,002 | DataParallel(\n",
      "  (module): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "    (linear): Linear(in_features=2048, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('=====> Building model...')\n",
    "model = resnet50(fds=args.fds, bucket_num=args.bucket_num, bucket_start=args.bucket_start,\n",
    "                 start_update=args.start_update, start_smooth=args.start_smooth,\n",
    "                 kernel=args.fds_kernel, ks=args.fds_ks, sigma=args.fds_sigma, momentum=args.fds_mmt)\n",
    "model = torch.nn.DataParallel(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    print(\"Load train loader\")\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        print(\"===> Batch : \" + str(idx+1))\n",
    "        #inputs, targets, weights = \\\n",
    "        #    inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        if args.fds:\n",
    "            print(\"FDS enable\")\n",
    "            outputs, _ = model(inputs, targets, epoch)\n",
    "        else:\n",
    "            print(\"FDS disable\")\n",
    "            outputs = model(inputs, targets, epoch)\n",
    "\n",
    "        print(\"Calculate Loss\")\n",
    "        loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        print(\"Update Loss\")\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Backward\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx+1)\n",
    "\n",
    "    if args.fds and epoch >= args.start_update:\n",
    "        print(f\"Create Epoch [{epoch}] features of all training data...\")\n",
    "        encodings, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets, _) in tqdm(train_loader):\n",
    "                #inputs = inputs.cuda(non_blocking=True)\n",
    "                outputs, feature = model(inputs, targets, epoch)\n",
    "                encodings.extend(feature.data.squeeze().cpu().numpy())\n",
    "                labels.extend(targets.data.squeeze().cpu().numpy())\n",
    "\n",
    "        encodings, labels = torch.from_numpy(np.vstack(encodings)), torch.from_numpy(np.hstack(labels))\n",
    "        model.module.FDS.update_last_epoch_stats(epoch)\n",
    "        model.module.FDS.update_running_stats(encodings, labels, epoch)\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            #inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=100, low_shot_thr=20):\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "    return shot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 05:02:38,088 | Training...\n",
      "2022-01-20 05:02:38,090 | Load train loader\n",
      "2022-01-20 05:02:39,463 | ===> Batch : 1\n",
      "2022-01-20 05:02:39,465 | FDS disable\n",
      "2022-01-20 05:02:52,200 | Calculate Loss\n",
      "2022-01-20 05:02:52,202 | Update Loss\n",
      "2022-01-20 05:02:52,203 | Backward\n",
      "2022-01-20 05:03:14,178 | Epoch: [0][ 1/48]\tTime  36.09 ( 36.09)\tData 1.3734 (1.3734)\tLoss (L1) 47.981 (47.981)\n",
      "2022-01-20 05:03:14,197 | ===> Batch : 2\n",
      "2022-01-20 05:03:14,197 | FDS disable\n",
      "2022-01-20 05:03:26,213 | Calculate Loss\n",
      "2022-01-20 05:03:26,215 | Update Loss\n",
      "2022-01-20 05:03:26,218 | Backward\n",
      "2022-01-20 05:03:47,870 | Epoch: [0][ 2/48]\tTime  33.69 ( 34.89)\tData 0.0192 (0.6963)\tLoss (L1) 49.535 (48.758)\n",
      "2022-01-20 05:03:47,884 | ===> Batch : 3\n",
      "2022-01-20 05:03:47,885 | FDS disable\n",
      "2022-01-20 05:03:59,929 | Calculate Loss\n",
      "2022-01-20 05:03:59,930 | Update Loss\n",
      "2022-01-20 05:03:59,933 | Backward\n",
      "2022-01-20 05:04:21,512 | Epoch: [0][ 3/48]\tTime  33.64 ( 34.47)\tData 0.0142 (0.4690)\tLoss (L1) 37.668 (45.061)\n",
      "2022-01-20 05:04:21,524 | ===> Batch : 4\n",
      "2022-01-20 05:04:21,525 | FDS disable\n",
      "2022-01-20 05:04:33,471 | Calculate Loss\n",
      "2022-01-20 05:04:33,472 | Update Loss\n",
      "2022-01-20 05:04:33,476 | Backward\n",
      "2022-01-20 05:04:55,274 | Epoch: [0][ 4/48]\tTime  33.76 ( 34.30)\tData 0.0126 (0.3549)\tLoss (L1) 38.437 (43.405)\n",
      "2022-01-20 05:04:55,287 | ===> Batch : 5\n",
      "2022-01-20 05:04:55,287 | FDS disable\n",
      "2022-01-20 05:05:07,251 | Calculate Loss\n",
      "2022-01-20 05:05:07,252 | Update Loss\n",
      "2022-01-20 05:05:07,256 | Backward\n",
      "2022-01-20 05:05:28,345 | Epoch: [0][ 5/48]\tTime  33.07 ( 34.05)\tData 0.0128 (0.2865)\tLoss (L1) 29.663 (40.657)\n",
      "2022-01-20 05:05:28,358 | ===> Batch : 6\n",
      "2022-01-20 05:05:28,358 | FDS disable\n",
      "2022-01-20 05:05:40,313 | Calculate Loss\n",
      "2022-01-20 05:05:40,315 | Update Loss\n",
      "2022-01-20 05:05:40,319 | Backward\n",
      "2022-01-20 05:06:01,548 | Epoch: [0][ 6/48]\tTime  33.20 ( 33.91)\tData 0.0129 (0.2409)\tLoss (L1) 30.921 (39.034)\n",
      "2022-01-20 05:06:01,560 | ===> Batch : 7\n",
      "2022-01-20 05:06:01,561 | FDS disable\n",
      "2022-01-20 05:06:13,534 | Calculate Loss\n",
      "2022-01-20 05:06:13,535 | Update Loss\n",
      "2022-01-20 05:06:13,539 | Backward\n",
      "2022-01-20 05:06:35,349 | Epoch: [0][ 7/48]\tTime  33.80 ( 33.89)\tData 0.0126 (0.2083)\tLoss (L1) 21.472 (36.525)\n",
      "2022-01-20 05:06:35,362 | ===> Batch : 8\n",
      "2022-01-20 05:06:35,363 | FDS disable\n",
      "2022-01-20 05:06:47,321 | Calculate Loss\n",
      "2022-01-20 05:06:47,322 | Update Loss\n",
      "2022-01-20 05:06:47,326 | Backward\n",
      "2022-01-20 05:07:08,131 | Epoch: [0][ 8/48]\tTime  32.78 ( 33.76)\tData 0.0128 (0.1838)\tLoss (L1) 23.772 (34.931)\n",
      "2022-01-20 05:07:08,145 | ===> Batch : 9\n",
      "2022-01-20 05:07:08,145 | FDS disable\n",
      "2022-01-20 05:07:20,129 | Calculate Loss\n",
      "2022-01-20 05:07:20,131 | Update Loss\n",
      "2022-01-20 05:07:20,134 | Backward\n",
      "2022-01-20 05:07:41,778 | Epoch: [0][ 9/48]\tTime  33.65 ( 33.74)\tData 0.0144 (0.1650)\tLoss (L1) 17.181 (32.959)\n",
      "2022-01-20 05:07:41,799 | ===> Batch : 10\n",
      "2022-01-20 05:07:41,799 | FDS disable\n",
      "2022-01-20 05:07:53,857 | Calculate Loss\n",
      "2022-01-20 05:07:53,858 | Update Loss\n",
      "2022-01-20 05:07:53,862 | Backward\n",
      "2022-01-20 05:08:15,650 | Epoch: [0][10/48]\tTime  33.87 ( 33.76)\tData 0.0205 (0.1506)\tLoss (L1) 17.667 (31.430)\n",
      "2022-01-20 05:08:15,663 | ===> Batch : 11\n",
      "2022-01-20 05:08:15,663 | FDS disable\n",
      "2022-01-20 05:08:27,574 | Calculate Loss\n",
      "2022-01-20 05:08:27,577 | Update Loss\n",
      "2022-01-20 05:08:27,580 | Backward\n",
      "2022-01-20 05:08:48,863 | Epoch: [0][11/48]\tTime  33.21 ( 33.71)\tData 0.0129 (0.1380)\tLoss (L1) 15.862 (30.015)\n",
      "2022-01-20 05:08:48,876 | ===> Batch : 12\n",
      "2022-01-20 05:08:48,876 | FDS disable\n",
      "2022-01-20 05:09:00,840 | Calculate Loss\n",
      "2022-01-20 05:09:00,842 | Update Loss\n",
      "2022-01-20 05:09:00,845 | Backward\n",
      "2022-01-20 05:09:21,965 | Epoch: [0][12/48]\tTime  33.10 ( 33.66)\tData 0.0129 (0.1276)\tLoss (L1) 13.654 (28.651)\n",
      "2022-01-20 05:09:21,978 | ===> Batch : 13\n",
      "2022-01-20 05:09:21,979 | FDS disable\n",
      "2022-01-20 05:09:33,822 | Calculate Loss\n",
      "2022-01-20 05:09:33,824 | Update Loss\n",
      "2022-01-20 05:09:33,827 | Backward\n",
      "2022-01-20 05:09:55,264 | Epoch: [0][13/48]\tTime  33.30 ( 33.63)\tData 0.0125 (0.1188)\tLoss (L1) 21.584 (28.107)\n",
      "2022-01-20 05:09:55,276 | ===> Batch : 14\n",
      "2022-01-20 05:09:55,277 | FDS disable\n",
      "2022-01-20 05:10:07,161 | Calculate Loss\n",
      "2022-01-20 05:10:07,162 | Update Loss\n",
      "2022-01-20 05:10:07,166 | Backward\n",
      "2022-01-20 05:10:28,653 | Epoch: [0][14/48]\tTime  33.39 ( 33.61)\tData 0.0125 (0.1112)\tLoss (L1) 14.660 (27.147)\n",
      "2022-01-20 05:10:28,666 | ===> Batch : 15\n",
      "2022-01-20 05:10:28,666 | FDS disable\n",
      "2022-01-20 05:10:40,528 | Calculate Loss\n",
      "2022-01-20 05:10:40,529 | Update Loss\n",
      "2022-01-20 05:10:40,533 | Backward\n",
      "2022-01-20 05:11:01,393 | Epoch: [0][15/48]\tTime  32.74 ( 33.55)\tData 0.0127 (0.1046)\tLoss (L1) 12.876 (26.196)\n",
      "2022-01-20 05:11:01,411 | ===> Batch : 16\n",
      "2022-01-20 05:11:01,411 | FDS disable\n",
      "2022-01-20 05:11:13,316 | Calculate Loss\n",
      "2022-01-20 05:11:13,318 | Update Loss\n",
      "2022-01-20 05:11:13,322 | Backward\n",
      "2022-01-20 05:11:34,640 | Epoch: [0][16/48]\tTime  33.25 ( 33.53)\tData 0.0182 (0.0992)\tLoss (L1) 20.772 (25.857)\n",
      "2022-01-20 05:11:34,654 | ===> Batch : 17\n",
      "2022-01-20 05:11:34,655 | FDS disable\n",
      "2022-01-20 05:11:46,509 | Calculate Loss\n",
      "2022-01-20 05:11:46,510 | Update Loss\n",
      "2022-01-20 05:11:46,514 | Backward\n",
      "2022-01-20 05:12:07,549 | Epoch: [0][17/48]\tTime  32.91 ( 33.50)\tData 0.0139 (0.0942)\tLoss (L1) 14.200 (25.171)\n",
      "2022-01-20 05:12:07,563 | ===> Batch : 18\n",
      "2022-01-20 05:12:07,564 | FDS disable\n",
      "2022-01-20 05:12:19,417 | Calculate Loss\n",
      "2022-01-20 05:12:19,419 | Update Loss\n",
      "2022-01-20 05:12:19,423 | Backward\n",
      "2022-01-20 05:12:40,277 | Epoch: [0][18/48]\tTime  32.73 ( 33.45)\tData 0.0137 (0.0897)\tLoss (L1) 15.435 (24.630)\n",
      "2022-01-20 05:12:40,290 | ===> Batch : 19\n",
      "2022-01-20 05:12:40,291 | FDS disable\n",
      "2022-01-20 05:12:52,154 | Calculate Loss\n",
      "2022-01-20 05:12:52,155 | Update Loss\n",
      "2022-01-20 05:12:52,159 | Backward\n",
      "2022-01-20 05:13:13,069 | Epoch: [0][19/48]\tTime  32.79 ( 33.42)\tData 0.0136 (0.0857)\tLoss (L1) 14.009 (24.071)\n",
      "2022-01-20 05:13:13,082 | ===> Batch : 20\n",
      "2022-01-20 05:13:13,083 | FDS disable\n",
      "2022-01-20 05:13:24,868 | Calculate Loss\n",
      "2022-01-20 05:13:24,869 | Update Loss\n",
      "2022-01-20 05:13:24,873 | Backward\n",
      "2022-01-20 05:13:45,689 | Epoch: [0][20/48]\tTime  32.62 ( 33.38)\tData 0.0136 (0.0821)\tLoss (L1) 12.784 (23.507)\n",
      "2022-01-20 05:13:45,704 | ===> Batch : 21\n",
      "2022-01-20 05:13:45,705 | FDS disable\n",
      "2022-01-20 05:13:57,552 | Calculate Loss\n",
      "2022-01-20 05:13:57,553 | Update Loss\n",
      "2022-01-20 05:13:57,556 | Backward\n",
      "2022-01-20 05:14:18,624 | Epoch: [0][21/48]\tTime  32.94 ( 33.36)\tData 0.0154 (0.0789)\tLoss (L1) 17.820 (23.236)\n",
      "2022-01-20 05:14:18,643 | ===> Batch : 22\n",
      "2022-01-20 05:14:18,643 | FDS disable\n",
      "2022-01-20 05:14:30,441 | Calculate Loss\n",
      "2022-01-20 05:14:30,442 | Update Loss\n",
      "2022-01-20 05:14:30,445 | Backward\n",
      "2022-01-20 05:14:51,196 | Epoch: [0][22/48]\tTime  32.57 ( 33.32)\tData 0.0186 (0.0762)\tLoss (L1) 16.376 (22.924)\n",
      "2022-01-20 05:14:51,211 | ===> Batch : 23\n",
      "2022-01-20 05:14:51,211 | FDS disable\n",
      "2022-01-20 05:15:03,028 | Calculate Loss\n",
      "2022-01-20 05:15:03,030 | Update Loss\n",
      "2022-01-20 05:15:03,033 | Backward\n",
      "2022-01-20 05:15:24,092 | Epoch: [0][23/48]\tTime  32.90 ( 33.30)\tData 0.0144 (0.0735)\tLoss (L1) 12.217 (22.459)\n",
      "2022-01-20 05:15:24,106 | ===> Batch : 24\n",
      "2022-01-20 05:15:24,107 | FDS disable\n",
      "2022-01-20 05:15:35,989 | Calculate Loss\n",
      "2022-01-20 05:15:35,990 | Update Loss\n",
      "2022-01-20 05:15:35,994 | Backward\n",
      "2022-01-20 05:15:56,983 | Epoch: [0][24/48]\tTime  32.89 ( 33.29)\tData 0.0142 (0.0710)\tLoss (L1) 16.406 (22.206)\n",
      "2022-01-20 05:15:56,997 | ===> Batch : 25\n",
      "2022-01-20 05:15:56,998 | FDS disable\n",
      "2022-01-20 05:16:08,822 | Calculate Loss\n",
      "2022-01-20 05:16:08,823 | Update Loss\n",
      "2022-01-20 05:16:08,826 | Backward\n",
      "2022-01-20 05:16:29,699 | Epoch: [0][25/48]\tTime  32.72 ( 33.26)\tData 0.0138 (0.0687)\tLoss (L1) 14.002 (21.878)\n",
      "2022-01-20 05:16:29,714 | ===> Batch : 26\n",
      "2022-01-20 05:16:29,714 | FDS disable\n",
      "2022-01-20 05:16:41,553 | Calculate Loss\n",
      "2022-01-20 05:16:41,554 | Update Loss\n",
      "2022-01-20 05:16:41,557 | Backward\n",
      "2022-01-20 05:17:02,421 | Epoch: [0][26/48]\tTime  32.72 ( 33.24)\tData 0.0149 (0.0667)\tLoss (L1) 12.686 (21.525)\n",
      "2022-01-20 05:17:02,435 | ===> Batch : 27\n",
      "2022-01-20 05:17:02,436 | FDS disable\n",
      "2022-01-20 05:17:14,289 | Calculate Loss\n",
      "2022-01-20 05:17:14,291 | Update Loss\n",
      "2022-01-20 05:17:14,295 | Backward\n",
      "2022-01-20 05:17:35,385 | Epoch: [0][27/48]\tTime  32.96 ( 33.23)\tData 0.0149 (0.0648)\tLoss (L1) 12.599 (21.194)\n",
      "2022-01-20 05:17:35,399 | ===> Batch : 28\n",
      "2022-01-20 05:17:35,399 | FDS disable\n",
      "2022-01-20 05:17:47,259 | Calculate Loss\n",
      "2022-01-20 05:17:47,261 | Update Loss\n",
      "2022-01-20 05:17:47,265 | Backward\n",
      "2022-01-20 05:18:08,511 | Epoch: [0][28/48]\tTime  33.13 ( 33.23)\tData 0.0140 (0.0629)\tLoss (L1) 13.612 (20.923)\n",
      "2022-01-20 05:18:08,525 | ===> Batch : 29\n",
      "2022-01-20 05:18:08,526 | FDS disable\n",
      "2022-01-20 05:18:20,352 | Calculate Loss\n",
      "2022-01-20 05:18:20,355 | Update Loss\n",
      "2022-01-20 05:18:20,359 | Backward\n",
      "2022-01-20 05:18:41,228 | Epoch: [0][29/48]\tTime  32.72 ( 33.21)\tData 0.0140 (0.0613)\tLoss (L1) 11.468 (20.597)\n",
      "2022-01-20 05:18:41,243 | ===> Batch : 30\n",
      "2022-01-20 05:18:41,243 | FDS disable\n",
      "2022-01-20 05:18:53,081 | Calculate Loss\n",
      "2022-01-20 05:18:53,082 | Update Loss\n",
      "2022-01-20 05:18:53,086 | Backward\n",
      "2022-01-20 05:19:14,124 | Epoch: [0][30/48]\tTime  32.90 ( 33.20)\tData 0.0145 (0.0597)\tLoss (L1) 15.371 (20.423)\n",
      "2022-01-20 05:19:14,138 | ===> Batch : 31\n",
      "2022-01-20 05:19:14,139 | FDS disable\n",
      "2022-01-20 05:19:25,954 | Calculate Loss\n",
      "2022-01-20 05:19:25,955 | Update Loss\n",
      "2022-01-20 05:19:25,959 | Backward\n",
      "2022-01-20 05:19:47,223 | Epoch: [0][31/48]\tTime  33.10 ( 33.20)\tData 0.0143 (0.0582)\tLoss (L1) 12.667 (20.173)\n",
      "2022-01-20 05:19:47,238 | ===> Batch : 32\n",
      "2022-01-20 05:19:47,238 | FDS disable\n",
      "2022-01-20 05:19:58,992 | Calculate Loss\n",
      "2022-01-20 05:19:58,994 | Update Loss\n",
      "2022-01-20 05:19:58,997 | Backward\n",
      "2022-01-20 05:20:19,769 | Epoch: [0][32/48]\tTime  32.55 ( 33.18)\tData 0.0145 (0.0569)\tLoss (L1) 13.493 (19.964)\n",
      "2022-01-20 05:20:19,783 | ===> Batch : 33\n",
      "2022-01-20 05:20:19,784 | FDS disable\n",
      "2022-01-20 05:20:31,381 | Calculate Loss\n",
      "2022-01-20 05:20:31,382 | Update Loss\n",
      "2022-01-20 05:20:31,385 | Backward\n",
      "2022-01-20 05:20:52,163 | Epoch: [0][33/48]\tTime  32.39 ( 33.15)\tData 0.0139 (0.0556)\tLoss (L1) 14.731 (19.805)\n",
      "2022-01-20 05:20:52,177 | ===> Batch : 34\n",
      "2022-01-20 05:20:52,177 | FDS disable\n",
      "2022-01-20 05:21:03,720 | Calculate Loss\n",
      "2022-01-20 05:21:03,723 | Update Loss\n",
      "2022-01-20 05:21:03,727 | Backward\n",
      "2022-01-20 05:21:24,984 | Epoch: [0][34/48]\tTime  32.82 ( 33.14)\tData 0.0136 (0.0543)\tLoss (L1) 15.976 (19.693)\n",
      "2022-01-20 05:21:24,999 | ===> Batch : 35\n",
      "2022-01-20 05:21:24,999 | FDS disable\n",
      "2022-01-20 05:21:36,583 | Calculate Loss\n",
      "2022-01-20 05:21:36,585 | Update Loss\n",
      "2022-01-20 05:21:36,589 | Backward\n",
      "2022-01-20 05:21:57,680 | Epoch: [0][35/48]\tTime  32.70 ( 33.13)\tData 0.0142 (0.0532)\tLoss (L1) 14.897 (19.556)\n",
      "2022-01-20 05:21:57,694 | ===> Batch : 36\n",
      "2022-01-20 05:21:57,695 | FDS disable\n",
      "2022-01-20 05:22:09,255 | Calculate Loss\n",
      "2022-01-20 05:22:09,257 | Update Loss\n",
      "2022-01-20 05:22:09,260 | Backward\n",
      "2022-01-20 05:22:29,667 | Epoch: [0][36/48]\tTime  31.99 ( 33.10)\tData 0.0146 (0.0521)\tLoss (L1) 12.498 (19.360)\n",
      "2022-01-20 05:22:29,681 | ===> Batch : 37\n",
      "2022-01-20 05:22:29,681 | FDS disable\n",
      "2022-01-20 05:22:41,353 | Calculate Loss\n",
      "2022-01-20 05:22:41,354 | Update Loss\n",
      "2022-01-20 05:22:41,358 | Backward\n",
      "2022-01-20 05:23:02,417 | Epoch: [0][37/48]\tTime  32.75 ( 33.09)\tData 0.0139 (0.0511)\tLoss (L1) 12.005 (19.161)\n",
      "2022-01-20 05:23:02,431 | ===> Batch : 38\n",
      "2022-01-20 05:23:02,432 | FDS disable\n",
      "2022-01-20 05:23:13,887 | Calculate Loss\n",
      "2022-01-20 05:23:13,888 | Update Loss\n",
      "2022-01-20 05:23:13,892 | Backward\n",
      "2022-01-20 05:23:34,495 | Epoch: [0][38/48]\tTime  32.08 ( 33.06)\tData 0.0142 (0.0501)\tLoss (L1) 15.021 (19.052)\n",
      "2022-01-20 05:23:34,508 | ===> Batch : 39\n",
      "2022-01-20 05:23:34,509 | FDS disable\n",
      "2022-01-20 05:23:46,080 | Calculate Loss\n",
      "2022-01-20 05:23:46,081 | Update Loss\n",
      "2022-01-20 05:23:46,085 | Backward\n",
      "2022-01-20 05:24:07,065 | Epoch: [0][39/48]\tTime  32.57 ( 33.05)\tData 0.0138 (0.0492)\tLoss (L1) 14.360 (18.932)\n",
      "2022-01-20 05:24:07,079 | ===> Batch : 40\n",
      "2022-01-20 05:24:07,080 | FDS disable\n",
      "2022-01-20 05:24:18,656 | Calculate Loss\n",
      "2022-01-20 05:24:18,657 | Update Loss\n",
      "2022-01-20 05:24:18,660 | Backward\n",
      "2022-01-20 05:24:38,699 | Epoch: [0][40/48]\tTime  31.63 ( 33.02)\tData 0.0143 (0.0483)\tLoss (L1) 10.825 (18.729)\n",
      "2022-01-20 05:24:38,713 | ===> Batch : 41\n",
      "2022-01-20 05:24:38,713 | FDS disable\n",
      "2022-01-20 05:24:50,202 | Calculate Loss\n",
      "2022-01-20 05:24:50,203 | Update Loss\n",
      "2022-01-20 05:24:50,207 | Backward\n",
      "2022-01-20 05:25:10,839 | Epoch: [0][41/48]\tTime  32.14 ( 32.99)\tData 0.0139 (0.0475)\tLoss (L1) 14.507 (18.626)\n",
      "2022-01-20 05:25:10,853 | ===> Batch : 42\n",
      "2022-01-20 05:25:10,853 | FDS disable\n",
      "2022-01-20 05:25:22,418 | Calculate Loss\n",
      "2022-01-20 05:25:22,420 | Update Loss\n",
      "2022-01-20 05:25:22,423 | Backward\n",
      "2022-01-20 05:25:43,299 | Epoch: [0][42/48]\tTime  32.46 ( 32.98)\tData 0.0137 (0.0467)\tLoss (L1) 14.373 (18.525)\n",
      "2022-01-20 05:25:43,313 | ===> Batch : 43\n",
      "2022-01-20 05:25:43,313 | FDS disable\n",
      "2022-01-20 05:25:54,871 | Calculate Loss\n",
      "2022-01-20 05:25:54,872 | Update Loss\n",
      "2022-01-20 05:25:54,876 | Backward\n",
      "2022-01-20 05:26:15,606 | Epoch: [0][43/48]\tTime  32.31 ( 32.97)\tData 0.0134 (0.0459)\tLoss (L1) 11.254 (18.356)\n",
      "2022-01-20 05:26:15,620 | ===> Batch : 44\n",
      "2022-01-20 05:26:15,620 | FDS disable\n",
      "2022-01-20 05:26:27,202 | Calculate Loss\n",
      "2022-01-20 05:26:27,203 | Update Loss\n",
      "2022-01-20 05:26:27,207 | Backward\n",
      "2022-01-20 05:26:48,378 | Epoch: [0][44/48]\tTime  32.77 ( 32.96)\tData 0.0138 (0.0452)\tLoss (L1) 12.303 (18.218)\n",
      "2022-01-20 05:26:48,392 | ===> Batch : 45\n",
      "2022-01-20 05:26:48,392 | FDS disable\n",
      "2022-01-20 05:27:00,024 | Calculate Loss\n",
      "2022-01-20 05:27:00,025 | Update Loss\n",
      "2022-01-20 05:27:00,028 | Backward\n",
      "2022-01-20 05:27:21,344 | Epoch: [0][45/48]\tTime  32.97 ( 32.96)\tData 0.0138 (0.0445)\tLoss (L1) 13.710 (18.118)\n",
      "2022-01-20 05:27:21,358 | ===> Batch : 46\n",
      "2022-01-20 05:27:21,358 | FDS disable\n",
      "2022-01-20 05:27:32,966 | Calculate Loss\n",
      "2022-01-20 05:27:32,968 | Update Loss\n",
      "2022-01-20 05:27:32,971 | Backward\n",
      "2022-01-20 05:27:53,543 | Epoch: [0][46/48]\tTime  32.20 ( 32.94)\tData 0.0137 (0.0438)\tLoss (L1) 15.352 (18.058)\n",
      "2022-01-20 05:27:53,558 | ===> Batch : 47\n",
      "2022-01-20 05:27:53,559 | FDS disable\n",
      "2022-01-20 05:28:05,101 | Calculate Loss\n",
      "2022-01-20 05:28:05,102 | Update Loss\n",
      "2022-01-20 05:28:05,106 | Backward\n",
      "2022-01-20 05:28:25,958 | Epoch: [0][47/48]\tTime  32.41 ( 32.93)\tData 0.0150 (0.0432)\tLoss (L1) 15.853 (18.011)\n",
      "2022-01-20 05:28:25,972 | ===> Batch : 48\n",
      "2022-01-20 05:28:25,972 | FDS disable\n",
      "2022-01-20 05:28:33,739 | Calculate Loss\n",
      "2022-01-20 05:28:33,741 | Update Loss\n",
      "2022-01-20 05:28:33,744 | Backward\n",
      "2022-01-20 05:28:48,563 | Epoch: [0][48/48]\tTime  22.60 ( 32.72)\tData 0.0137 (0.0426)\tLoss (L1) 11.688 (17.920)\n",
      "2022-01-20 05:29:00,273 | Val: [0/9]\tTime 11.664 (11.664)\tLoss (MSE) 388.444 (388.444)\tLoss (L1) 15.507 (15.507)\n",
      "2022-01-20 05:29:08,863 | Val: [1/9]\tTime  8.591 (10.127)\tLoss (MSE) 361.836 (375.140)\tLoss (L1) 15.537 (15.522)\n",
      "2022-01-20 05:29:17,534 | Val: [2/9]\tTime  8.670 ( 9.642)\tLoss (MSE) 428.020 (392.767)\tLoss (L1) 16.638 (15.894)\n",
      "2022-01-20 05:29:26,144 | Val: [3/9]\tTime  8.610 ( 9.384)\tLoss (MSE) 399.862 (394.540)\tLoss (L1) 15.900 (15.895)\n",
      "2022-01-20 05:29:34,792 | Val: [4/9]\tTime  8.648 ( 9.237)\tLoss (MSE) 409.969 (397.626)\tLoss (L1) 16.374 (15.991)\n",
      "2022-01-20 05:29:43,465 | Val: [5/9]\tTime  8.674 ( 9.143)\tLoss (MSE) 378.276 (394.401)\tLoss (L1) 15.921 (15.980)\n",
      "2022-01-20 05:29:52,167 | Val: [6/9]\tTime  8.701 ( 9.080)\tLoss (MSE) 378.386 (392.113)\tLoss (L1) 15.021 (15.843)\n",
      "2022-01-20 05:30:00,840 | Val: [7/9]\tTime  8.674 ( 9.029)\tLoss (MSE) 437.692 (397.811)\tLoss (L1) 16.648 (15.943)\n",
      "2022-01-20 05:30:04,298 | Val: [8/9]\tTime  3.457 ( 8.410)\tLoss (MSE) 403.776 (398.067)\tLoss (L1) 15.808 (15.937)\n",
      "2022-01-20 05:30:04,439 |  * Overall: MSE 398.067\tL1 15.937\tG-Mean 10.702\n",
      "2022-01-20 05:30:04,440 |  * Many: MSE 330.773\tL1 14.411\tG-Mean 9.721\n",
      "2022-01-20 05:30:04,441 |  * Median: MSE 445.302\tL1 17.275\tG-Mean 11.529\n",
      "2022-01-20 05:30:04,441 |  * Low: MSE 902.997\tL1 26.658\tG-Mean 21.612\n",
      "2022-01-20 05:30:04,446 | Best L1 Loss: 15.937\n",
      "2022-01-20 05:30:04,665 | ===> Saving current best checkpoint...\n",
      "2022-01-20 05:30:04,775 | Epoch #0: Train loss [17.9198]; Val loss: MSE [398.0671], L1 [15.9375], G-Mean [10.7022]\n",
      "2022-01-20 05:30:04,776 | Training...\n",
      "2022-01-20 05:30:04,777 | Load train loader\n",
      "2022-01-20 05:30:05,880 | ===> Batch : 1\n",
      "2022-01-20 05:30:05,881 | FDS disable\n",
      "2022-01-20 05:30:20,576 | Calculate Loss\n",
      "2022-01-20 05:30:20,577 | Update Loss\n",
      "2022-01-20 05:30:20,706 | Backward\n",
      "2022-01-20 05:30:42,688 | Epoch: [1][ 1/48]\tTime  37.91 ( 37.91)\tData 1.1027 (1.1027)\tLoss (L1) 15.217 (15.217)\n",
      "2022-01-20 05:30:42,703 | ===> Batch : 2\n",
      "2022-01-20 05:30:42,704 | FDS disable\n",
      "2022-01-20 05:30:54,482 | Calculate Loss\n",
      "2022-01-20 05:30:54,483 | Update Loss\n",
      "2022-01-20 05:30:54,487 | Backward\n",
      "2022-01-20 05:31:15,674 | Epoch: [1][ 2/48]\tTime  32.99 ( 35.45)\tData 0.0158 (0.5592)\tLoss (L1) 15.152 (15.185)\n",
      "2022-01-20 05:31:15,687 | ===> Batch : 3\n",
      "2022-01-20 05:31:15,688 | FDS disable\n",
      "2022-01-20 05:31:27,431 | Calculate Loss\n",
      "2022-01-20 05:31:27,432 | Update Loss\n",
      "2022-01-20 05:31:27,436 | Backward\n",
      "2022-01-20 05:31:48,623 | Epoch: [1][ 3/48]\tTime  32.95 ( 34.62)\tData 0.0129 (0.3771)\tLoss (L1) 13.545 (14.638)\n",
      "2022-01-20 05:31:48,639 | ===> Batch : 4\n",
      "2022-01-20 05:31:48,639 | FDS disable\n",
      "2022-01-20 05:32:00,395 | Calculate Loss\n",
      "2022-01-20 05:32:00,396 | Update Loss\n",
      "2022-01-20 05:32:00,400 | Backward\n",
      "2022-01-20 05:32:21,294 | Epoch: [1][ 4/48]\tTime  32.67 ( 34.13)\tData 0.0155 (0.2867)\tLoss (L1) 14.490 (14.601)\n",
      "2022-01-20 05:32:21,315 | ===> Batch : 5\n",
      "2022-01-20 05:32:21,315 | FDS disable\n",
      "2022-01-20 05:32:33,092 | Calculate Loss\n",
      "2022-01-20 05:32:33,094 | Update Loss\n",
      "2022-01-20 05:32:33,097 | Backward\n",
      "2022-01-20 05:32:53,841 | Epoch: [1][ 5/48]\tTime  32.55 ( 33.81)\tData 0.0202 (0.2334)\tLoss (L1) 11.754 (14.032)\n",
      "2022-01-20 05:32:53,857 | ===> Batch : 6\n",
      "2022-01-20 05:32:53,857 | FDS disable\n",
      "2022-01-20 05:33:05,578 | Calculate Loss\n",
      "2022-01-20 05:33:05,579 | Update Loss\n",
      "2022-01-20 05:33:05,583 | Backward\n",
      "2022-01-20 05:33:26,442 | Epoch: [1][ 6/48]\tTime  32.60 ( 33.61)\tData 0.0152 (0.1971)\tLoss (L1) 12.455 (13.769)\n",
      "2022-01-20 05:33:26,456 | ===> Batch : 7\n",
      "2022-01-20 05:33:26,457 | FDS disable\n",
      "2022-01-20 05:33:38,220 | Calculate Loss\n",
      "2022-01-20 05:33:38,222 | Update Loss\n",
      "2022-01-20 05:33:38,226 | Backward\n",
      "2022-01-20 05:33:59,122 | Epoch: [1][ 7/48]\tTime  32.68 ( 33.48)\tData 0.0144 (0.1710)\tLoss (L1) 11.100 (13.388)\n",
      "2022-01-20 05:33:59,135 | ===> Batch : 8\n",
      "2022-01-20 05:33:59,136 | FDS disable\n",
      "2022-01-20 05:34:10,843 | Calculate Loss\n",
      "2022-01-20 05:34:10,845 | Update Loss\n",
      "2022-01-20 05:34:10,848 | Backward\n",
      "2022-01-20 05:34:32,018 | Epoch: [1][ 8/48]\tTime  32.90 ( 33.41)\tData 0.0129 (0.1512)\tLoss (L1) 13.892 (13.451)\n",
      "2022-01-20 05:34:32,033 | ===> Batch : 9\n",
      "2022-01-20 05:34:32,034 | FDS disable\n",
      "2022-01-20 05:34:43,753 | Calculate Loss\n",
      "2022-01-20 05:34:43,754 | Update Loss\n",
      "2022-01-20 05:34:43,758 | Backward\n",
      "2022-01-20 05:35:05,181 | Epoch: [1][ 9/48]\tTime  33.16 ( 33.38)\tData 0.0149 (0.1361)\tLoss (L1) 12.736 (13.371)\n",
      "2022-01-20 05:35:05,202 | ===> Batch : 10\n",
      "2022-01-20 05:35:05,202 | FDS disable\n",
      "2022-01-20 05:35:16,932 | Calculate Loss\n",
      "2022-01-20 05:35:16,933 | Update Loss\n",
      "2022-01-20 05:35:16,937 | Backward\n",
      "2022-01-20 05:35:37,519 | Epoch: [1][10/48]\tTime  32.34 ( 33.27)\tData 0.0207 (0.1245)\tLoss (L1) 13.813 (13.416)\n",
      "2022-01-20 05:35:37,535 | ===> Batch : 11\n",
      "2022-01-20 05:35:37,536 | FDS disable\n",
      "2022-01-20 05:35:49,290 | Calculate Loss\n",
      "2022-01-20 05:35:49,292 | Update Loss\n",
      "2022-01-20 05:35:49,295 | Backward\n",
      "2022-01-20 05:36:10,518 | Epoch: [1][11/48]\tTime  33.00 ( 33.25)\tData 0.0162 (0.1147)\tLoss (L1) 12.808 (13.360)\n",
      "2022-01-20 05:36:10,531 | ===> Batch : 12\n",
      "2022-01-20 05:36:10,531 | FDS disable\n",
      "2022-01-20 05:36:22,262 | Calculate Loss\n",
      "2022-01-20 05:36:22,264 | Update Loss\n",
      "2022-01-20 05:36:22,267 | Backward\n",
      "2022-01-20 05:36:43,187 | Epoch: [1][12/48]\tTime  32.67 ( 33.20)\tData 0.0127 (0.1062)\tLoss (L1) 12.261 (13.269)\n",
      "2022-01-20 05:36:43,201 | ===> Batch : 13\n",
      "2022-01-20 05:36:43,201 | FDS disable\n",
      "2022-01-20 05:36:54,927 | Calculate Loss\n",
      "2022-01-20 05:36:54,929 | Update Loss\n",
      "2022-01-20 05:36:54,932 | Backward\n",
      "2022-01-20 05:37:15,823 | Epoch: [1][13/48]\tTime  32.64 ( 33.16)\tData 0.0137 (0.0991)\tLoss (L1) 13.939 (13.320)\n",
      "2022-01-20 05:37:15,838 | ===> Batch : 14\n",
      "2022-01-20 05:37:15,838 | FDS disable\n",
      "2022-01-20 05:37:27,580 | Calculate Loss\n",
      "2022-01-20 05:37:27,583 | Update Loss\n",
      "2022-01-20 05:37:27,586 | Backward\n",
      "2022-01-20 05:37:48,882 | Epoch: [1][14/48]\tTime  33.06 ( 33.15)\tData 0.0143 (0.0930)\tLoss (L1) 12.755 (13.280)\n",
      "2022-01-20 05:37:48,896 | ===> Batch : 15\n",
      "2022-01-20 05:37:48,897 | FDS disable\n",
      "2022-01-20 05:38:00,621 | Calculate Loss\n",
      "2022-01-20 05:38:00,622 | Update Loss\n",
      "2022-01-20 05:38:00,626 | Backward\n",
      "2022-01-20 05:38:21,656 | Epoch: [1][15/48]\tTime  32.77 ( 33.13)\tData 0.0146 (0.0878)\tLoss (L1) 13.385 (13.287)\n",
      "2022-01-20 05:38:21,670 | ===> Batch : 16\n",
      "2022-01-20 05:38:21,670 | FDS disable\n",
      "2022-01-20 05:38:33,399 | Calculate Loss\n",
      "2022-01-20 05:38:33,401 | Update Loss\n",
      "2022-01-20 05:38:33,404 | Backward\n",
      "2022-01-20 05:38:54,264 | Epoch: [1][16/48]\tTime  32.61 ( 33.09)\tData 0.0138 (0.0832)\tLoss (L1) 15.100 (13.400)\n",
      "2022-01-20 05:38:54,278 | ===> Batch : 17\n",
      "2022-01-20 05:38:54,278 | FDS disable\n",
      "2022-01-20 05:39:06,036 | Calculate Loss\n",
      "2022-01-20 05:39:06,037 | Update Loss\n",
      "2022-01-20 05:39:06,041 | Backward\n",
      "2022-01-20 05:39:27,416 | Epoch: [1][17/48]\tTime  33.15 ( 33.10)\tData 0.0141 (0.0791)\tLoss (L1) 15.289 (13.511)\n",
      "2022-01-20 05:39:27,430 | ===> Batch : 18\n",
      "2022-01-20 05:39:27,430 | FDS disable\n",
      "2022-01-20 05:39:39,180 | Calculate Loss\n",
      "2022-01-20 05:39:39,182 | Update Loss\n",
      "2022-01-20 05:39:39,186 | Backward\n",
      "2022-01-20 05:40:00,142 | Epoch: [1][18/48]\tTime  32.73 ( 33.08)\tData 0.0141 (0.0755)\tLoss (L1) 11.868 (13.420)\n",
      "2022-01-20 05:40:00,156 | ===> Batch : 19\n",
      "2022-01-20 05:40:00,157 | FDS disable\n",
      "2022-01-20 05:40:11,968 | Calculate Loss\n",
      "2022-01-20 05:40:11,970 | Update Loss\n",
      "2022-01-20 05:40:11,974 | Backward\n",
      "2022-01-20 05:40:32,718 | Epoch: [1][19/48]\tTime  32.58 ( 33.05)\tData 0.0141 (0.0723)\tLoss (L1) 14.410 (13.472)\n",
      "2022-01-20 05:40:32,732 | ===> Batch : 20\n",
      "2022-01-20 05:40:32,732 | FDS disable\n",
      "2022-01-20 05:40:44,440 | Calculate Loss\n",
      "2022-01-20 05:40:44,441 | Update Loss\n",
      "2022-01-20 05:40:44,445 | Backward\n",
      "2022-01-20 05:41:05,210 | Epoch: [1][20/48]\tTime  32.49 ( 33.02)\tData 0.0136 (0.0693)\tLoss (L1) 11.956 (13.396)\n",
      "2022-01-20 05:41:05,225 | ===> Batch : 21\n",
      "2022-01-20 05:41:05,226 | FDS disable\n",
      "2022-01-20 05:41:16,984 | Calculate Loss\n",
      "2022-01-20 05:41:16,985 | Update Loss\n",
      "2022-01-20 05:41:16,989 | Backward\n",
      "2022-01-20 05:41:38,263 | Epoch: [1][21/48]\tTime  33.05 ( 33.02)\tData 0.0152 (0.0667)\tLoss (L1) 11.205 (13.292)\n",
      "2022-01-20 05:41:38,278 | ===> Batch : 22\n",
      "2022-01-20 05:41:38,278 | FDS disable\n",
      "2022-01-20 05:41:49,997 | Calculate Loss\n",
      "2022-01-20 05:41:49,999 | Update Loss\n",
      "2022-01-20 05:41:50,001 | Backward\n",
      "2022-01-20 05:42:10,891 | Epoch: [1][22/48]\tTime  32.63 ( 33.01)\tData 0.0150 (0.0644)\tLoss (L1) 10.709 (13.175)\n",
      "2022-01-20 05:42:10,905 | ===> Batch : 23\n",
      "2022-01-20 05:42:10,906 | FDS disable\n",
      "2022-01-20 05:42:22,659 | Calculate Loss\n",
      "2022-01-20 05:42:22,660 | Update Loss\n",
      "2022-01-20 05:42:22,664 | Backward\n",
      "2022-01-20 05:42:43,571 | Epoch: [1][23/48]\tTime  32.68 ( 32.99)\tData 0.0139 (0.0622)\tLoss (L1) 15.225 (13.264)\n",
      "2022-01-20 05:42:43,585 | ===> Batch : 24\n",
      "2022-01-20 05:42:43,586 | FDS disable\n",
      "2022-01-20 05:42:55,309 | Calculate Loss\n",
      "2022-01-20 05:42:55,310 | Update Loss\n",
      "2022-01-20 05:42:55,315 | Backward\n",
      "2022-01-20 05:43:16,535 | Epoch: [1][24/48]\tTime  32.96 ( 32.99)\tData 0.0140 (0.0602)\tLoss (L1) 13.316 (13.266)\n",
      "2022-01-20 05:43:16,549 | ===> Batch : 25\n",
      "2022-01-20 05:43:16,550 | FDS disable\n",
      "2022-01-20 05:43:28,267 | Calculate Loss\n",
      "2022-01-20 05:43:28,269 | Update Loss\n",
      "2022-01-20 05:43:28,272 | Backward\n",
      "2022-01-20 05:43:49,083 | Epoch: [1][25/48]\tTime  32.55 ( 32.97)\tData 0.0139 (0.0583)\tLoss (L1) 12.517 (13.236)\n",
      "2022-01-20 05:43:49,102 | ===> Batch : 26\n",
      "2022-01-20 05:43:49,103 | FDS disable\n",
      "2022-01-20 05:44:00,836 | Calculate Loss\n",
      "2022-01-20 05:44:00,838 | Update Loss\n",
      "2022-01-20 05:44:00,841 | Backward\n",
      "2022-01-20 05:44:22,201 | Epoch: [1][26/48]\tTime  33.12 ( 32.98)\tData 0.0193 (0.0568)\tLoss (L1) 13.809 (13.258)\n",
      "2022-01-20 05:44:22,215 | ===> Batch : 27\n",
      "2022-01-20 05:44:22,215 | FDS disable\n",
      "2022-01-20 05:44:33,990 | Calculate Loss\n",
      "2022-01-20 05:44:33,992 | Update Loss\n",
      "2022-01-20 05:44:33,995 | Backward\n",
      "2022-01-20 05:44:54,723 | Epoch: [1][27/48]\tTime  32.52 ( 32.96)\tData 0.0140 (0.0552)\tLoss (L1) 13.864 (13.280)\n",
      "2022-01-20 05:44:54,738 | ===> Batch : 28\n",
      "2022-01-20 05:44:54,739 | FDS disable\n",
      "2022-01-20 05:45:06,450 | Calculate Loss\n",
      "2022-01-20 05:45:06,452 | Update Loss\n",
      "2022-01-20 05:45:06,456 | Backward\n",
      "2022-01-20 05:45:27,717 | Epoch: [1][28/48]\tTime  32.99 ( 32.96)\tData 0.0146 (0.0538)\tLoss (L1) 13.923 (13.303)\n",
      "2022-01-20 05:45:27,731 | ===> Batch : 29\n",
      "2022-01-20 05:45:27,731 | FDS disable\n",
      "2022-01-20 05:45:39,473 | Calculate Loss\n",
      "2022-01-20 05:45:39,474 | Update Loss\n",
      "2022-01-20 05:45:39,478 | Backward\n",
      "2022-01-20 05:46:00,023 | Epoch: [1][29/48]\tTime  32.31 ( 32.94)\tData 0.0136 (0.0524)\tLoss (L1) 15.276 (13.371)\n",
      "2022-01-20 05:46:00,037 | ===> Batch : 30\n",
      "2022-01-20 05:46:00,038 | FDS disable\n",
      "2022-01-20 05:46:11,861 | Calculate Loss\n",
      "2022-01-20 05:46:11,862 | Update Loss\n",
      "2022-01-20 05:46:11,865 | Backward\n",
      "2022-01-20 05:46:32,957 | Epoch: [1][30/48]\tTime  32.93 ( 32.94)\tData 0.0142 (0.0511)\tLoss (L1) 15.889 (13.455)\n",
      "2022-01-20 05:46:32,971 | ===> Batch : 31\n",
      "2022-01-20 05:46:32,972 | FDS disable\n",
      "2022-01-20 05:46:44,634 | Calculate Loss\n",
      "2022-01-20 05:46:44,635 | Update Loss\n",
      "2022-01-20 05:46:44,639 | Backward\n",
      "2022-01-20 05:47:05,363 | Epoch: [1][31/48]\tTime  32.41 ( 32.92)\tData 0.0137 (0.0499)\tLoss (L1) 13.124 (13.445)\n",
      "2022-01-20 05:47:05,377 | ===> Batch : 32\n",
      "2022-01-20 05:47:05,377 | FDS disable\n",
      "2022-01-20 05:47:17,022 | Calculate Loss\n",
      "2022-01-20 05:47:17,024 | Update Loss\n",
      "2022-01-20 05:47:17,028 | Backward\n",
      "2022-01-20 05:47:37,579 | Epoch: [1][32/48]\tTime  32.22 ( 32.90)\tData 0.0139 (0.0488)\tLoss (L1) 13.344 (13.441)\n",
      "2022-01-20 05:47:37,594 | ===> Batch : 33\n",
      "2022-01-20 05:47:37,595 | FDS disable\n",
      "2022-01-20 05:47:49,165 | Calculate Loss\n",
      "2022-01-20 05:47:49,166 | Update Loss\n",
      "2022-01-20 05:47:49,171 | Backward\n",
      "2022-01-20 05:48:09,937 | Epoch: [1][33/48]\tTime  32.36 ( 32.88)\tData 0.0149 (0.0478)\tLoss (L1) 12.196 (13.404)\n",
      "2022-01-20 05:48:09,952 | ===> Batch : 34\n",
      "2022-01-20 05:48:09,952 | FDS disable\n",
      "2022-01-20 05:48:21,572 | Calculate Loss\n",
      "2022-01-20 05:48:21,573 | Update Loss\n",
      "2022-01-20 05:48:21,576 | Backward\n",
      "2022-01-20 05:48:43,048 | Epoch: [1][34/48]\tTime  33.11 ( 32.89)\tData 0.0148 (0.0468)\tLoss (L1) 11.950 (13.361)\n",
      "2022-01-20 05:48:43,062 | ===> Batch : 35\n",
      "2022-01-20 05:48:43,063 | FDS disable\n",
      "2022-01-20 05:48:54,680 | Calculate Loss\n",
      "2022-01-20 05:48:54,681 | Update Loss\n",
      "2022-01-20 05:48:54,685 | Backward\n",
      "2022-01-20 05:49:15,573 | Epoch: [1][35/48]\tTime  32.52 ( 32.88)\tData 0.0139 (0.0459)\tLoss (L1) 13.083 (13.353)\n",
      "2022-01-20 05:49:15,587 | ===> Batch : 36\n",
      "2022-01-20 05:49:15,588 | FDS disable\n",
      "2022-01-20 05:49:27,197 | Calculate Loss\n",
      "2022-01-20 05:49:27,200 | Update Loss\n",
      "2022-01-20 05:49:27,203 | Backward\n",
      "2022-01-20 05:49:47,938 | Epoch: [1][36/48]\tTime  32.36 ( 32.87)\tData 0.0139 (0.0450)\tLoss (L1) 12.340 (13.325)\n",
      "2022-01-20 05:49:47,951 | ===> Batch : 37\n",
      "2022-01-20 05:49:47,952 | FDS disable\n",
      "2022-01-20 05:49:59,509 | Calculate Loss\n",
      "2022-01-20 05:49:59,510 | Update Loss\n",
      "2022-01-20 05:49:59,514 | Backward\n",
      "2022-01-20 05:50:20,988 | Epoch: [1][37/48]\tTime  33.05 ( 32.87)\tData 0.0135 (0.0441)\tLoss (L1) 14.179 (13.348)\n",
      "2022-01-20 05:50:21,002 | ===> Batch : 38\n",
      "2022-01-20 05:50:21,002 | FDS disable\n",
      "2022-01-20 05:50:32,529 | Calculate Loss\n",
      "2022-01-20 05:50:32,531 | Update Loss\n",
      "2022-01-20 05:50:32,534 | Backward\n",
      "2022-01-20 05:50:53,766 | Epoch: [1][38/48]\tTime  32.78 ( 32.87)\tData 0.0136 (0.0433)\tLoss (L1) 13.158 (13.343)\n",
      "2022-01-20 05:50:53,782 | ===> Batch : 39\n",
      "2022-01-20 05:50:53,782 | FDS disable\n",
      "2022-01-20 05:51:05,370 | Calculate Loss\n",
      "2022-01-20 05:51:05,371 | Update Loss\n",
      "2022-01-20 05:51:05,374 | Backward\n",
      "2022-01-20 05:51:26,726 | Epoch: [1][39/48]\tTime  32.96 ( 32.87)\tData 0.0152 (0.0426)\tLoss (L1) 15.620 (13.401)\n",
      "2022-01-20 05:51:26,739 | ===> Batch : 40\n",
      "2022-01-20 05:51:26,740 | FDS disable\n",
      "2022-01-20 05:51:38,307 | Calculate Loss\n",
      "2022-01-20 05:51:38,309 | Update Loss\n",
      "2022-01-20 05:51:38,313 | Backward\n",
      "2022-01-20 05:51:59,049 | Epoch: [1][40/48]\tTime  32.32 ( 32.86)\tData 0.0137 (0.0419)\tLoss (L1) 14.922 (13.439)\n",
      "2022-01-20 05:51:59,064 | ===> Batch : 41\n",
      "2022-01-20 05:51:59,064 | FDS disable\n",
      "2022-01-20 05:52:10,573 | Calculate Loss\n",
      "2022-01-20 05:52:10,574 | Update Loss\n",
      "2022-01-20 05:52:10,577 | Backward\n",
      "2022-01-20 05:52:31,869 | Epoch: [1][41/48]\tTime  32.82 ( 32.86)\tData 0.0144 (0.0412)\tLoss (L1) 18.367 (13.560)\n",
      "2022-01-20 05:52:31,883 | ===> Batch : 42\n",
      "2022-01-20 05:52:31,883 | FDS disable\n",
      "2022-01-20 05:52:43,504 | Calculate Loss\n",
      "2022-01-20 05:52:43,505 | Update Loss\n",
      "2022-01-20 05:52:43,509 | Backward\n",
      "2022-01-20 05:53:04,307 | Epoch: [1][42/48]\tTime  32.44 ( 32.85)\tData 0.0142 (0.0406)\tLoss (L1) 14.431 (13.580)\n",
      "2022-01-20 05:53:04,321 | ===> Batch : 43\n",
      "2022-01-20 05:53:04,322 | FDS disable\n",
      "2022-01-20 05:53:15,890 | Calculate Loss\n",
      "2022-01-20 05:53:15,891 | Update Loss\n",
      "2022-01-20 05:53:15,895 | Backward\n",
      "2022-01-20 05:53:36,840 | Epoch: [1][43/48]\tTime  32.53 ( 32.84)\tData 0.0142 (0.0400)\tLoss (L1) 12.150 (13.547)\n",
      "2022-01-20 05:53:36,853 | ===> Batch : 44\n",
      "2022-01-20 05:53:36,854 | FDS disable\n",
      "2022-01-20 05:53:48,432 | Calculate Loss\n",
      "2022-01-20 05:53:48,433 | Update Loss\n",
      "2022-01-20 05:53:48,437 | Backward\n",
      "2022-01-20 05:54:08,711 | Epoch: [1][44/48]\tTime  31.87 ( 32.82)\tData 0.0138 (0.0394)\tLoss (L1) 14.716 (13.574)\n",
      "2022-01-20 05:54:08,726 | ===> Batch : 45\n",
      "2022-01-20 05:54:08,726 | FDS disable\n",
      "2022-01-20 05:54:20,286 | Calculate Loss\n",
      "2022-01-20 05:54:20,287 | Update Loss\n",
      "2022-01-20 05:54:20,290 | Backward\n",
      "2022-01-20 05:54:41,473 | Epoch: [1][45/48]\tTime  32.76 ( 32.82)\tData 0.0149 (0.0388)\tLoss (L1) 14.308 (13.590)\n",
      "2022-01-20 05:54:41,487 | ===> Batch : 46\n",
      "2022-01-20 05:54:41,488 | FDS disable\n",
      "2022-01-20 05:54:53,073 | Calculate Loss\n",
      "2022-01-20 05:54:53,075 | Update Loss\n",
      "2022-01-20 05:54:53,078 | Backward\n",
      "2022-01-20 05:55:13,944 | Epoch: [1][46/48]\tTime  32.47 ( 32.81)\tData 0.0138 (0.0383)\tLoss (L1) 14.397 (13.607)\n",
      "2022-01-20 05:55:13,958 | ===> Batch : 47\n",
      "2022-01-20 05:55:13,959 | FDS disable\n",
      "2022-01-20 05:55:25,507 | Calculate Loss\n",
      "2022-01-20 05:55:25,508 | Update Loss\n",
      "2022-01-20 05:55:25,512 | Backward\n",
      "2022-01-20 05:55:46,267 | Epoch: [1][47/48]\tTime  32.32 ( 32.80)\tData 0.0140 (0.0378)\tLoss (L1) 11.293 (13.558)\n",
      "2022-01-20 05:55:46,283 | ===> Batch : 48\n",
      "2022-01-20 05:55:46,283 | FDS disable\n",
      "2022-01-20 05:55:54,460 | Calculate Loss\n",
      "2022-01-20 05:55:54,462 | Update Loss\n",
      "2022-01-20 05:55:54,466 | Backward\n",
      "2022-01-20 05:56:08,830 | Epoch: [1][48/48]\tTime  22.56 ( 32.58)\tData 0.0153 (0.0373)\tLoss (L1) 13.759 (13.561)\n",
      "2022-01-20 05:56:20,680 | Val: [0/9]\tTime 11.683 (11.683)\tLoss (MSE) 305.261 (305.261)\tLoss (L1) 13.794 (13.794)\n",
      "2022-01-20 05:56:29,407 | Val: [1/9]\tTime  8.727 (10.205)\tLoss (MSE) 295.030 (300.146)\tLoss (L1) 13.662 (13.728)\n",
      "2022-01-20 05:56:38,170 | Val: [2/9]\tTime  8.763 ( 9.725)\tLoss (MSE) 372.896 (324.396)\tLoss (L1) 15.623 (14.359)\n",
      "2022-01-20 05:56:47,038 | Val: [3/9]\tTime  8.868 ( 9.510)\tLoss (MSE) 353.176 (331.591)\tLoss (L1) 14.841 (14.480)\n",
      "2022-01-20 05:56:55,777 | Val: [4/9]\tTime  8.739 ( 9.356)\tLoss (MSE) 324.944 (330.262)\tLoss (L1) 14.912 (14.566)\n",
      "2022-01-20 05:57:04,460 | Val: [5/9]\tTime  8.682 ( 9.244)\tLoss (MSE) 394.440 (340.958)\tLoss (L1) 15.957 (14.798)\n",
      "2022-01-20 05:57:13,200 | Val: [6/9]\tTime  8.741 ( 9.172)\tLoss (MSE) 342.348 (341.157)\tLoss (L1) 15.097 (14.841)\n",
      "2022-01-20 05:57:21,947 | Val: [7/9]\tTime  8.746 ( 9.119)\tLoss (MSE) 369.896 (344.749)\tLoss (L1) 15.339 (14.903)\n",
      "2022-01-20 05:57:24,912 | Val: [8/9]\tTime  2.965 ( 8.435)\tLoss (MSE) 349.860 (344.969)\tLoss (L1) 14.438 (14.883)\n",
      "2022-01-20 05:57:25,051 |  * Overall: MSE 344.969\tL1 14.883\tG-Mean 10.066\n",
      "2022-01-20 05:57:25,052 |  * Many: MSE 197.624\tL1 11.191\tG-Mean 7.496\n",
      "2022-01-20 05:57:25,053 |  * Median: MSE 664.890\tL1 23.176\tG-Mean 20.040\n",
      "2022-01-20 05:57:25,053 |  * Low: MSE 851.834\tL1 26.813\tG-Mean 24.270\n",
      "2022-01-20 05:57:25,059 | Best L1 Loss: 14.883\n",
      "2022-01-20 05:57:26,295 | ===> Saving current best checkpoint...\n",
      "2022-01-20 05:57:27,375 | Epoch #1: Train loss [13.5611]; Val loss: MSE [344.9687], L1 [14.8830], G-Mean [10.0665]\n",
      "2022-01-20 05:57:27,386 | Training...\n",
      "2022-01-20 05:57:27,387 | Load train loader\n",
      "2022-01-20 05:57:28,597 | ===> Batch : 1\n",
      "2022-01-20 05:57:28,599 | FDS disable\n",
      "2022-01-20 05:57:43,229 | Calculate Loss\n",
      "2022-01-20 05:57:43,230 | Update Loss\n",
      "2022-01-20 05:57:43,362 | Backward\n",
      "2022-01-20 05:58:05,155 | Epoch: [2][ 1/48]\tTime  37.77 ( 37.77)\tData 1.2102 (1.2102)\tLoss (L1) 14.125 (14.125)\n",
      "2022-01-20 05:58:05,173 | ===> Batch : 2\n",
      "2022-01-20 05:58:05,173 | FDS disable\n",
      "2022-01-20 05:58:16,952 | Calculate Loss\n",
      "2022-01-20 05:58:16,953 | Update Loss\n",
      "2022-01-20 05:58:16,957 | Backward\n",
      "2022-01-20 05:58:38,711 | Epoch: [2][ 2/48]\tTime  33.56 ( 35.66)\tData 0.0175 (0.6138)\tLoss (L1) 14.055 (14.090)\n",
      "2022-01-20 05:58:38,724 | ===> Batch : 3\n",
      "2022-01-20 05:58:38,725 | FDS disable\n",
      "2022-01-20 05:58:50,439 | Calculate Loss\n",
      "2022-01-20 05:58:50,440 | Update Loss\n",
      "2022-01-20 05:58:50,444 | Backward\n",
      "2022-01-20 05:59:11,350 | Epoch: [2][ 3/48]\tTime  32.64 ( 34.65)\tData 0.0129 (0.4135)\tLoss (L1) 12.678 (13.619)\n",
      "2022-01-20 05:59:11,363 | ===> Batch : 4\n",
      "2022-01-20 05:59:11,363 | FDS disable\n",
      "2022-01-20 05:59:23,113 | Calculate Loss\n",
      "2022-01-20 05:59:23,114 | Update Loss\n",
      "2022-01-20 05:59:23,118 | Backward\n",
      "2022-01-20 05:59:44,197 | Epoch: [2][ 4/48]\tTime  32.85 ( 34.20)\tData 0.0130 (0.3134)\tLoss (L1) 13.791 (13.662)\n",
      "2022-01-20 05:59:44,210 | ===> Batch : 5\n",
      "2022-01-20 05:59:44,211 | FDS disable\n",
      "2022-01-20 05:59:55,962 | Calculate Loss\n",
      "2022-01-20 05:59:55,963 | Update Loss\n",
      "2022-01-20 05:59:55,967 | Backward\n",
      "2022-01-20 06:00:17,121 | Epoch: [2][ 5/48]\tTime  32.92 ( 33.95)\tData 0.0129 (0.2533)\tLoss (L1) 12.686 (13.467)\n",
      "2022-01-20 06:00:17,139 | ===> Batch : 6\n",
      "2022-01-20 06:00:17,139 | FDS disable\n",
      "2022-01-20 06:00:28,863 | Calculate Loss\n",
      "2022-01-20 06:00:28,864 | Update Loss\n",
      "2022-01-20 06:00:28,868 | Backward\n",
      "2022-01-20 06:00:49,578 | Epoch: [2][ 6/48]\tTime  32.46 ( 33.70)\tData 0.0182 (0.2141)\tLoss (L1) 12.832 (13.361)\n",
      "2022-01-20 06:00:49,591 | ===> Batch : 7\n",
      "2022-01-20 06:00:49,592 | FDS disable\n",
      "2022-01-20 06:01:01,328 | Calculate Loss\n",
      "2022-01-20 06:01:01,330 | Update Loss\n",
      "2022-01-20 06:01:01,333 | Backward\n",
      "2022-01-20 06:01:21,790 | Epoch: [2][ 7/48]\tTime  32.21 ( 33.49)\tData 0.0129 (0.1854)\tLoss (L1) 13.040 (13.315)\n",
      "2022-01-20 06:01:21,805 | ===> Batch : 8\n",
      "2022-01-20 06:01:21,805 | FDS disable\n",
      "2022-01-20 06:01:33,558 | Calculate Loss\n",
      "2022-01-20 06:01:33,559 | Update Loss\n",
      "2022-01-20 06:01:33,563 | Backward\n",
      "2022-01-20 06:01:54,193 | Epoch: [2][ 8/48]\tTime  32.40 ( 33.35)\tData 0.0149 (0.1641)\tLoss (L1) 13.515 (13.340)\n",
      "2022-01-20 06:01:54,209 | ===> Batch : 9\n",
      "2022-01-20 06:01:54,209 | FDS disable\n",
      "2022-01-20 06:02:05,936 | Calculate Loss\n",
      "2022-01-20 06:02:05,938 | Update Loss\n",
      "2022-01-20 06:02:05,941 | Backward\n",
      "2022-01-20 06:02:26,457 | Epoch: [2][ 9/48]\tTime  32.26 ( 33.23)\tData 0.0157 (0.1476)\tLoss (L1) 10.884 (13.067)\n",
      "2022-01-20 06:02:26,475 | ===> Batch : 10\n",
      "2022-01-20 06:02:26,476 | FDS disable\n",
      "2022-01-20 06:02:38,161 | Calculate Loss\n",
      "2022-01-20 06:02:38,162 | Update Loss\n",
      "2022-01-20 06:02:38,166 | Backward\n",
      "2022-01-20 06:02:59,158 | Epoch: [2][10/48]\tTime  32.70 ( 33.18)\tData 0.0188 (0.1347)\tLoss (L1) 12.072 (12.968)\n",
      "2022-01-20 06:02:59,171 | ===> Batch : 11\n",
      "2022-01-20 06:02:59,172 | FDS disable\n",
      "2022-01-20 06:03:10,905 | Calculate Loss\n",
      "2022-01-20 06:03:10,906 | Update Loss\n",
      "2022-01-20 06:03:10,910 | Backward\n",
      "2022-01-20 06:03:30,494 | Epoch: [2][11/48]\tTime  31.34 ( 33.01)\tData 0.0128 (0.1236)\tLoss (L1) 13.400 (13.007)\n",
      "2022-01-20 06:03:30,507 | ===> Batch : 12\n",
      "2022-01-20 06:03:30,508 | FDS disable\n",
      "2022-01-20 06:03:42,248 | Calculate Loss\n",
      "2022-01-20 06:03:42,250 | Update Loss\n",
      "2022-01-20 06:03:42,254 | Backward\n",
      "2022-01-20 06:04:03,669 | Epoch: [2][12/48]\tTime  33.17 ( 33.02)\tData 0.0127 (0.1144)\tLoss (L1) 12.808 (12.990)\n",
      "2022-01-20 06:04:03,682 | ===> Batch : 13\n",
      "2022-01-20 06:04:03,682 | FDS disable\n",
      "2022-01-20 06:04:15,464 | Calculate Loss\n",
      "2022-01-20 06:04:15,465 | Update Loss\n",
      "2022-01-20 06:04:15,469 | Backward\n",
      "2022-01-20 06:04:36,194 | Epoch: [2][13/48]\tTime  32.53 ( 32.99)\tData 0.0127 (0.1065)\tLoss (L1) 12.527 (12.955)\n",
      "2022-01-20 06:04:36,209 | ===> Batch : 14\n",
      "2022-01-20 06:04:36,209 | FDS disable\n",
      "2022-01-20 06:04:47,933 | Calculate Loss\n",
      "2022-01-20 06:04:47,934 | Update Loss\n",
      "2022-01-20 06:04:47,938 | Backward\n",
      "2022-01-20 06:05:08,982 | Epoch: [2][14/48]\tTime  32.79 ( 32.97)\tData 0.0147 (0.1000)\tLoss (L1) 12.556 (12.926)\n",
      "2022-01-20 06:05:08,995 | ===> Batch : 15\n",
      "2022-01-20 06:05:08,996 | FDS disable\n",
      "2022-01-20 06:05:20,732 | Calculate Loss\n",
      "2022-01-20 06:05:20,734 | Update Loss\n",
      "2022-01-20 06:05:20,737 | Backward\n",
      "2022-01-20 06:05:41,955 | Epoch: [2][15/48]\tTime  32.97 ( 32.97)\tData 0.0129 (0.0942)\tLoss (L1) 14.447 (13.028)\n",
      "2022-01-20 06:05:41,969 | ===> Batch : 16\n",
      "2022-01-20 06:05:41,970 | FDS disable\n",
      "2022-01-20 06:05:53,695 | Calculate Loss\n",
      "2022-01-20 06:05:53,696 | Update Loss\n",
      "2022-01-20 06:05:53,699 | Backward\n",
      "2022-01-20 06:06:14,376 | Epoch: [2][16/48]\tTime  32.42 ( 32.94)\tData 0.0140 (0.0892)\tLoss (L1) 14.046 (13.091)\n",
      "2022-01-20 06:06:14,391 | ===> Batch : 17\n",
      "2022-01-20 06:06:14,392 | FDS disable\n",
      "2022-01-20 06:06:26,148 | Calculate Loss\n",
      "2022-01-20 06:06:26,149 | Update Loss\n",
      "2022-01-20 06:06:26,153 | Backward\n",
      "2022-01-20 06:06:47,496 | Epoch: [2][17/48]\tTime  33.12 ( 32.95)\tData 0.0151 (0.0848)\tLoss (L1) 11.843 (13.018)\n",
      "2022-01-20 06:06:47,510 | ===> Batch : 18\n",
      "2022-01-20 06:06:47,511 | FDS disable\n",
      "2022-01-20 06:06:59,258 | Calculate Loss\n",
      "2022-01-20 06:06:59,259 | Update Loss\n",
      "2022-01-20 06:06:59,263 | Backward\n",
      "2022-01-20 06:07:19,942 | Epoch: [2][18/48]\tTime  32.45 ( 32.92)\tData 0.0140 (0.0809)\tLoss (L1) 12.222 (12.974)\n",
      "2022-01-20 06:07:19,957 | ===> Batch : 19\n",
      "2022-01-20 06:07:19,958 | FDS disable\n",
      "2022-01-20 06:07:31,629 | Calculate Loss\n",
      "2022-01-20 06:07:31,630 | Update Loss\n",
      "2022-01-20 06:07:31,633 | Backward\n",
      "2022-01-20 06:07:52,915 | Epoch: [2][19/48]\tTime  32.97 ( 32.92)\tData 0.0149 (0.0774)\tLoss (L1) 13.243 (12.988)\n",
      "2022-01-20 06:07:52,929 | ===> Batch : 20\n",
      "2022-01-20 06:07:52,930 | FDS disable\n",
      "2022-01-20 06:08:04,656 | Calculate Loss\n",
      "2022-01-20 06:08:04,657 | Update Loss\n",
      "2022-01-20 06:08:04,661 | Backward\n",
      "2022-01-20 06:08:25,949 | Epoch: [2][20/48]\tTime  33.03 ( 32.93)\tData 0.0137 (0.0742)\tLoss (L1) 12.893 (12.983)\n",
      "2022-01-20 06:08:25,964 | ===> Batch : 21\n",
      "2022-01-20 06:08:25,964 | FDS disable\n",
      "2022-01-20 06:08:37,727 | Calculate Loss\n",
      "2022-01-20 06:08:37,728 | Update Loss\n",
      "2022-01-20 06:08:37,732 | Backward\n",
      "2022-01-20 06:08:58,346 | Epoch: [2][21/48]\tTime  32.40 ( 32.90)\tData 0.0147 (0.0714)\tLoss (L1) 11.431 (12.909)\n",
      "2022-01-20 06:08:58,359 | ===> Batch : 22\n",
      "2022-01-20 06:08:58,360 | FDS disable\n",
      "2022-01-20 06:09:10,049 | Calculate Loss\n",
      "2022-01-20 06:09:10,052 | Update Loss\n",
      "2022-01-20 06:09:10,055 | Backward\n",
      "2022-01-20 06:09:31,325 | Epoch: [2][22/48]\tTime  32.98 ( 32.91)\tData 0.0136 (0.0688)\tLoss (L1) 14.463 (12.980)\n",
      "2022-01-20 06:09:31,340 | ===> Batch : 23\n",
      "2022-01-20 06:09:31,340 | FDS disable\n",
      "2022-01-20 06:09:43,058 | Calculate Loss\n",
      "2022-01-20 06:09:43,060 | Update Loss\n",
      "2022-01-20 06:09:43,063 | Backward\n",
      "2022-01-20 06:10:04,279 | Epoch: [2][23/48]\tTime  32.95 ( 32.91)\tData 0.0150 (0.0664)\tLoss (L1) 12.118 (12.942)\n",
      "2022-01-20 06:10:04,294 | ===> Batch : 24\n",
      "2022-01-20 06:10:04,295 | FDS disable\n",
      "2022-01-20 06:10:16,064 | Calculate Loss\n",
      "2022-01-20 06:10:16,065 | Update Loss\n",
      "2022-01-20 06:10:16,069 | Backward\n",
      "2022-01-20 06:10:36,811 | Epoch: [2][24/48]\tTime  32.53 ( 32.89)\tData 0.0150 (0.0643)\tLoss (L1) 10.746 (12.851)\n",
      "2022-01-20 06:10:36,831 | ===> Batch : 25\n",
      "2022-01-20 06:10:36,831 | FDS disable\n",
      "2022-01-20 06:10:48,513 | Calculate Loss\n",
      "2022-01-20 06:10:48,515 | Update Loss\n",
      "2022-01-20 06:10:48,518 | Backward\n",
      "2022-01-20 06:11:09,658 | Epoch: [2][25/48]\tTime  32.85 ( 32.89)\tData 0.0196 (0.0625)\tLoss (L1) 13.500 (12.877)\n",
      "2022-01-20 06:11:09,672 | ===> Batch : 26\n",
      "2022-01-20 06:11:09,673 | FDS disable\n",
      "2022-01-20 06:11:21,358 | Calculate Loss\n",
      "2022-01-20 06:11:21,360 | Update Loss\n",
      "2022-01-20 06:11:21,364 | Backward\n",
      "2022-01-20 06:11:42,187 | Epoch: [2][26/48]\tTime  32.53 ( 32.88)\tData 0.0143 (0.0606)\tLoss (L1) 12.304 (12.855)\n",
      "2022-01-20 06:11:42,201 | ===> Batch : 27\n",
      "2022-01-20 06:11:42,201 | FDS disable\n",
      "2022-01-20 06:11:53,868 | Calculate Loss\n",
      "2022-01-20 06:11:53,869 | Update Loss\n",
      "2022-01-20 06:11:53,872 | Backward\n",
      "2022-01-20 06:12:15,278 | Epoch: [2][27/48]\tTime  33.09 ( 32.88)\tData 0.0138 (0.0589)\tLoss (L1) 12.446 (12.840)\n",
      "2022-01-20 06:12:15,297 | ===> Batch : 28\n",
      "2022-01-20 06:12:15,298 | FDS disable\n",
      "2022-01-20 06:12:27,018 | Calculate Loss\n",
      "2022-01-20 06:12:27,020 | Update Loss\n",
      "2022-01-20 06:12:27,023 | Backward\n",
      "2022-01-20 06:12:48,033 | Epoch: [2][28/48]\tTime  32.76 ( 32.88)\tData 0.0195 (0.0575)\tLoss (L1) 13.418 (12.860)\n",
      "2022-01-20 06:12:48,047 | ===> Batch : 29\n",
      "2022-01-20 06:12:48,047 | FDS disable\n",
      "2022-01-20 06:12:59,772 | Calculate Loss\n",
      "2022-01-20 06:12:59,773 | Update Loss\n",
      "2022-01-20 06:12:59,777 | Backward\n",
      "2022-01-20 06:13:20,776 | Epoch: [2][29/48]\tTime  32.74 ( 32.88)\tData 0.0139 (0.0560)\tLoss (L1) 12.614 (12.852)\n",
      "2022-01-20 06:13:20,789 | ===> Batch : 30\n",
      "2022-01-20 06:13:20,790 | FDS disable\n",
      "2022-01-20 06:13:32,502 | Calculate Loss\n",
      "2022-01-20 06:13:32,504 | Update Loss\n",
      "2022-01-20 06:13:32,508 | Backward\n",
      "2022-01-20 06:13:53,486 | Epoch: [2][30/48]\tTime  32.71 ( 32.87)\tData 0.0139 (0.0546)\tLoss (L1) 13.373 (12.869)\n",
      "2022-01-20 06:13:53,506 | ===> Batch : 31\n",
      "2022-01-20 06:13:53,506 | FDS disable\n",
      "2022-01-20 06:14:05,208 | Calculate Loss\n",
      "2022-01-20 06:14:05,210 | Update Loss\n",
      "2022-01-20 06:14:05,213 | Backward\n",
      "2022-01-20 06:14:26,469 | Epoch: [2][31/48]\tTime  32.98 ( 32.87)\tData 0.0195 (0.0535)\tLoss (L1) 16.440 (12.984)\n",
      "2022-01-20 06:14:26,483 | ===> Batch : 32\n",
      "2022-01-20 06:14:26,483 | FDS disable\n",
      "2022-01-20 06:14:38,138 | Calculate Loss\n",
      "2022-01-20 06:14:38,140 | Update Loss\n",
      "2022-01-20 06:14:38,144 | Backward\n",
      "2022-01-20 06:14:59,321 | Epoch: [2][32/48]\tTime  32.85 ( 32.87)\tData 0.0139 (0.0522)\tLoss (L1) 13.151 (12.990)\n",
      "2022-01-20 06:14:59,335 | ===> Batch : 33\n",
      "2022-01-20 06:14:59,335 | FDS disable\n",
      "2022-01-20 06:15:10,942 | Calculate Loss\n",
      "2022-01-20 06:15:10,943 | Update Loss\n",
      "2022-01-20 06:15:10,947 | Backward\n",
      "2022-01-20 06:15:31,628 | Epoch: [2][33/48]\tTime  32.31 ( 32.86)\tData 0.0137 (0.0511)\tLoss (L1) 14.506 (13.035)\n",
      "2022-01-20 06:15:31,641 | ===> Batch : 34\n",
      "2022-01-20 06:15:31,642 | FDS disable\n",
      "2022-01-20 06:15:43,230 | Calculate Loss\n",
      "2022-01-20 06:15:43,232 | Update Loss\n",
      "2022-01-20 06:15:43,235 | Backward\n",
      "2022-01-20 06:16:04,193 | Epoch: [2][34/48]\tTime  32.57 ( 32.85)\tData 0.0137 (0.0500)\tLoss (L1) 11.491 (12.990)\n",
      "2022-01-20 06:16:04,208 | ===> Batch : 35\n",
      "2022-01-20 06:16:04,209 | FDS disable\n",
      "2022-01-20 06:16:15,795 | Calculate Loss\n",
      "2022-01-20 06:16:15,796 | Update Loss\n",
      "2022-01-20 06:16:15,799 | Backward\n",
      "2022-01-20 06:16:36,367 | Epoch: [2][35/48]\tTime  32.17 ( 32.83)\tData 0.0149 (0.0490)\tLoss (L1) 12.423 (12.974)\n",
      "2022-01-20 06:16:36,381 | ===> Batch : 36\n",
      "2022-01-20 06:16:36,382 | FDS disable\n",
      "2022-01-20 06:16:47,927 | Calculate Loss\n",
      "2022-01-20 06:16:47,928 | Update Loss\n",
      "2022-01-20 06:16:47,931 | Backward\n",
      "2022-01-20 06:17:08,760 | Epoch: [2][36/48]\tTime  32.39 ( 32.82)\tData 0.0140 (0.0480)\tLoss (L1) 10.511 (12.905)\n",
      "2022-01-20 06:17:08,773 | ===> Batch : 37\n",
      "2022-01-20 06:17:08,774 | FDS disable\n",
      "2022-01-20 06:17:20,284 | Calculate Loss\n",
      "2022-01-20 06:17:20,285 | Update Loss\n",
      "2022-01-20 06:17:20,289 | Backward\n",
      "2022-01-20 06:17:41,684 | Epoch: [2][37/48]\tTime  32.92 ( 32.82)\tData 0.0133 (0.0470)\tLoss (L1) 11.363 (12.864)\n",
      "2022-01-20 06:17:41,698 | ===> Batch : 38\n",
      "2022-01-20 06:17:41,699 | FDS disable\n",
      "2022-01-20 06:17:53,269 | Calculate Loss\n",
      "2022-01-20 06:17:53,270 | Update Loss\n",
      "2022-01-20 06:17:53,273 | Backward\n",
      "2022-01-20 06:18:13,739 | Epoch: [2][38/48]\tTime  32.06 ( 32.80)\tData 0.0138 (0.0462)\tLoss (L1) 11.060 (12.816)\n",
      "2022-01-20 06:18:13,754 | ===> Batch : 39\n",
      "2022-01-20 06:18:13,754 | FDS disable\n",
      "2022-01-20 06:18:25,285 | Calculate Loss\n",
      "2022-01-20 06:18:25,286 | Update Loss\n",
      "2022-01-20 06:18:25,290 | Backward\n",
      "2022-01-20 06:18:46,408 | Epoch: [2][39/48]\tTime  32.67 ( 32.80)\tData 0.0142 (0.0454)\tLoss (L1) 11.229 (12.776)\n",
      "2022-01-20 06:18:46,422 | ===> Batch : 40\n",
      "2022-01-20 06:18:46,423 | FDS disable\n",
      "2022-01-20 06:18:57,947 | Calculate Loss\n",
      "2022-01-20 06:18:57,948 | Update Loss\n",
      "2022-01-20 06:18:57,951 | Backward\n",
      "2022-01-20 06:19:19,352 | Epoch: [2][40/48]\tTime  32.94 ( 32.80)\tData 0.0142 (0.0446)\tLoss (L1) 12.984 (12.781)\n",
      "2022-01-20 06:19:19,367 | ===> Batch : 41\n",
      "2022-01-20 06:19:19,368 | FDS disable\n",
      "2022-01-20 06:19:30,929 | Calculate Loss\n",
      "2022-01-20 06:19:30,930 | Update Loss\n",
      "2022-01-20 06:19:30,934 | Backward\n",
      "2022-01-20 06:19:51,881 | Epoch: [2][41/48]\tTime  32.53 ( 32.79)\tData 0.0148 (0.0439)\tLoss (L1) 12.385 (12.771)\n",
      "2022-01-20 06:19:51,895 | ===> Batch : 42\n",
      "2022-01-20 06:19:51,896 | FDS disable\n",
      "2022-01-20 06:20:03,372 | Calculate Loss\n",
      "2022-01-20 06:20:03,374 | Update Loss\n",
      "2022-01-20 06:20:03,377 | Backward\n",
      "2022-01-20 06:20:24,238 | Epoch: [2][42/48]\tTime  32.36 ( 32.78)\tData 0.0141 (0.0431)\tLoss (L1) 12.389 (12.762)\n",
      "2022-01-20 06:20:24,252 | ===> Batch : 43\n",
      "2022-01-20 06:20:24,253 | FDS disable\n",
      "2022-01-20 06:20:35,787 | Calculate Loss\n",
      "2022-01-20 06:20:35,789 | Update Loss\n",
      "2022-01-20 06:20:35,792 | Backward\n",
      "2022-01-20 06:20:56,738 | Epoch: [2][43/48]\tTime  32.50 ( 32.78)\tData 0.0140 (0.0425)\tLoss (L1) 11.690 (12.737)\n",
      "2022-01-20 06:20:56,752 | ===> Batch : 44\n",
      "2022-01-20 06:20:56,752 | FDS disable\n",
      "2022-01-20 06:21:08,353 | Calculate Loss\n",
      "2022-01-20 06:21:08,355 | Update Loss\n",
      "2022-01-20 06:21:08,358 | Backward\n",
      "2022-01-20 06:21:28,777 | Epoch: [2][44/48]\tTime  32.04 ( 32.76)\tData 0.0138 (0.0418)\tLoss (L1) 14.206 (12.770)\n",
      "2022-01-20 06:21:28,791 | ===> Batch : 45\n",
      "2022-01-20 06:21:28,791 | FDS disable\n",
      "2022-01-20 06:21:40,348 | Calculate Loss\n",
      "2022-01-20 06:21:40,349 | Update Loss\n",
      "2022-01-20 06:21:40,352 | Backward\n",
      "2022-01-20 06:22:01,641 | Epoch: [2][45/48]\tTime  32.86 ( 32.76)\tData 0.0136 (0.0412)\tLoss (L1) 13.272 (12.782)\n",
      "2022-01-20 06:22:01,656 | ===> Batch : 46\n",
      "2022-01-20 06:22:01,657 | FDS disable\n",
      "2022-01-20 06:22:13,191 | Calculate Loss\n",
      "2022-01-20 06:22:13,193 | Update Loss\n",
      "2022-01-20 06:22:13,196 | Backward\n",
      "2022-01-20 06:22:34,140 | Epoch: [2][46/48]\tTime  32.50 ( 32.76)\tData 0.0151 (0.0406)\tLoss (L1) 11.946 (12.763)\n",
      "2022-01-20 06:22:34,154 | ===> Batch : 47\n",
      "2022-01-20 06:22:34,154 | FDS disable\n",
      "2022-01-20 06:22:45,742 | Calculate Loss\n",
      "2022-01-20 06:22:45,743 | Update Loss\n",
      "2022-01-20 06:22:45,746 | Backward\n",
      "2022-01-20 06:23:06,920 | Epoch: [2][47/48]\tTime  32.78 ( 32.76)\tData 0.0140 (0.0401)\tLoss (L1) 12.781 (12.764)\n",
      "2022-01-20 06:23:06,933 | ===> Batch : 48\n",
      "2022-01-20 06:23:06,934 | FDS disable\n",
      "2022-01-20 06:23:14,992 | Calculate Loss\n",
      "2022-01-20 06:23:14,993 | Update Loss\n",
      "2022-01-20 06:23:14,997 | Backward\n",
      "2022-01-20 06:23:29,384 | Epoch: [2][48/48]\tTime  22.46 ( 32.54)\tData 0.0136 (0.0395)\tLoss (L1) 14.673 (12.791)\n",
      "2022-01-20 06:23:41,355 | Val: [0/9]\tTime 11.806 (11.806)\tLoss (MSE) 432.600 (432.600)\tLoss (L1) 16.413 (16.413)\n",
      "2022-01-20 06:23:50,122 | Val: [1/9]\tTime  8.767 (10.286)\tLoss (MSE) 473.904 (453.252)\tLoss (L1) 17.120 (16.766)\n",
      "2022-01-20 06:23:58,823 | Val: [2/9]\tTime  8.701 ( 9.758)\tLoss (MSE) 540.619 (482.375)\tLoss (L1) 19.198 (17.577)\n",
      "2022-01-20 06:24:07,586 | Val: [3/9]\tTime  8.763 ( 9.509)\tLoss (MSE) 506.363 (488.372)\tLoss (L1) 18.053 (17.696)\n",
      "2022-01-20 06:24:16,323 | Val: [4/9]\tTime  8.737 ( 9.355)\tLoss (MSE) 458.644 (482.426)\tLoss (L1) 17.603 (17.677)\n",
      "2022-01-20 06:24:25,057 | Val: [5/9]\tTime  8.734 ( 9.251)\tLoss (MSE) 586.743 (499.812)\tLoss (L1) 20.100 (18.081)\n",
      "2022-01-20 06:24:33,808 | Val: [6/9]\tTime  8.751 ( 9.180)\tLoss (MSE) 541.188 (505.723)\tLoss (L1) 19.256 (18.249)\n",
      "2022-01-20 06:24:42,639 | Val: [7/9]\tTime  8.831 ( 9.136)\tLoss (MSE) 517.196 (507.157)\tLoss (L1) 18.502 (18.281)\n",
      "2022-01-20 06:24:45,608 | Val: [8/9]\tTime  2.968 ( 8.451)\tLoss (MSE) 541.625 (508.639)\tLoss (L1) 19.248 (18.322)\n",
      "2022-01-20 06:24:45,753 |  * Overall: MSE 508.639\tL1 18.322\tG-Mean 12.191\n",
      "2022-01-20 06:24:45,754 |  * Many: MSE 262.400\tL1 13.144\tG-Mean 8.731\n",
      "2022-01-20 06:24:45,755 |  * Median: MSE 1069.823\tL1 30.764\tG-Mean 27.855\n",
      "2022-01-20 06:24:45,755 |  * Low: MSE 1282.311\tL1 32.823\tG-Mean 29.037\n",
      "2022-01-20 06:24:45,760 | Best L1 Loss: 14.883\n",
      "2022-01-20 06:24:47,005 | Epoch #2: Train loss [12.7914]; Val loss: MSE [508.6391], L1 [18.3222], G-Mean [12.1912]\n",
      "2022-01-20 06:24:47,006 | Training...\n",
      "2022-01-20 06:24:47,008 | Load train loader\n",
      "2022-01-20 06:24:48,190 | ===> Batch : 1\n",
      "2022-01-20 06:24:48,192 | FDS disable\n",
      "2022-01-20 06:25:02,888 | Calculate Loss\n",
      "2022-01-20 06:25:02,889 | Update Loss\n",
      "2022-01-20 06:25:03,021 | Backward\n",
      "2022-01-20 06:25:24,943 | Epoch: [3][ 1/48]\tTime  37.94 ( 37.94)\tData 1.1828 (1.1828)\tLoss (L1) 10.989 (10.989)\n",
      "2022-01-20 06:25:24,959 | ===> Batch : 2\n",
      "2022-01-20 06:25:24,960 | FDS disable\n",
      "2022-01-20 06:25:36,672 | Calculate Loss\n",
      "2022-01-20 06:25:36,673 | Update Loss\n",
      "2022-01-20 06:25:36,677 | Backward\n",
      "2022-01-20 06:25:57,714 | Epoch: [3][ 2/48]\tTime  32.77 ( 35.35)\tData 0.0160 (0.5994)\tLoss (L1) 11.502 (11.246)\n",
      "2022-01-20 06:25:57,727 | ===> Batch : 3\n",
      "2022-01-20 06:25:57,728 | FDS disable\n",
      "2022-01-20 06:26:09,400 | Calculate Loss\n",
      "2022-01-20 06:26:09,401 | Update Loss\n",
      "2022-01-20 06:26:09,405 | Backward\n",
      "2022-01-20 06:26:29,948 | Epoch: [3][ 3/48]\tTime  32.23 ( 34.31)\tData 0.0132 (0.4040)\tLoss (L1) 12.566 (11.686)\n",
      "2022-01-20 06:26:29,963 | ===> Batch : 4\n",
      "2022-01-20 06:26:29,964 | FDS disable\n",
      "2022-01-20 06:26:41,608 | Calculate Loss\n",
      "2022-01-20 06:26:41,609 | Update Loss\n",
      "2022-01-20 06:26:41,613 | Backward\n",
      "2022-01-20 06:27:02,393 | Epoch: [3][ 4/48]\tTime  32.45 ( 33.85)\tData 0.0154 (0.3068)\tLoss (L1) 11.523 (11.645)\n",
      "2022-01-20 06:27:02,409 | ===> Batch : 5\n",
      "2022-01-20 06:27:02,410 | FDS disable\n",
      "2022-01-20 06:27:14,091 | Calculate Loss\n",
      "2022-01-20 06:27:14,093 | Update Loss\n",
      "2022-01-20 06:27:14,097 | Backward\n",
      "2022-01-20 06:27:34,963 | Epoch: [3][ 5/48]\tTime  32.57 ( 33.59)\tData 0.0157 (0.2486)\tLoss (L1) 12.949 (11.906)\n",
      "2022-01-20 06:27:34,976 | ===> Batch : 6\n",
      "2022-01-20 06:27:34,976 | FDS disable\n",
      "2022-01-20 06:27:46,657 | Calculate Loss\n",
      "2022-01-20 06:27:46,658 | Update Loss\n",
      "2022-01-20 06:27:46,661 | Backward\n",
      "2022-01-20 06:28:07,842 | Epoch: [3][ 6/48]\tTime  32.88 ( 33.47)\tData 0.0130 (0.2093)\tLoss (L1) 13.156 (12.114)\n",
      "2022-01-20 06:28:07,855 | ===> Batch : 7\n",
      "2022-01-20 06:28:07,856 | FDS disable\n",
      "2022-01-20 06:28:19,557 | Calculate Loss\n",
      "2022-01-20 06:28:19,559 | Update Loss\n",
      "2022-01-20 06:28:19,563 | Backward\n",
      "2022-01-20 06:28:39,471 | Epoch: [3][ 7/48]\tTime  31.63 ( 33.21)\tData 0.0132 (0.1813)\tLoss (L1) 13.427 (12.302)\n",
      "2022-01-20 06:28:39,484 | ===> Batch : 8\n",
      "2022-01-20 06:28:39,485 | FDS disable\n",
      "2022-01-20 06:28:51,188 | Calculate Loss\n",
      "2022-01-20 06:28:51,189 | Update Loss\n",
      "2022-01-20 06:28:51,193 | Backward\n",
      "2022-01-20 06:29:12,113 | Epoch: [3][ 8/48]\tTime  32.64 ( 33.14)\tData 0.0133 (0.1603)\tLoss (L1) 13.848 (12.495)\n",
      "2022-01-20 06:29:12,128 | ===> Batch : 9\n",
      "2022-01-20 06:29:12,129 | FDS disable\n",
      "2022-01-20 06:29:23,812 | Calculate Loss\n",
      "2022-01-20 06:29:23,813 | Update Loss\n",
      "2022-01-20 06:29:23,816 | Backward\n",
      "2022-01-20 06:29:44,265 | Epoch: [3][ 9/48]\tTime  32.15 ( 33.03)\tData 0.0152 (0.1442)\tLoss (L1) 13.233 (12.577)\n",
      "2022-01-20 06:29:44,280 | ===> Batch : 10\n",
      "2022-01-20 06:29:44,281 | FDS disable\n",
      "2022-01-20 06:29:55,927 | Calculate Loss\n",
      "2022-01-20 06:29:55,929 | Update Loss\n",
      "2022-01-20 06:29:55,932 | Backward\n",
      "2022-01-20 06:30:16,681 | Epoch: [3][10/48]\tTime  32.42 ( 32.97)\tData 0.0158 (0.1314)\tLoss (L1) 13.882 (12.707)\n",
      "2022-01-20 06:30:16,694 | ===> Batch : 11\n",
      "2022-01-20 06:30:16,695 | FDS disable\n",
      "2022-01-20 06:30:28,333 | Calculate Loss\n",
      "2022-01-20 06:30:28,335 | Update Loss\n",
      "2022-01-20 06:30:28,338 | Backward\n",
      "2022-01-20 06:30:49,580 | Epoch: [3][11/48]\tTime  32.90 ( 32.96)\tData 0.0135 (0.1206)\tLoss (L1) 12.016 (12.645)\n",
      "2022-01-20 06:30:49,593 | ===> Batch : 12\n",
      "2022-01-20 06:30:49,593 | FDS disable\n",
      "2022-01-20 06:31:01,214 | Calculate Loss\n",
      "2022-01-20 06:31:01,215 | Update Loss\n",
      "2022-01-20 06:31:01,219 | Backward\n",
      "2022-01-20 06:31:22,237 | Epoch: [3][12/48]\tTime  32.66 ( 32.94)\tData 0.0127 (0.1116)\tLoss (L1) 12.270 (12.613)\n",
      "2022-01-20 06:31:22,252 | ===> Batch : 13\n",
      "2022-01-20 06:31:22,252 | FDS disable\n",
      "2022-01-20 06:31:33,905 | Calculate Loss\n",
      "2022-01-20 06:31:33,907 | Update Loss\n",
      "2022-01-20 06:31:33,911 | Backward\n",
      "2022-01-20 06:31:54,756 | Epoch: [3][13/48]\tTime  32.52 ( 32.90)\tData 0.0144 (0.1042)\tLoss (L1) 12.867 (12.633)\n",
      "2022-01-20 06:31:54,772 | ===> Batch : 14\n",
      "2022-01-20 06:31:54,772 | FDS disable\n",
      "2022-01-20 06:32:06,498 | Calculate Loss\n",
      "2022-01-20 06:32:06,499 | Update Loss\n",
      "2022-01-20 06:32:06,503 | Backward\n",
      "2022-01-20 06:32:27,788 | Epoch: [3][14/48]\tTime  33.03 ( 32.91)\tData 0.0158 (0.0979)\tLoss (L1) 12.776 (12.643)\n",
      "2022-01-20 06:32:27,801 | ===> Batch : 15\n",
      "2022-01-20 06:32:27,802 | FDS disable\n",
      "2022-01-20 06:32:39,535 | Calculate Loss\n",
      "2022-01-20 06:32:39,536 | Update Loss\n",
      "2022-01-20 06:32:39,539 | Backward\n",
      "2022-01-20 06:33:00,169 | Epoch: [3][15/48]\tTime  32.38 ( 32.88)\tData 0.0135 (0.0922)\tLoss (L1) 12.816 (12.655)\n",
      "2022-01-20 06:33:00,182 | ===> Batch : 16\n",
      "2022-01-20 06:33:00,182 | FDS disable\n",
      "2022-01-20 06:33:11,862 | Calculate Loss\n",
      "2022-01-20 06:33:11,864 | Update Loss\n",
      "2022-01-20 06:33:11,868 | Backward\n",
      "2022-01-20 06:33:33,413 | Epoch: [3][16/48]\tTime  33.24 ( 32.90)\tData 0.0125 (0.0872)\tLoss (L1) 12.903 (12.670)\n",
      "2022-01-20 06:33:33,427 | ===> Batch : 17\n",
      "2022-01-20 06:33:33,428 | FDS disable\n",
      "2022-01-20 06:33:45,174 | Calculate Loss\n",
      "2022-01-20 06:33:45,175 | Update Loss\n",
      "2022-01-20 06:33:45,179 | Backward\n",
      "2022-01-20 06:34:05,430 | Epoch: [3][17/48]\tTime  32.02 ( 32.85)\tData 0.0145 (0.0830)\tLoss (L1) 14.044 (12.751)\n",
      "2022-01-20 06:34:05,444 | ===> Batch : 18\n",
      "2022-01-20 06:34:05,444 | FDS disable\n",
      "2022-01-20 06:34:17,175 | Calculate Loss\n",
      "2022-01-20 06:34:17,176 | Update Loss\n",
      "2022-01-20 06:34:17,180 | Backward\n",
      "2022-01-20 06:34:38,638 | Epoch: [3][18/48]\tTime  33.21 ( 32.87)\tData 0.0141 (0.0791)\tLoss (L1) 14.294 (12.837)\n",
      "2022-01-20 06:34:38,651 | ===> Batch : 19\n",
      "2022-01-20 06:34:38,652 | FDS disable\n",
      "2022-01-20 06:34:50,369 | Calculate Loss\n",
      "2022-01-20 06:34:50,370 | Update Loss\n",
      "2022-01-20 06:34:50,374 | Backward\n",
      "2022-01-20 06:35:10,837 | Epoch: [3][19/48]\tTime  32.20 ( 32.83)\tData 0.0138 (0.0757)\tLoss (L1) 12.744 (12.832)\n",
      "2022-01-20 06:35:10,850 | ===> Batch : 20\n",
      "2022-01-20 06:35:10,851 | FDS disable\n",
      "2022-01-20 06:35:22,538 | Calculate Loss\n",
      "2022-01-20 06:35:22,539 | Update Loss\n",
      "2022-01-20 06:35:22,543 | Backward\n",
      "2022-01-20 06:35:43,737 | Epoch: [3][20/48]\tTime  32.90 ( 32.84)\tData 0.0134 (0.0726)\tLoss (L1) 12.163 (12.798)\n",
      "2022-01-20 06:35:43,751 | ===> Batch : 21\n",
      "2022-01-20 06:35:43,751 | FDS disable\n",
      "2022-01-20 06:35:55,436 | Calculate Loss\n",
      "2022-01-20 06:35:55,438 | Update Loss\n",
      "2022-01-20 06:35:55,441 | Backward\n",
      "2022-01-20 06:36:15,481 | Epoch: [3][21/48]\tTime  31.74 ( 32.78)\tData 0.0139 (0.0698)\tLoss (L1) 13.310 (12.823)\n",
      "2022-01-20 06:36:15,495 | ===> Batch : 22\n",
      "2022-01-20 06:36:15,496 | FDS disable\n",
      "2022-01-20 06:36:27,262 | Calculate Loss\n",
      "2022-01-20 06:36:27,263 | Update Loss\n",
      "2022-01-20 06:36:27,267 | Backward\n",
      "2022-01-20 06:36:47,689 | Epoch: [3][22/48]\tTime  32.21 ( 32.76)\tData 0.0139 (0.0673)\tLoss (L1) 9.951 (12.692)\n",
      "2022-01-20 06:36:47,703 | ===> Batch : 23\n",
      "2022-01-20 06:36:47,704 | FDS disable\n",
      "2022-01-20 06:36:59,490 | Calculate Loss\n",
      "2022-01-20 06:36:59,491 | Update Loss\n",
      "2022-01-20 06:36:59,495 | Backward\n",
      "2022-01-20 06:37:20,621 | Epoch: [3][23/48]\tTime  32.93 ( 32.77)\tData 0.0147 (0.0650)\tLoss (L1) 11.555 (12.643)\n",
      "2022-01-20 06:37:20,636 | ===> Batch : 24\n",
      "2022-01-20 06:37:20,636 | FDS disable\n",
      "2022-01-20 06:37:32,313 | Calculate Loss\n",
      "2022-01-20 06:37:32,315 | Update Loss\n",
      "2022-01-20 06:37:32,319 | Backward\n",
      "2022-01-20 06:37:53,488 | Epoch: [3][24/48]\tTime  32.87 ( 32.77)\tData 0.0150 (0.0629)\tLoss (L1) 11.599 (12.599)\n",
      "2022-01-20 06:37:53,502 | ===> Batch : 25\n",
      "2022-01-20 06:37:53,502 | FDS disable\n",
      "2022-01-20 06:38:05,115 | Calculate Loss\n",
      "2022-01-20 06:38:05,117 | Update Loss\n",
      "2022-01-20 06:38:05,120 | Backward\n",
      "2022-01-20 06:38:25,987 | Epoch: [3][25/48]\tTime  32.50 ( 32.76)\tData 0.0139 (0.0609)\tLoss (L1) 12.164 (12.582)\n",
      "2022-01-20 06:38:26,003 | ===> Batch : 26\n",
      "2022-01-20 06:38:26,003 | FDS disable\n",
      "2022-01-20 06:38:37,709 | Calculate Loss\n",
      "2022-01-20 06:38:37,710 | Update Loss\n",
      "2022-01-20 06:38:37,714 | Backward\n",
      "2022-01-20 06:38:59,147 | Epoch: [3][26/48]\tTime  33.16 ( 32.77)\tData 0.0161 (0.0592)\tLoss (L1) 12.618 (12.583)\n",
      "2022-01-20 06:38:59,161 | ===> Batch : 27\n",
      "2022-01-20 06:38:59,162 | FDS disable\n",
      "2022-01-20 06:39:10,871 | Calculate Loss\n",
      "2022-01-20 06:39:10,872 | Update Loss\n",
      "2022-01-20 06:39:10,876 | Backward\n",
      "2022-01-20 06:39:31,751 | Epoch: [3][27/48]\tTime  32.60 ( 32.77)\tData 0.0140 (0.0575)\tLoss (L1) 11.986 (12.561)\n",
      "2022-01-20 06:39:31,766 | ===> Batch : 28\n",
      "2022-01-20 06:39:31,766 | FDS disable\n",
      "2022-01-20 06:39:43,490 | Calculate Loss\n",
      "2022-01-20 06:39:43,491 | Update Loss\n",
      "2022-01-20 06:39:43,495 | Backward\n",
      "2022-01-20 06:40:04,405 | Epoch: [3][28/48]\tTime  32.65 ( 32.76)\tData 0.0153 (0.0560)\tLoss (L1) 12.181 (12.548)\n",
      "2022-01-20 06:40:04,420 | ===> Batch : 29\n",
      "2022-01-20 06:40:04,421 | FDS disable\n",
      "2022-01-20 06:40:16,185 | Calculate Loss\n",
      "2022-01-20 06:40:16,186 | Update Loss\n",
      "2022-01-20 06:40:16,191 | Backward\n",
      "2022-01-20 06:40:36,939 | Epoch: [3][29/48]\tTime  32.53 ( 32.76)\tData 0.0152 (0.0546)\tLoss (L1) 12.825 (12.557)\n",
      "2022-01-20 06:40:36,953 | ===> Batch : 30\n",
      "2022-01-20 06:40:36,953 | FDS disable\n",
      "2022-01-20 06:40:48,662 | Calculate Loss\n",
      "2022-01-20 06:40:48,663 | Update Loss\n",
      "2022-01-20 06:40:48,667 | Backward\n",
      "2022-01-20 06:41:09,751 | Epoch: [3][30/48]\tTime  32.81 ( 32.76)\tData 0.0142 (0.0533)\tLoss (L1) 11.019 (12.506)\n",
      "2022-01-20 06:41:09,766 | ===> Batch : 31\n",
      "2022-01-20 06:41:09,766 | FDS disable\n",
      "2022-01-20 06:41:21,401 | Calculate Loss\n",
      "2022-01-20 06:41:21,403 | Update Loss\n",
      "2022-01-20 06:41:21,406 | Backward\n",
      "2022-01-20 06:41:43,016 | Epoch: [3][31/48]\tTime  33.27 ( 32.77)\tData 0.0152 (0.0520)\tLoss (L1) 11.008 (12.458)\n",
      "2022-01-20 06:41:43,032 | ===> Batch : 32\n",
      "2022-01-20 06:41:43,033 | FDS disable\n",
      "2022-01-20 06:41:54,692 | Calculate Loss\n",
      "2022-01-20 06:41:54,693 | Update Loss\n",
      "2022-01-20 06:41:54,696 | Backward\n",
      "2022-01-20 06:42:15,977 | Epoch: [3][32/48]\tTime  32.96 ( 32.78)\tData 0.0160 (0.0509)\tLoss (L1) 12.329 (12.454)\n",
      "2022-01-20 06:42:15,990 | ===> Batch : 33\n",
      "2022-01-20 06:42:15,991 | FDS disable\n",
      "2022-01-20 06:42:27,571 | Calculate Loss\n",
      "2022-01-20 06:42:27,572 | Update Loss\n",
      "2022-01-20 06:42:27,576 | Backward\n",
      "2022-01-20 06:42:48,149 | Epoch: [3][33/48]\tTime  32.17 ( 32.76)\tData 0.0137 (0.0498)\tLoss (L1) 11.781 (12.433)\n",
      "2022-01-20 06:42:48,163 | ===> Batch : 34\n",
      "2022-01-20 06:42:48,164 | FDS disable\n",
      "2022-01-20 06:42:59,723 | Calculate Loss\n",
      "2022-01-20 06:42:59,724 | Update Loss\n",
      "2022-01-20 06:42:59,728 | Backward\n",
      "2022-01-20 06:43:20,461 | Epoch: [3][34/48]\tTime  32.31 ( 32.75)\tData 0.0148 (0.0488)\tLoss (L1) 13.096 (12.453)\n",
      "2022-01-20 06:43:20,475 | ===> Batch : 35\n",
      "2022-01-20 06:43:20,476 | FDS disable\n",
      "2022-01-20 06:43:32,045 | Calculate Loss\n",
      "2022-01-20 06:43:32,046 | Update Loss\n",
      "2022-01-20 06:43:32,049 | Backward\n",
      "2022-01-20 06:43:53,546 | Epoch: [3][35/48]\tTime  33.08 ( 32.76)\tData 0.0141 (0.0478)\tLoss (L1) 13.356 (12.478)\n",
      "2022-01-20 06:43:53,559 | ===> Batch : 36\n",
      "2022-01-20 06:43:53,560 | FDS disable\n",
      "2022-01-20 06:44:05,138 | Calculate Loss\n",
      "2022-01-20 06:44:05,139 | Update Loss\n",
      "2022-01-20 06:44:05,143 | Backward\n",
      "2022-01-20 06:44:25,759 | Epoch: [3][36/48]\tTime  32.21 ( 32.74)\tData 0.0136 (0.0468)\tLoss (L1) 11.781 (12.459)\n",
      "2022-01-20 06:44:25,773 | ===> Batch : 37\n",
      "2022-01-20 06:44:25,774 | FDS disable\n",
      "2022-01-20 06:44:37,355 | Calculate Loss\n",
      "2022-01-20 06:44:37,356 | Update Loss\n",
      "2022-01-20 06:44:37,359 | Backward\n",
      "2022-01-20 06:44:58,449 | Epoch: [3][37/48]\tTime  32.69 ( 32.74)\tData 0.0141 (0.0459)\tLoss (L1) 13.199 (12.479)\n",
      "2022-01-20 06:44:58,462 | ===> Batch : 38\n",
      "2022-01-20 06:44:58,462 | FDS disable\n",
      "2022-01-20 06:45:10,016 | Calculate Loss\n",
      "2022-01-20 06:45:10,017 | Update Loss\n",
      "2022-01-20 06:45:10,021 | Backward\n",
      "2022-01-20 06:45:30,815 | Epoch: [3][38/48]\tTime  32.37 ( 32.73)\tData 0.0134 (0.0451)\tLoss (L1) 11.052 (12.442)\n",
      "2022-01-20 06:45:30,828 | ===> Batch : 39\n",
      "2022-01-20 06:45:30,829 | FDS disable\n",
      "2022-01-20 06:45:42,401 | Calculate Loss\n",
      "2022-01-20 06:45:42,402 | Update Loss\n",
      "2022-01-20 06:45:42,406 | Backward\n",
      "2022-01-20 06:46:03,641 | Epoch: [3][39/48]\tTime  32.83 ( 32.73)\tData 0.0138 (0.0443)\tLoss (L1) 11.403 (12.415)\n",
      "2022-01-20 06:46:03,656 | ===> Batch : 40\n",
      "2022-01-20 06:46:03,657 | FDS disable\n",
      "2022-01-20 06:46:15,332 | Calculate Loss\n",
      "2022-01-20 06:46:15,334 | Update Loss\n",
      "2022-01-20 06:46:15,338 | Backward\n",
      "2022-01-20 06:46:35,906 | Epoch: [3][40/48]\tTime  32.26 ( 32.72)\tData 0.0152 (0.0435)\tLoss (L1) 11.617 (12.395)\n",
      "2022-01-20 06:46:35,921 | ===> Batch : 41\n",
      "2022-01-20 06:46:35,921 | FDS disable\n",
      "2022-01-20 06:46:47,371 | Calculate Loss\n",
      "2022-01-20 06:46:47,373 | Update Loss\n",
      "2022-01-20 06:46:47,377 | Backward\n",
      "2022-01-20 06:47:08,913 | Epoch: [3][41/48]\tTime  33.01 ( 32.73)\tData 0.0149 (0.0428)\tLoss (L1) 13.529 (12.423)\n",
      "2022-01-20 06:47:08,927 | ===> Batch : 42\n",
      "2022-01-20 06:47:08,927 | FDS disable\n",
      "2022-01-20 06:47:20,476 | Calculate Loss\n",
      "2022-01-20 06:47:20,477 | Update Loss\n",
      "2022-01-20 06:47:20,480 | Backward\n",
      "2022-01-20 06:47:41,063 | Epoch: [3][42/48]\tTime  32.15 ( 32.72)\tData 0.0142 (0.0422)\tLoss (L1) 11.047 (12.390)\n",
      "2022-01-20 06:47:41,077 | ===> Batch : 43\n",
      "2022-01-20 06:47:41,078 | FDS disable\n",
      "2022-01-20 06:47:52,714 | Calculate Loss\n",
      "2022-01-20 06:47:52,715 | Update Loss\n",
      "2022-01-20 06:47:52,718 | Backward\n",
      "2022-01-20 06:48:13,704 | Epoch: [3][43/48]\tTime  32.64 ( 32.71)\tData 0.0140 (0.0415)\tLoss (L1) 13.364 (12.412)\n",
      "2022-01-20 06:48:13,719 | ===> Batch : 44\n",
      "2022-01-20 06:48:13,719 | FDS disable\n",
      "2022-01-20 06:48:25,350 | Calculate Loss\n",
      "2022-01-20 06:48:25,352 | Update Loss\n",
      "2022-01-20 06:48:25,356 | Backward\n",
      "2022-01-20 06:48:46,665 | Epoch: [3][44/48]\tTime  32.96 ( 32.72)\tData 0.0150 (0.0409)\tLoss (L1) 12.736 (12.420)\n",
      "2022-01-20 06:48:46,679 | ===> Batch : 45\n",
      "2022-01-20 06:48:46,680 | FDS disable\n",
      "2022-01-20 06:48:58,234 | Calculate Loss\n",
      "2022-01-20 06:48:58,236 | Update Loss\n",
      "2022-01-20 06:48:58,239 | Backward\n",
      "2022-01-20 06:49:19,199 | Epoch: [3][45/48]\tTime  32.53 ( 32.72)\tData 0.0138 (0.0403)\tLoss (L1) 14.711 (12.471)\n",
      "2022-01-20 06:49:19,213 | ===> Batch : 46\n",
      "2022-01-20 06:49:19,214 | FDS disable\n",
      "2022-01-20 06:49:30,848 | Calculate Loss\n",
      "2022-01-20 06:49:30,849 | Update Loss\n",
      "2022-01-20 06:49:30,853 | Backward\n",
      "2022-01-20 06:49:52,318 | Epoch: [3][46/48]\tTime  33.12 ( 32.72)\tData 0.0149 (0.0398)\tLoss (L1) 10.978 (12.438)\n",
      "2022-01-20 06:49:52,333 | ===> Batch : 47\n",
      "2022-01-20 06:49:52,333 | FDS disable\n",
      "2022-01-20 06:50:03,906 | Calculate Loss\n",
      "2022-01-20 06:50:03,907 | Update Loss\n",
      "2022-01-20 06:50:03,910 | Backward\n",
      "2022-01-20 06:50:25,186 | Epoch: [3][47/48]\tTime  32.87 ( 32.73)\tData 0.0147 (0.0392)\tLoss (L1) 15.510 (12.504)\n",
      "2022-01-20 06:50:25,200 | ===> Batch : 48\n",
      "2022-01-20 06:50:25,200 | FDS disable\n",
      "2022-01-20 06:50:33,289 | Calculate Loss\n",
      "2022-01-20 06:50:33,291 | Update Loss\n",
      "2022-01-20 06:50:33,295 | Backward\n",
      "2022-01-20 06:50:47,296 | Epoch: [3][48/48]\tTime  22.11 ( 32.51)\tData 0.0139 (0.0387)\tLoss (L1) 14.053 (12.526)\n",
      "2022-01-20 06:50:58,895 | Val: [0/9]\tTime 11.433 (11.433)\tLoss (MSE) 449.007 (449.007)\tLoss (L1) 17.068 (17.068)\n",
      "2022-01-20 06:51:07,672 | Val: [1/9]\tTime  8.778 (10.105)\tLoss (MSE) 474.294 (461.650)\tLoss (L1) 17.218 (17.143)\n",
      "2022-01-20 06:51:16,460 | Val: [2/9]\tTime  8.788 ( 9.666)\tLoss (MSE) 595.174 (506.158)\tLoss (L1) 20.209 (18.165)\n",
      "2022-01-20 06:51:25,213 | Val: [3/9]\tTime  8.753 ( 9.438)\tLoss (MSE) 518.157 (509.158)\tLoss (L1) 18.284 (18.195)\n",
      "2022-01-20 06:51:33,976 | Val: [4/9]\tTime  8.763 ( 9.303)\tLoss (MSE) 504.598 (508.246)\tLoss (L1) 18.612 (18.278)\n",
      "2022-01-20 06:51:42,714 | Val: [5/9]\tTime  8.738 ( 9.209)\tLoss (MSE) 575.365 (519.433)\tLoss (L1) 19.598 (18.498)\n",
      "2022-01-20 06:51:51,365 | Val: [6/9]\tTime  8.651 ( 9.129)\tLoss (MSE) 538.313 (522.130)\tLoss (L1) 19.270 (18.609)\n",
      "2022-01-20 06:52:00,135 | Val: [7/9]\tTime  8.770 ( 9.084)\tLoss (MSE) 514.556 (521.183)\tLoss (L1) 18.432 (18.587)\n",
      "2022-01-20 06:52:03,055 | Val: [8/9]\tTime  2.920 ( 8.399)\tLoss (MSE) 549.009 (522.379)\tLoss (L1) 19.383 (18.621)\n",
      "2022-01-20 06:52:03,212 |  * Overall: MSE 522.379\tL1 18.621\tG-Mean 12.632\n",
      "2022-01-20 06:52:03,213 |  * Many: MSE 284.000\tL1 13.702\tG-Mean 9.246\n",
      "2022-01-20 06:52:03,213 |  * Median: MSE 1076.860\tL1 30.510\tG-Mean 27.247\n",
      "2022-01-20 06:52:03,214 |  * Low: MSE 1240.362\tL1 32.200\tG-Mean 28.712\n",
      "2022-01-20 06:52:03,220 | Best L1 Loss: 14.883\n",
      "2022-01-20 06:52:04,470 | Epoch #3: Train loss [12.5260]; Val loss: MSE [522.3793], L1 [18.6207], G-Mean [12.6316]\n",
      "2022-01-20 06:52:04,476 | Training...\n",
      "2022-01-20 06:52:04,477 | Load train loader\n",
      "2022-01-20 06:52:05,742 | ===> Batch : 1\n",
      "2022-01-20 06:52:05,744 | FDS disable\n",
      "2022-01-20 06:52:20,821 | Calculate Loss\n",
      "2022-01-20 06:52:20,823 | Update Loss\n",
      "2022-01-20 06:52:20,955 | Backward\n",
      "2022-01-20 06:52:42,786 | Epoch: [4][ 1/48]\tTime  38.31 ( 38.31)\tData 1.2649 (1.2649)\tLoss (L1) 14.039 (14.039)\n",
      "2022-01-20 06:52:42,799 | ===> Batch : 2\n",
      "2022-01-20 06:52:42,800 | FDS disable\n",
      "2022-01-20 06:52:54,519 | Calculate Loss\n",
      "2022-01-20 06:52:54,520 | Update Loss\n",
      "2022-01-20 06:52:54,524 | Backward\n",
      "2022-01-20 06:53:15,407 | Epoch: [4][ 2/48]\tTime  32.62 ( 35.46)\tData 0.0132 (0.6390)\tLoss (L1) 10.853 (12.446)\n",
      "2022-01-20 06:53:15,420 | ===> Batch : 3\n",
      "2022-01-20 06:53:15,421 | FDS disable\n",
      "2022-01-20 06:53:27,032 | Calculate Loss\n",
      "2022-01-20 06:53:27,033 | Update Loss\n",
      "2022-01-20 06:53:27,037 | Backward\n",
      "2022-01-20 06:53:47,994 | Epoch: [4][ 3/48]\tTime  32.59 ( 34.51)\tData 0.0133 (0.4304)\tLoss (L1) 13.978 (12.957)\n",
      "2022-01-20 06:53:48,007 | ===> Batch : 4\n",
      "2022-01-20 06:53:48,007 | FDS disable\n",
      "2022-01-20 06:53:59,691 | Calculate Loss\n",
      "2022-01-20 06:53:59,693 | Update Loss\n",
      "2022-01-20 06:53:59,696 | Backward\n",
      "2022-01-20 06:54:20,188 | Epoch: [4][ 4/48]\tTime  32.19 ( 33.93)\tData 0.0132 (0.3261)\tLoss (L1) 13.419 (13.072)\n",
      "2022-01-20 06:54:20,201 | ===> Batch : 5\n",
      "2022-01-20 06:54:20,201 | FDS disable\n",
      "2022-01-20 06:54:31,821 | Calculate Loss\n",
      "2022-01-20 06:54:31,822 | Update Loss\n",
      "2022-01-20 06:54:31,825 | Backward\n",
      "2022-01-20 06:54:52,726 | Epoch: [4][ 5/48]\tTime  32.54 ( 33.65)\tData 0.0129 (0.2635)\tLoss (L1) 14.845 (13.427)\n",
      "2022-01-20 06:54:52,739 | ===> Batch : 6\n",
      "2022-01-20 06:54:52,739 | FDS disable\n",
      "2022-01-20 06:55:04,373 | Calculate Loss\n",
      "2022-01-20 06:55:04,374 | Update Loss\n",
      "2022-01-20 06:55:04,378 | Backward\n",
      "2022-01-20 06:55:25,157 | Epoch: [4][ 6/48]\tTime  32.43 ( 33.45)\tData 0.0128 (0.2217)\tLoss (L1) 10.992 (13.021)\n",
      "2022-01-20 06:55:25,170 | ===> Batch : 7\n",
      "2022-01-20 06:55:25,170 | FDS disable\n",
      "2022-01-20 06:55:36,865 | Calculate Loss\n",
      "2022-01-20 06:55:36,868 | Update Loss\n",
      "2022-01-20 06:55:36,872 | Backward\n",
      "2022-01-20 06:55:57,737 | Epoch: [4][ 7/48]\tTime  32.58 ( 33.32)\tData 0.0130 (0.1919)\tLoss (L1) 12.456 (12.940)\n",
      "2022-01-20 06:55:57,750 | ===> Batch : 8\n",
      "2022-01-20 06:55:57,751 | FDS disable\n",
      "2022-01-20 06:56:09,461 | Calculate Loss\n",
      "2022-01-20 06:56:09,462 | Update Loss\n",
      "2022-01-20 06:56:09,466 | Backward\n",
      "2022-01-20 06:56:30,084 | Epoch: [4][ 8/48]\tTime  32.35 ( 33.20)\tData 0.0130 (0.1695)\tLoss (L1) 10.996 (12.697)\n",
      "2022-01-20 06:56:30,103 | ===> Batch : 9\n",
      "2022-01-20 06:56:30,104 | FDS disable\n",
      "2022-01-20 06:56:41,750 | Calculate Loss\n",
      "2022-01-20 06:56:41,752 | Update Loss\n",
      "2022-01-20 06:56:41,755 | Backward\n",
      "2022-01-20 06:57:02,666 | Epoch: [4][ 9/48]\tTime  32.58 ( 33.13)\tData 0.0193 (0.1528)\tLoss (L1) 11.641 (12.580)\n",
      "2022-01-20 06:57:02,680 | ===> Batch : 10\n",
      "2022-01-20 06:57:02,681 | FDS disable\n",
      "2022-01-20 06:57:14,331 | Calculate Loss\n",
      "2022-01-20 06:57:14,332 | Update Loss\n",
      "2022-01-20 06:57:14,336 | Backward\n",
      "2022-01-20 06:57:35,205 | Epoch: [4][10/48]\tTime  32.54 ( 33.07)\tData 0.0137 (0.1389)\tLoss (L1) 12.394 (12.561)\n",
      "2022-01-20 06:57:35,217 | ===> Batch : 11\n",
      "2022-01-20 06:57:35,218 | FDS disable\n",
      "2022-01-20 06:57:46,869 | Calculate Loss\n",
      "2022-01-20 06:57:46,870 | Update Loss\n",
      "2022-01-20 06:57:46,873 | Backward\n",
      "2022-01-20 06:58:07,258 | Epoch: [4][11/48]\tTime  32.05 ( 32.98)\tData 0.0125 (0.1274)\tLoss (L1) 11.311 (12.448)\n",
      "2022-01-20 06:58:07,272 | ===> Batch : 12\n",
      "2022-01-20 06:58:07,273 | FDS disable\n",
      "2022-01-20 06:58:19,041 | Calculate Loss\n",
      "2022-01-20 06:58:19,043 | Update Loss\n",
      "2022-01-20 06:58:19,046 | Backward\n",
      "2022-01-20 06:58:40,365 | Epoch: [4][12/48]\tTime  33.11 ( 32.99)\tData 0.0140 (0.1180)\tLoss (L1) 12.547 (12.456)\n",
      "2022-01-20 06:58:40,381 | ===> Batch : 13\n",
      "2022-01-20 06:58:40,381 | FDS disable\n",
      "2022-01-20 06:58:52,144 | Calculate Loss\n",
      "2022-01-20 06:58:52,146 | Update Loss\n",
      "2022-01-20 06:58:52,149 | Backward\n",
      "2022-01-20 06:59:12,490 | Epoch: [4][13/48]\tTime  32.12 ( 32.92)\tData 0.0159 (0.1101)\tLoss (L1) 10.947 (12.340)\n",
      "2022-01-20 06:59:12,502 | ===> Batch : 14\n",
      "2022-01-20 06:59:12,503 | FDS disable\n",
      "2022-01-20 06:59:24,167 | Calculate Loss\n",
      "2022-01-20 06:59:24,168 | Update Loss\n",
      "2022-01-20 06:59:24,172 | Backward\n",
      "2022-01-20 06:59:45,423 | Epoch: [4][14/48]\tTime  32.93 ( 32.92)\tData 0.0128 (0.1032)\tLoss (L1) 10.847 (12.233)\n",
      "2022-01-20 06:59:45,435 | ===> Batch : 15\n",
      "2022-01-20 06:59:45,436 | FDS disable\n",
      "2022-01-20 06:59:57,196 | Calculate Loss\n",
      "2022-01-20 06:59:57,198 | Update Loss\n",
      "2022-01-20 06:59:57,201 | Backward\n",
      "2022-01-20 07:00:17,811 | Epoch: [4][15/48]\tTime  32.39 ( 32.89)\tData 0.0125 (0.0971)\tLoss (L1) 13.253 (12.301)\n",
      "2022-01-20 07:00:17,824 | ===> Batch : 16\n",
      "2022-01-20 07:00:17,824 | FDS disable\n",
      "2022-01-20 07:00:29,509 | Calculate Loss\n",
      "2022-01-20 07:00:29,510 | Update Loss\n",
      "2022-01-20 07:00:29,514 | Backward\n",
      "2022-01-20 07:00:50,463 | Epoch: [4][16/48]\tTime  32.65 ( 32.87)\tData 0.0127 (0.0919)\tLoss (L1) 12.978 (12.344)\n",
      "2022-01-20 07:00:50,477 | ===> Batch : 17\n",
      "2022-01-20 07:00:50,478 | FDS disable\n",
      "2022-01-20 07:01:02,209 | Calculate Loss\n",
      "2022-01-20 07:01:02,211 | Update Loss\n",
      "2022-01-20 07:01:02,215 | Backward\n",
      "2022-01-20 07:01:23,239 | Epoch: [4][17/48]\tTime  32.78 ( 32.87)\tData 0.0145 (0.0873)\tLoss (L1) 13.256 (12.397)\n",
      "2022-01-20 07:01:23,254 | ===> Batch : 18\n",
      "2022-01-20 07:01:23,255 | FDS disable\n",
      "2022-01-20 07:01:34,957 | Calculate Loss\n",
      "2022-01-20 07:01:34,960 | Update Loss\n",
      "2022-01-20 07:01:34,963 | Backward\n",
      "2022-01-20 07:01:55,855 | Epoch: [4][18/48]\tTime  32.62 ( 32.85)\tData 0.0148 (0.0833)\tLoss (L1) 13.479 (12.457)\n",
      "2022-01-20 07:01:55,869 | ===> Batch : 19\n",
      "2022-01-20 07:01:55,870 | FDS disable\n",
      "2022-01-20 07:02:07,501 | Calculate Loss\n",
      "2022-01-20 07:02:07,502 | Update Loss\n",
      "2022-01-20 07:02:07,506 | Backward\n",
      "2022-01-20 07:02:28,798 | Epoch: [4][19/48]\tTime  32.94 ( 32.86)\tData 0.0136 (0.0796)\tLoss (L1) 11.558 (12.410)\n",
      "2022-01-20 07:02:28,813 | ===> Batch : 20\n",
      "2022-01-20 07:02:28,813 | FDS disable\n",
      "2022-01-20 07:02:40,498 | Calculate Loss\n",
      "2022-01-20 07:02:40,499 | Update Loss\n",
      "2022-01-20 07:02:40,503 | Backward\n",
      "2022-01-20 07:03:01,506 | Epoch: [4][20/48]\tTime  32.71 ( 32.85)\tData 0.0142 (0.0763)\tLoss (L1) 10.606 (12.320)\n",
      "2022-01-20 07:03:01,520 | ===> Batch : 21\n",
      "2022-01-20 07:03:01,520 | FDS disable\n",
      "2022-01-20 07:03:13,166 | Calculate Loss\n",
      "2022-01-20 07:03:13,168 | Update Loss\n",
      "2022-01-20 07:03:13,171 | Backward\n",
      "2022-01-20 07:03:34,111 | Epoch: [4][21/48]\tTime  32.60 ( 32.84)\tData 0.0137 (0.0734)\tLoss (L1) 12.963 (12.351)\n",
      "2022-01-20 07:03:34,125 | ===> Batch : 22\n",
      "2022-01-20 07:03:34,126 | FDS disable\n",
      "2022-01-20 07:03:45,877 | Calculate Loss\n",
      "2022-01-20 07:03:45,879 | Update Loss\n",
      "2022-01-20 07:03:45,883 | Backward\n",
      "2022-01-20 07:04:07,126 | Epoch: [4][22/48]\tTime  33.02 ( 32.85)\tData 0.0145 (0.0707)\tLoss (L1) 11.432 (12.309)\n",
      "2022-01-20 07:04:07,140 | ===> Batch : 23\n",
      "2022-01-20 07:04:07,141 | FDS disable\n",
      "2022-01-20 07:04:18,882 | Calculate Loss\n",
      "2022-01-20 07:04:18,883 | Update Loss\n",
      "2022-01-20 07:04:18,886 | Backward\n",
      "2022-01-20 07:04:39,714 | Epoch: [4][23/48]\tTime  32.59 ( 32.84)\tData 0.0147 (0.0682)\tLoss (L1) 12.577 (12.320)\n",
      "2022-01-20 07:04:39,728 | ===> Batch : 24\n",
      "2022-01-20 07:04:39,729 | FDS disable\n",
      "2022-01-20 07:04:51,401 | Calculate Loss\n",
      "2022-01-20 07:04:51,402 | Update Loss\n",
      "2022-01-20 07:04:51,406 | Backward\n",
      "2022-01-20 07:05:12,372 | Epoch: [4][24/48]\tTime  32.66 ( 32.83)\tData 0.0139 (0.0660)\tLoss (L1) 12.119 (12.312)\n",
      "2022-01-20 07:05:12,387 | ===> Batch : 25\n",
      "2022-01-20 07:05:12,388 | FDS disable\n",
      "2022-01-20 07:05:24,075 | Calculate Loss\n",
      "2022-01-20 07:05:24,076 | Update Loss\n",
      "2022-01-20 07:05:24,080 | Backward\n",
      "2022-01-20 07:05:45,271 | Epoch: [4][25/48]\tTime  32.90 ( 32.83)\tData 0.0152 (0.0639)\tLoss (L1) 11.583 (12.283)\n",
      "2022-01-20 07:05:45,285 | ===> Batch : 26\n",
      "2022-01-20 07:05:45,286 | FDS disable\n",
      "2022-01-20 07:05:57,030 | Calculate Loss\n",
      "2022-01-20 07:05:57,031 | Update Loss\n",
      "2022-01-20 07:05:57,035 | Backward\n",
      "2022-01-20 07:06:18,240 | Epoch: [4][26/48]\tTime  32.97 ( 32.84)\tData 0.0142 (0.0620)\tLoss (L1) 11.357 (12.247)\n",
      "2022-01-20 07:06:18,255 | ===> Batch : 27\n",
      "2022-01-20 07:06:18,255 | FDS disable\n",
      "2022-01-20 07:06:29,934 | Calculate Loss\n",
      "2022-01-20 07:06:29,935 | Update Loss\n",
      "2022-01-20 07:06:29,939 | Backward\n",
      "2022-01-20 07:06:50,580 | Epoch: [4][27/48]\tTime  32.34 ( 32.82)\tData 0.0145 (0.0603)\tLoss (L1) 12.238 (12.247)\n",
      "2022-01-20 07:06:50,595 | ===> Batch : 28\n",
      "2022-01-20 07:06:50,596 | FDS disable\n",
      "2022-01-20 07:07:02,283 | Calculate Loss\n",
      "2022-01-20 07:07:02,284 | Update Loss\n",
      "2022-01-20 07:07:02,288 | Backward\n",
      "2022-01-20 07:07:22,750 | Epoch: [4][28/48]\tTime  32.17 ( 32.80)\tData 0.0155 (0.0587)\tLoss (L1) 10.624 (12.189)\n",
      "2022-01-20 07:07:22,764 | ===> Batch : 29\n",
      "2022-01-20 07:07:22,765 | FDS disable\n",
      "2022-01-20 07:07:34,473 | Calculate Loss\n",
      "2022-01-20 07:07:34,476 | Update Loss\n",
      "2022-01-20 07:07:34,479 | Backward\n",
      "2022-01-20 07:07:55,990 | Epoch: [4][29/48]\tTime  33.24 ( 32.81)\tData 0.0143 (0.0571)\tLoss (L1) 13.319 (12.228)\n",
      "2022-01-20 07:07:56,005 | ===> Batch : 30\n",
      "2022-01-20 07:07:56,006 | FDS disable\n",
      "2022-01-20 07:08:07,668 | Calculate Loss\n",
      "2022-01-20 07:08:07,669 | Update Loss\n",
      "2022-01-20 07:08:07,672 | Backward\n",
      "2022-01-20 07:08:28,231 | Epoch: [4][30/48]\tTime  32.24 ( 32.79)\tData 0.0147 (0.0557)\tLoss (L1) 11.950 (12.219)\n",
      "2022-01-20 07:08:28,247 | ===> Batch : 31\n",
      "2022-01-20 07:08:28,248 | FDS disable\n",
      "2022-01-20 07:08:39,863 | Calculate Loss\n",
      "2022-01-20 07:08:39,864 | Update Loss\n",
      "2022-01-20 07:08:39,868 | Backward\n",
      "2022-01-20 07:09:01,038 | Epoch: [4][31/48]\tTime  32.81 ( 32.79)\tData 0.0158 (0.0544)\tLoss (L1) 14.777 (12.301)\n",
      "2022-01-20 07:09:01,052 | ===> Batch : 32\n",
      "2022-01-20 07:09:01,052 | FDS disable\n",
      "2022-01-20 07:09:12,663 | Calculate Loss\n",
      "2022-01-20 07:09:12,665 | Update Loss\n",
      "2022-01-20 07:09:12,669 | Backward\n",
      "2022-01-20 07:09:33,981 | Epoch: [4][32/48]\tTime  32.94 ( 32.80)\tData 0.0138 (0.0532)\tLoss (L1) 12.540 (12.309)\n",
      "2022-01-20 07:09:33,995 | ===> Batch : 33\n",
      "2022-01-20 07:09:33,995 | FDS disable\n",
      "2022-01-20 07:09:45,520 | Calculate Loss\n",
      "2022-01-20 07:09:45,521 | Update Loss\n",
      "2022-01-20 07:09:45,525 | Backward\n",
      "2022-01-20 07:10:06,414 | Epoch: [4][33/48]\tTime  32.43 ( 32.79)\tData 0.0139 (0.0520)\tLoss (L1) 11.193 (12.275)\n",
      "2022-01-20 07:10:06,428 | ===> Batch : 34\n",
      "2022-01-20 07:10:06,429 | FDS disable\n",
      "2022-01-20 07:10:18,020 | Calculate Loss\n",
      "2022-01-20 07:10:18,022 | Update Loss\n",
      "2022-01-20 07:10:18,025 | Backward\n",
      "2022-01-20 07:10:39,235 | Epoch: [4][34/48]\tTime  32.82 ( 32.79)\tData 0.0140 (0.0509)\tLoss (L1) 10.032 (12.209)\n",
      "2022-01-20 07:10:39,250 | ===> Batch : 35\n",
      "2022-01-20 07:10:39,250 | FDS disable\n",
      "2022-01-20 07:10:50,810 | Calculate Loss\n",
      "2022-01-20 07:10:50,812 | Update Loss\n",
      "2022-01-20 07:10:50,815 | Backward\n",
      "2022-01-20 07:11:11,551 | Epoch: [4][35/48]\tTime  32.32 ( 32.77)\tData 0.0146 (0.0498)\tLoss (L1) 13.430 (12.244)\n",
      "2022-01-20 07:11:11,565 | ===> Batch : 36\n",
      "2022-01-20 07:11:11,565 | FDS disable\n",
      "2022-01-20 07:11:23,121 | Calculate Loss\n",
      "2022-01-20 07:11:23,122 | Update Loss\n",
      "2022-01-20 07:11:23,126 | Backward\n",
      "2022-01-20 07:11:44,186 | Epoch: [4][36/48]\tTime  32.64 ( 32.77)\tData 0.0135 (0.0488)\tLoss (L1) 12.334 (12.246)\n",
      "2022-01-20 07:11:44,200 | ===> Batch : 37\n",
      "2022-01-20 07:11:44,201 | FDS disable\n",
      "2022-01-20 07:11:55,757 | Calculate Loss\n",
      "2022-01-20 07:11:55,758 | Update Loss\n",
      "2022-01-20 07:11:55,761 | Backward\n",
      "2022-01-20 07:12:16,567 | Epoch: [4][37/48]\tTime  32.38 ( 32.76)\tData 0.0137 (0.0479)\tLoss (L1) 11.949 (12.238)\n",
      "2022-01-20 07:12:16,581 | ===> Batch : 38\n",
      "2022-01-20 07:12:16,582 | FDS disable\n",
      "2022-01-20 07:12:28,113 | Calculate Loss\n",
      "2022-01-20 07:12:28,115 | Update Loss\n",
      "2022-01-20 07:12:28,119 | Backward\n",
      "2022-01-20 07:12:49,195 | Epoch: [4][38/48]\tTime  32.63 ( 32.76)\tData 0.0140 (0.0470)\tLoss (L1) 10.754 (12.199)\n",
      "2022-01-20 07:12:49,208 | ===> Batch : 39\n",
      "2022-01-20 07:12:49,209 | FDS disable\n",
      "2022-01-20 07:13:00,798 | Calculate Loss\n",
      "2022-01-20 07:13:00,800 | Update Loss\n",
      "2022-01-20 07:13:00,803 | Backward\n",
      "2022-01-20 07:13:22,170 | Epoch: [4][39/48]\tTime  32.98 ( 32.76)\tData 0.0138 (0.0461)\tLoss (L1) 12.424 (12.205)\n",
      "2022-01-20 07:13:22,186 | ===> Batch : 40\n",
      "2022-01-20 07:13:22,187 | FDS disable\n",
      "2022-01-20 07:13:33,737 | Calculate Loss\n",
      "2022-01-20 07:13:33,739 | Update Loss\n",
      "2022-01-20 07:13:33,743 | Backward\n",
      "2022-01-20 07:13:55,240 | Epoch: [4][40/48]\tTime  33.07 ( 32.77)\tData 0.0161 (0.0454)\tLoss (L1) 11.354 (12.184)\n",
      "2022-01-20 07:13:55,253 | ===> Batch : 41\n",
      "2022-01-20 07:13:55,254 | FDS disable\n",
      "2022-01-20 07:14:06,848 | Calculate Loss\n",
      "2022-01-20 07:14:06,849 | Update Loss\n",
      "2022-01-20 07:14:06,852 | Backward\n",
      "2022-01-20 07:14:27,026 | Epoch: [4][41/48]\tTime  31.79 ( 32.75)\tData 0.0135 (0.0446)\tLoss (L1) 10.735 (12.148)\n",
      "2022-01-20 07:14:27,040 | ===> Batch : 42\n",
      "2022-01-20 07:14:27,040 | FDS disable\n",
      "2022-01-20 07:14:38,559 | Calculate Loss\n",
      "2022-01-20 07:14:38,561 | Update Loss\n",
      "2022-01-20 07:14:38,565 | Backward\n",
      "2022-01-20 07:14:59,546 | Epoch: [4][42/48]\tTime  32.52 ( 32.74)\tData 0.0135 (0.0439)\tLoss (L1) 12.725 (12.162)\n",
      "2022-01-20 07:14:59,560 | ===> Batch : 43\n",
      "2022-01-20 07:14:59,560 | FDS disable\n",
      "2022-01-20 07:15:11,158 | Calculate Loss\n",
      "2022-01-20 07:15:11,160 | Update Loss\n",
      "2022-01-20 07:15:11,164 | Backward\n",
      "2022-01-20 07:15:32,062 | Epoch: [4][43/48]\tTime  32.52 ( 32.73)\tData 0.0134 (0.0432)\tLoss (L1) 10.818 (12.131)\n",
      "2022-01-20 07:15:32,078 | ===> Batch : 44\n",
      "2022-01-20 07:15:32,078 | FDS disable\n",
      "2022-01-20 07:15:43,613 | Calculate Loss\n",
      "2022-01-20 07:15:43,615 | Update Loss\n",
      "2022-01-20 07:15:43,619 | Backward\n",
      "2022-01-20 07:16:04,685 | Epoch: [4][44/48]\tTime  32.62 ( 32.73)\tData 0.0157 (0.0425)\tLoss (L1) 11.765 (12.123)\n",
      "2022-01-20 07:16:04,700 | ===> Batch : 45\n",
      "2022-01-20 07:16:04,700 | FDS disable\n",
      "2022-01-20 07:16:16,351 | Calculate Loss\n",
      "2022-01-20 07:16:16,353 | Update Loss\n",
      "2022-01-20 07:16:16,356 | Backward\n",
      "2022-01-20 07:16:37,280 | Epoch: [4][45/48]\tTime  32.60 ( 32.73)\tData 0.0149 (0.0419)\tLoss (L1) 10.415 (12.085)\n",
      "2022-01-20 07:16:37,295 | ===> Batch : 46\n",
      "2022-01-20 07:16:37,295 | FDS disable\n",
      "2022-01-20 07:16:48,864 | Calculate Loss\n",
      "2022-01-20 07:16:48,865 | Update Loss\n",
      "2022-01-20 07:16:48,869 | Backward\n",
      "2022-01-20 07:17:09,859 | Epoch: [4][46/48]\tTime  32.58 ( 32.73)\tData 0.0143 (0.0413)\tLoss (L1) 11.852 (12.079)\n",
      "2022-01-20 07:17:09,874 | ===> Batch : 47\n",
      "2022-01-20 07:17:09,874 | FDS disable\n",
      "2022-01-20 07:17:21,450 | Calculate Loss\n",
      "2022-01-20 07:17:21,451 | Update Loss\n",
      "2022-01-20 07:17:21,455 | Backward\n",
      "2022-01-20 07:17:42,325 | Epoch: [4][47/48]\tTime  32.47 ( 32.72)\tData 0.0149 (0.0407)\tLoss (L1) 11.126 (12.059)\n",
      "2022-01-20 07:17:42,339 | ===> Batch : 48\n",
      "2022-01-20 07:17:42,340 | FDS disable\n",
      "2022-01-20 07:17:50,439 | Calculate Loss\n",
      "2022-01-20 07:17:50,440 | Update Loss\n",
      "2022-01-20 07:17:50,444 | Backward\n",
      "2022-01-20 07:18:04,576 | Epoch: [4][48/48]\tTime  22.25 ( 32.50)\tData 0.0137 (0.0402)\tLoss (L1) 12.311 (12.063)\n",
      "2022-01-20 07:18:16,291 | Val: [0/9]\tTime 11.541 (11.541)\tLoss (MSE) 267.899 (267.899)\tLoss (L1) 13.395 (13.395)\n",
      "2022-01-20 07:18:25,021 | Val: [1/9]\tTime  8.731 (10.136)\tLoss (MSE) 264.964 (266.432)\tLoss (L1) 13.318 (13.356)\n",
      "2022-01-20 07:18:33,698 | Val: [2/9]\tTime  8.676 ( 9.649)\tLoss (MSE) 281.747 (271.537)\tLoss (L1) 14.088 (13.600)\n",
      "2022-01-20 07:18:42,455 | Val: [3/9]\tTime  8.757 ( 9.426)\tLoss (MSE) 269.196 (270.952)\tLoss (L1) 13.359 (13.540)\n",
      "2022-01-20 07:18:51,133 | Val: [4/9]\tTime  8.678 ( 9.277)\tLoss (MSE) 274.253 (271.612)\tLoss (L1) 13.757 (13.584)\n",
      "2022-01-20 07:18:59,825 | Val: [5/9]\tTime  8.692 ( 9.179)\tLoss (MSE) 293.066 (275.188)\tLoss (L1) 13.992 (13.652)\n",
      "2022-01-20 07:19:08,564 | Val: [6/9]\tTime  8.739 ( 9.116)\tLoss (MSE) 265.195 (273.760)\tLoss (L1) 13.359 (13.610)\n",
      "2022-01-20 07:19:17,372 | Val: [7/9]\tTime  8.808 ( 9.078)\tLoss (MSE) 291.432 (275.969)\tLoss (L1) 13.756 (13.628)\n",
      "2022-01-20 07:19:20,354 | Val: [8/9]\tTime  2.982 ( 8.400)\tLoss (MSE) 257.610 (275.180)\tLoss (L1) 13.112 (13.606)\n",
      "2022-01-20 07:19:20,503 |  * Overall: MSE 275.180\tL1 13.606\tG-Mean 9.472\n",
      "2022-01-20 07:19:20,504 |  * Many: MSE 159.581\tL1 10.379\tG-Mean 7.255\n",
      "2022-01-20 07:19:20,504 |  * Median: MSE 509.422\tL1 20.360\tG-Mean 16.888\n",
      "2022-01-20 07:19:20,505 |  * Low: MSE 719.162\tL1 25.401\tG-Mean 23.747\n",
      "2022-01-20 07:19:20,510 | Best L1 Loss: 13.606\n",
      "2022-01-20 07:19:21,773 | ===> Saving current best checkpoint...\n",
      "2022-01-20 07:19:22,909 | Epoch #4: Train loss [12.0628]; Val loss: MSE [275.1798], L1 [13.6058], G-Mean [9.4719]\n",
      "2022-01-20 07:19:22,910 | Training...\n",
      "2022-01-20 07:19:22,911 | Load train loader\n",
      "2022-01-20 07:19:24,141 | ===> Batch : 1\n",
      "2022-01-20 07:19:24,143 | FDS disable\n",
      "2022-01-20 07:19:38,772 | Calculate Loss\n",
      "2022-01-20 07:19:38,774 | Update Loss\n",
      "2022-01-20 07:19:38,905 | Backward\n",
      "2022-01-20 07:20:00,965 | Epoch: [5][ 1/48]\tTime  38.05 ( 38.05)\tData 1.2302 (1.2302)\tLoss (L1) 11.272 (11.272)\n",
      "2022-01-20 07:20:00,978 | ===> Batch : 2\n",
      "2022-01-20 07:20:00,979 | FDS disable\n",
      "2022-01-20 07:20:12,591 | Calculate Loss\n",
      "2022-01-20 07:20:12,592 | Update Loss\n",
      "2022-01-20 07:20:12,596 | Backward\n",
      "2022-01-20 07:20:33,448 | Epoch: [5][ 2/48]\tTime  32.48 ( 35.27)\tData 0.0132 (0.6217)\tLoss (L1) 11.908 (11.590)\n",
      "2022-01-20 07:20:33,464 | ===> Batch : 3\n",
      "2022-01-20 07:20:33,464 | FDS disable\n",
      "2022-01-20 07:20:45,248 | Calculate Loss\n",
      "2022-01-20 07:20:45,250 | Update Loss\n",
      "2022-01-20 07:20:45,253 | Backward\n",
      "2022-01-20 07:21:06,075 | Epoch: [5][ 3/48]\tTime  32.63 ( 34.39)\tData 0.0156 (0.4197)\tLoss (L1) 12.364 (11.848)\n",
      "2022-01-20 07:21:06,089 | ===> Batch : 4\n",
      "2022-01-20 07:21:06,089 | FDS disable\n",
      "2022-01-20 07:21:17,901 | Calculate Loss\n",
      "2022-01-20 07:21:17,903 | Update Loss\n",
      "2022-01-20 07:21:17,906 | Backward\n",
      "2022-01-20 07:21:38,783 | Epoch: [5][ 4/48]\tTime  32.71 ( 33.97)\tData 0.0132 (0.3181)\tLoss (L1) 12.533 (12.019)\n",
      "2022-01-20 07:21:38,796 | ===> Batch : 5\n",
      "2022-01-20 07:21:38,797 | FDS disable\n",
      "2022-01-20 07:21:50,502 | Calculate Loss\n",
      "2022-01-20 07:21:50,504 | Update Loss\n",
      "2022-01-20 07:21:50,508 | Backward\n",
      "2022-01-20 07:22:11,511 | Epoch: [5][ 5/48]\tTime  32.73 ( 33.72)\tData 0.0131 (0.2571)\tLoss (L1) 12.602 (12.135)\n",
      "2022-01-20 07:22:11,526 | ===> Batch : 6\n",
      "2022-01-20 07:22:11,526 | FDS disable\n",
      "2022-01-20 07:22:23,229 | Calculate Loss\n",
      "2022-01-20 07:22:23,230 | Update Loss\n",
      "2022-01-20 07:22:23,233 | Backward\n",
      "2022-01-20 07:22:43,877 | Epoch: [5][ 6/48]\tTime  32.37 ( 33.49)\tData 0.0146 (0.2167)\tLoss (L1) 13.414 (12.349)\n",
      "2022-01-20 07:22:43,890 | ===> Batch : 7\n",
      "2022-01-20 07:22:43,891 | FDS disable\n",
      "2022-01-20 07:22:55,634 | Calculate Loss\n",
      "2022-01-20 07:22:55,635 | Update Loss\n",
      "2022-01-20 07:22:55,639 | Backward\n",
      "2022-01-20 07:23:17,099 | Epoch: [5][ 7/48]\tTime  33.22 ( 33.46)\tData 0.0130 (0.1876)\tLoss (L1) 11.195 (12.184)\n",
      "2022-01-20 07:23:17,112 | ===> Batch : 8\n",
      "2022-01-20 07:23:17,113 | FDS disable\n",
      "2022-01-20 07:23:28,863 | Calculate Loss\n",
      "2022-01-20 07:23:28,865 | Update Loss\n",
      "2022-01-20 07:23:28,868 | Backward\n",
      "2022-01-20 07:23:49,515 | Epoch: [5][ 8/48]\tTime  32.42 ( 33.33)\tData 0.0133 (0.1658)\tLoss (L1) 10.250 (11.942)\n",
      "2022-01-20 07:23:49,536 | ===> Batch : 9\n",
      "2022-01-20 07:23:49,537 | FDS disable\n",
      "2022-01-20 07:24:01,271 | Calculate Loss\n",
      "2022-01-20 07:24:01,272 | Update Loss\n",
      "2022-01-20 07:24:01,276 | Backward\n",
      "2022-01-20 07:24:22,318 | Epoch: [5][ 9/48]\tTime  32.80 ( 33.27)\tData 0.0218 (0.1498)\tLoss (L1) 11.268 (11.867)\n",
      "2022-01-20 07:24:22,331 | ===> Batch : 10\n",
      "2022-01-20 07:24:22,332 | FDS disable\n",
      "2022-01-20 07:24:34,106 | Calculate Loss\n",
      "2022-01-20 07:24:34,107 | Update Loss\n",
      "2022-01-20 07:24:34,111 | Backward\n",
      "2022-01-20 07:24:54,673 | Epoch: [5][10/48]\tTime  32.35 ( 33.18)\tData 0.0127 (0.1361)\tLoss (L1) 12.301 (11.911)\n",
      "2022-01-20 07:24:54,686 | ===> Batch : 11\n",
      "2022-01-20 07:24:54,686 | FDS disable\n",
      "2022-01-20 07:25:06,359 | Calculate Loss\n",
      "2022-01-20 07:25:06,361 | Update Loss\n",
      "2022-01-20 07:25:06,364 | Backward\n",
      "2022-01-20 07:25:27,559 | Epoch: [5][11/48]\tTime  32.89 ( 33.15)\tData 0.0126 (0.1248)\tLoss (L1) 11.620 (11.884)\n",
      "2022-01-20 07:25:27,571 | ===> Batch : 12\n",
      "2022-01-20 07:25:27,572 | FDS disable\n",
      "2022-01-20 07:25:39,259 | Calculate Loss\n",
      "2022-01-20 07:25:39,260 | Update Loss\n",
      "2022-01-20 07:25:39,264 | Backward\n",
      "2022-01-20 07:26:00,494 | Epoch: [5][12/48]\tTime  32.94 ( 33.13)\tData 0.0126 (0.1155)\tLoss (L1) 11.427 (11.846)\n",
      "2022-01-20 07:26:00,508 | ===> Batch : 13\n",
      "2022-01-20 07:26:00,508 | FDS disable\n",
      "2022-01-20 07:26:12,248 | Calculate Loss\n",
      "2022-01-20 07:26:12,249 | Update Loss\n",
      "2022-01-20 07:26:12,253 | Backward\n",
      "2022-01-20 07:26:33,561 | Epoch: [5][13/48]\tTime  33.07 ( 33.13)\tData 0.0134 (0.1076)\tLoss (L1) 11.449 (11.816)\n",
      "2022-01-20 07:26:33,573 | ===> Batch : 14\n",
      "2022-01-20 07:26:33,574 | FDS disable\n",
      "2022-01-20 07:26:45,312 | Calculate Loss\n",
      "2022-01-20 07:26:45,314 | Update Loss\n",
      "2022-01-20 07:26:45,318 | Backward\n",
      "2022-01-20 07:27:06,211 | Epoch: [5][14/48]\tTime  32.65 ( 33.09)\tData 0.0127 (0.1009)\tLoss (L1) 13.857 (11.961)\n",
      "2022-01-20 07:27:06,225 | ===> Batch : 15\n",
      "2022-01-20 07:27:06,225 | FDS disable\n",
      "2022-01-20 07:27:17,910 | Calculate Loss\n",
      "2022-01-20 07:27:17,911 | Update Loss\n",
      "2022-01-20 07:27:17,914 | Backward\n",
      "2022-01-20 07:27:38,888 | Epoch: [5][15/48]\tTime  32.68 ( 33.07)\tData 0.0131 (0.0950)\tLoss (L1) 10.472 (11.862)\n",
      "2022-01-20 07:27:38,901 | ===> Batch : 16\n",
      "2022-01-20 07:27:38,902 | FDS disable\n",
      "2022-01-20 07:27:50,602 | Calculate Loss\n",
      "2022-01-20 07:27:50,604 | Update Loss\n",
      "2022-01-20 07:27:50,607 | Backward\n",
      "2022-01-20 07:28:10,692 | Epoch: [5][16/48]\tTime  31.80 ( 32.99)\tData 0.0137 (0.0899)\tLoss (L1) 11.329 (11.829)\n",
      "2022-01-20 07:28:10,706 | ===> Batch : 17\n",
      "2022-01-20 07:28:10,707 | FDS disable\n",
      "2022-01-20 07:28:22,392 | Calculate Loss\n",
      "2022-01-20 07:28:22,394 | Update Loss\n",
      "2022-01-20 07:28:22,398 | Backward\n",
      "2022-01-20 07:28:43,708 | Epoch: [5][17/48]\tTime  33.02 ( 32.99)\tData 0.0141 (0.0855)\tLoss (L1) 11.902 (11.833)\n",
      "2022-01-20 07:28:43,727 | ===> Batch : 18\n",
      "2022-01-20 07:28:43,728 | FDS disable\n",
      "2022-01-20 07:28:55,390 | Calculate Loss\n",
      "2022-01-20 07:28:55,391 | Update Loss\n",
      "2022-01-20 07:28:55,395 | Backward\n",
      "2022-01-20 07:29:16,788 | Epoch: [5][18/48]\tTime  33.08 ( 32.99)\tData 0.0198 (0.0818)\tLoss (L1) 10.790 (11.775)\n",
      "2022-01-20 07:29:16,802 | ===> Batch : 19\n",
      "2022-01-20 07:29:16,802 | FDS disable\n",
      "2022-01-20 07:29:28,491 | Calculate Loss\n",
      "2022-01-20 07:29:28,492 | Update Loss\n",
      "2022-01-20 07:29:28,496 | Backward\n",
      "2022-01-20 07:29:49,239 | Epoch: [5][19/48]\tTime  32.45 ( 32.96)\tData 0.0140 (0.0783)\tLoss (L1) 11.724 (11.772)\n",
      "2022-01-20 07:29:49,254 | ===> Batch : 20\n",
      "2022-01-20 07:29:49,255 | FDS disable\n",
      "2022-01-20 07:30:00,945 | Calculate Loss\n",
      "2022-01-20 07:30:00,947 | Update Loss\n",
      "2022-01-20 07:30:00,950 | Backward\n",
      "2022-01-20 07:30:21,819 | Epoch: [5][20/48]\tTime  32.58 ( 32.95)\tData 0.0151 (0.0751)\tLoss (L1) 9.903 (11.679)\n",
      "2022-01-20 07:30:21,833 | ===> Batch : 21\n",
      "2022-01-20 07:30:21,833 | FDS disable\n",
      "2022-01-20 07:30:33,564 | Calculate Loss\n",
      "2022-01-20 07:30:33,565 | Update Loss\n",
      "2022-01-20 07:30:33,569 | Backward\n",
      "2022-01-20 07:30:54,462 | Epoch: [5][21/48]\tTime  32.64 ( 32.93)\tData 0.0140 (0.0722)\tLoss (L1) 9.979 (11.598)\n",
      "2022-01-20 07:30:54,475 | ===> Batch : 22\n",
      "2022-01-20 07:30:54,476 | FDS disable\n",
      "2022-01-20 07:31:06,181 | Calculate Loss\n",
      "2022-01-20 07:31:06,182 | Update Loss\n",
      "2022-01-20 07:31:06,185 | Backward\n",
      "2022-01-20 07:31:27,138 | Epoch: [5][22/48]\tTime  32.68 ( 32.92)\tData 0.0136 (0.0695)\tLoss (L1) 13.126 (11.667)\n",
      "2022-01-20 07:31:27,152 | ===> Batch : 23\n",
      "2022-01-20 07:31:27,152 | FDS disable\n",
      "2022-01-20 07:31:38,922 | Calculate Loss\n",
      "2022-01-20 07:31:38,923 | Update Loss\n",
      "2022-01-20 07:31:38,927 | Backward\n",
      "2022-01-20 07:31:59,446 | Epoch: [5][23/48]\tTime  32.31 ( 32.89)\tData 0.0138 (0.0671)\tLoss (L1) 10.575 (11.620)\n",
      "2022-01-20 07:31:59,462 | ===> Batch : 24\n",
      "2022-01-20 07:31:59,463 | FDS disable\n",
      "2022-01-20 07:32:11,146 | Calculate Loss\n",
      "2022-01-20 07:32:11,147 | Update Loss\n",
      "2022-01-20 07:32:11,150 | Backward\n",
      "2022-01-20 07:32:32,052 | Epoch: [5][24/48]\tTime  32.61 ( 32.88)\tData 0.0166 (0.0650)\tLoss (L1) 13.855 (11.713)\n",
      "2022-01-20 07:32:32,066 | ===> Batch : 25\n",
      "2022-01-20 07:32:32,066 | FDS disable\n",
      "2022-01-20 07:32:43,784 | Calculate Loss\n",
      "2022-01-20 07:32:43,786 | Update Loss\n",
      "2022-01-20 07:32:43,789 | Backward\n",
      "2022-01-20 07:33:05,222 | Epoch: [5][25/48]\tTime  33.17 ( 32.89)\tData 0.0142 (0.0630)\tLoss (L1) 10.184 (11.652)\n",
      "2022-01-20 07:33:05,236 | ===> Batch : 26\n",
      "2022-01-20 07:33:05,237 | FDS disable\n",
      "2022-01-20 07:33:16,994 | Calculate Loss\n",
      "2022-01-20 07:33:16,996 | Update Loss\n",
      "2022-01-20 07:33:16,999 | Backward\n",
      "2022-01-20 07:33:38,461 | Epoch: [5][26/48]\tTime  33.24 ( 32.91)\tData 0.0143 (0.0611)\tLoss (L1) 11.101 (11.631)\n",
      "2022-01-20 07:33:38,475 | ===> Batch : 27\n",
      "2022-01-20 07:33:38,476 | FDS disable\n",
      "2022-01-20 07:33:50,103 | Calculate Loss\n",
      "2022-01-20 07:33:50,104 | Update Loss\n",
      "2022-01-20 07:33:50,108 | Backward\n",
      "2022-01-20 07:34:10,650 | Epoch: [5][27/48]\tTime  32.19 ( 32.88)\tData 0.0144 (0.0594)\tLoss (L1) 10.338 (11.583)\n",
      "2022-01-20 07:34:10,664 | ===> Batch : 28\n",
      "2022-01-20 07:34:10,665 | FDS disable\n",
      "2022-01-20 07:34:22,357 | Calculate Loss\n",
      "2022-01-20 07:34:22,359 | Update Loss\n",
      "2022-01-20 07:34:22,362 | Backward\n",
      "2022-01-20 07:34:43,564 | Epoch: [5][28/48]\tTime  32.91 ( 32.88)\tData 0.0146 (0.0578)\tLoss (L1) 9.841 (11.521)\n",
      "2022-01-20 07:34:43,578 | ===> Batch : 29\n",
      "2022-01-20 07:34:43,578 | FDS disable\n",
      "2022-01-20 07:34:55,237 | Calculate Loss\n",
      "2022-01-20 07:34:55,239 | Update Loss\n",
      "2022-01-20 07:34:55,242 | Backward\n",
      "2022-01-20 07:35:16,424 | Epoch: [5][29/48]\tTime  32.86 ( 32.88)\tData 0.0137 (0.0563)\tLoss (L1) 10.112 (11.472)\n",
      "2022-01-20 07:35:16,439 | ===> Batch : 30\n",
      "2022-01-20 07:35:16,439 | FDS disable\n",
      "2022-01-20 07:35:28,216 | Calculate Loss\n",
      "2022-01-20 07:35:28,217 | Update Loss\n",
      "2022-01-20 07:35:28,221 | Backward\n",
      "2022-01-20 07:35:49,190 | Epoch: [5][30/48]\tTime  32.77 ( 32.88)\tData 0.0145 (0.0549)\tLoss (L1) 11.939 (11.488)\n",
      "2022-01-20 07:35:49,204 | ===> Batch : 31\n",
      "2022-01-20 07:35:49,205 | FDS disable\n",
      "2022-01-20 07:36:00,942 | Calculate Loss\n",
      "2022-01-20 07:36:00,943 | Update Loss\n",
      "2022-01-20 07:36:00,947 | Backward\n",
      "2022-01-20 07:36:21,694 | Epoch: [5][31/48]\tTime  32.50 ( 32.86)\tData 0.0144 (0.0536)\tLoss (L1) 13.752 (11.561)\n",
      "2022-01-20 07:36:21,709 | ===> Batch : 32\n",
      "2022-01-20 07:36:21,709 | FDS disable\n",
      "2022-01-20 07:36:33,301 | Calculate Loss\n",
      "2022-01-20 07:36:33,303 | Update Loss\n",
      "2022-01-20 07:36:33,306 | Backward\n",
      "2022-01-20 07:36:54,953 | Epoch: [5][32/48]\tTime  33.26 ( 32.88)\tData 0.0151 (0.0524)\tLoss (L1) 11.096 (11.546)\n",
      "2022-01-20 07:36:54,967 | ===> Batch : 33\n",
      "2022-01-20 07:36:54,967 | FDS disable\n",
      "2022-01-20 07:37:06,531 | Calculate Loss\n",
      "2022-01-20 07:37:06,532 | Update Loss\n",
      "2022-01-20 07:37:06,536 | Backward\n",
      "2022-01-20 07:37:27,327 | Epoch: [5][33/48]\tTime  32.37 ( 32.86)\tData 0.0138 (0.0512)\tLoss (L1) 9.446 (11.483)\n",
      "2022-01-20 07:37:27,340 | ===> Batch : 34\n",
      "2022-01-20 07:37:27,341 | FDS disable\n",
      "2022-01-20 07:37:38,958 | Calculate Loss\n",
      "2022-01-20 07:37:38,959 | Update Loss\n",
      "2022-01-20 07:37:38,963 | Backward\n",
      "2022-01-20 07:37:59,797 | Epoch: [5][34/48]\tTime  32.47 ( 32.85)\tData 0.0134 (0.0501)\tLoss (L1) 10.573 (11.456)\n",
      "2022-01-20 07:37:59,811 | ===> Batch : 35\n",
      "2022-01-20 07:37:59,812 | FDS disable\n",
      "2022-01-20 07:38:11,395 | Calculate Loss\n",
      "2022-01-20 07:38:11,397 | Update Loss\n",
      "2022-01-20 07:38:11,401 | Backward\n",
      "2022-01-20 07:38:32,659 | Epoch: [5][35/48]\tTime  32.86 ( 32.85)\tData 0.0141 (0.0490)\tLoss (L1) 10.461 (11.427)\n",
      "2022-01-20 07:38:32,673 | ===> Batch : 36\n",
      "2022-01-20 07:38:32,673 | FDS disable\n",
      "2022-01-20 07:38:44,260 | Calculate Loss\n",
      "2022-01-20 07:38:44,261 | Update Loss\n",
      "2022-01-20 07:38:44,264 | Backward\n",
      "2022-01-20 07:39:05,205 | Epoch: [5][36/48]\tTime  32.55 ( 32.84)\tData 0.0141 (0.0481)\tLoss (L1) 10.292 (11.396)\n",
      "2022-01-20 07:39:05,220 | ===> Batch : 37\n",
      "2022-01-20 07:39:05,221 | FDS disable\n",
      "2022-01-20 07:39:16,879 | Calculate Loss\n",
      "2022-01-20 07:39:16,880 | Update Loss\n",
      "2022-01-20 07:39:16,884 | Backward\n",
      "2022-01-20 07:39:37,759 | Epoch: [5][37/48]\tTime  32.55 ( 32.83)\tData 0.0149 (0.0472)\tLoss (L1) 12.080 (11.414)\n",
      "2022-01-20 07:39:37,774 | ===> Batch : 38\n",
      "2022-01-20 07:39:37,775 | FDS disable\n",
      "2022-01-20 07:39:49,341 | Calculate Loss\n",
      "2022-01-20 07:39:49,342 | Update Loss\n",
      "2022-01-20 07:39:49,345 | Backward\n",
      "2022-01-20 07:40:10,328 | Epoch: [5][38/48]\tTime  32.57 ( 32.83)\tData 0.0150 (0.0463)\tLoss (L1) 12.584 (11.445)\n",
      "2022-01-20 07:40:10,341 | ===> Batch : 39\n",
      "2022-01-20 07:40:10,342 | FDS disable\n",
      "2022-01-20 07:40:21,921 | Calculate Loss\n",
      "2022-01-20 07:40:21,922 | Update Loss\n",
      "2022-01-20 07:40:21,926 | Backward\n",
      "2022-01-20 07:40:42,144 | Epoch: [5][39/48]\tTime  31.82 ( 32.80)\tData 0.0134 (0.0455)\tLoss (L1) 10.092 (11.410)\n",
      "2022-01-20 07:40:42,158 | ===> Batch : 40\n",
      "2022-01-20 07:40:42,158 | FDS disable\n",
      "2022-01-20 07:40:53,664 | Calculate Loss\n",
      "2022-01-20 07:40:53,665 | Update Loss\n",
      "2022-01-20 07:40:53,668 | Backward\n",
      "2022-01-20 07:41:14,966 | Epoch: [5][40/48]\tTime  32.82 ( 32.80)\tData 0.0135 (0.0447)\tLoss (L1) 12.122 (11.428)\n",
      "2022-01-20 07:41:14,979 | ===> Batch : 41\n",
      "2022-01-20 07:41:14,980 | FDS disable\n",
      "2022-01-20 07:41:26,577 | Calculate Loss\n",
      "2022-01-20 07:41:26,578 | Update Loss\n",
      "2022-01-20 07:41:26,582 | Backward\n",
      "2022-01-20 07:41:47,325 | Epoch: [5][41/48]\tTime  32.36 ( 32.79)\tData 0.0135 (0.0439)\tLoss (L1) 11.204 (11.423)\n",
      "2022-01-20 07:41:47,340 | ===> Batch : 42\n",
      "2022-01-20 07:41:47,341 | FDS disable\n",
      "2022-01-20 07:41:58,902 | Calculate Loss\n",
      "2022-01-20 07:41:58,903 | Update Loss\n",
      "2022-01-20 07:41:58,907 | Backward\n",
      "2022-01-20 07:42:19,858 | Epoch: [5][42/48]\tTime  32.53 ( 32.78)\tData 0.0149 (0.0432)\tLoss (L1) 9.278 (11.372)\n",
      "2022-01-20 07:42:19,873 | ===> Batch : 43\n",
      "2022-01-20 07:42:19,874 | FDS disable\n",
      "2022-01-20 07:42:31,418 | Calculate Loss\n",
      "2022-01-20 07:42:31,419 | Update Loss\n",
      "2022-01-20 07:42:31,423 | Backward\n",
      "2022-01-20 07:42:52,217 | Epoch: [5][43/48]\tTime  32.36 ( 32.77)\tData 0.0152 (0.0426)\tLoss (L1) 10.096 (11.342)\n",
      "2022-01-20 07:42:52,232 | ===> Batch : 44\n",
      "2022-01-20 07:42:52,233 | FDS disable\n",
      "2022-01-20 07:43:03,806 | Calculate Loss\n",
      "2022-01-20 07:43:03,808 | Update Loss\n",
      "2022-01-20 07:43:03,812 | Backward\n",
      "2022-01-20 07:43:25,379 | Epoch: [5][44/48]\tTime  33.16 ( 32.78)\tData 0.0151 (0.0420)\tLoss (L1) 11.715 (11.350)\n",
      "2022-01-20 07:43:25,394 | ===> Batch : 45\n",
      "2022-01-20 07:43:25,394 | FDS disable\n",
      "2022-01-20 07:43:36,986 | Calculate Loss\n",
      "2022-01-20 07:43:36,987 | Update Loss\n",
      "2022-01-20 07:43:36,991 | Backward\n",
      "2022-01-20 07:43:57,260 | Epoch: [5][45/48]\tTime  31.88 ( 32.76)\tData 0.0150 (0.0414)\tLoss (L1) 11.584 (11.356)\n",
      "2022-01-20 07:43:57,275 | ===> Batch : 46\n",
      "2022-01-20 07:43:57,275 | FDS disable\n",
      "2022-01-20 07:44:08,887 | Calculate Loss\n",
      "2022-01-20 07:44:08,888 | Update Loss\n",
      "2022-01-20 07:44:08,892 | Backward\n",
      "2022-01-20 07:44:29,912 | Epoch: [5][46/48]\tTime  32.65 ( 32.76)\tData 0.0147 (0.0408)\tLoss (L1) 14.065 (11.415)\n",
      "2022-01-20 07:44:29,926 | ===> Batch : 47\n",
      "2022-01-20 07:44:29,926 | FDS disable\n",
      "2022-01-20 07:44:41,479 | Calculate Loss\n",
      "2022-01-20 07:44:41,481 | Update Loss\n",
      "2022-01-20 07:44:41,485 | Backward\n",
      "2022-01-20 07:45:02,346 | Epoch: [5][47/48]\tTime  32.43 ( 32.75)\tData 0.0141 (0.0402)\tLoss (L1) 11.082 (11.407)\n",
      "2022-01-20 07:45:02,359 | ===> Batch : 48\n",
      "2022-01-20 07:45:02,360 | FDS disable\n",
      "2022-01-20 07:45:10,464 | Calculate Loss\n",
      "2022-01-20 07:45:10,465 | Update Loss\n",
      "2022-01-20 07:45:10,469 | Backward\n",
      "2022-01-20 07:45:24,627 | Epoch: [5][48/48]\tTime  22.28 ( 32.54)\tData 0.0136 (0.0397)\tLoss (L1) 11.108 (11.403)\n",
      "2022-01-20 07:45:36,590 | Val: [0/9]\tTime 11.794 (11.794)\tLoss (MSE) 226.280 (226.280)\tLoss (L1) 11.672 (11.672)\n",
      "2022-01-20 07:45:45,440 | Val: [1/9]\tTime  8.850 (10.322)\tLoss (MSE) 208.469 (217.375)\tLoss (L1) 11.493 (11.582)\n",
      "2022-01-20 07:45:54,243 | Val: [2/9]\tTime  8.803 ( 9.816)\tLoss (MSE) 213.964 (216.238)\tLoss (L1) 11.683 (11.616)\n",
      "2022-01-20 07:46:02,946 | Val: [3/9]\tTime  8.703 ( 9.537)\tLoss (MSE) 222.327 (217.760)\tLoss (L1) 12.179 (11.757)\n",
      "2022-01-20 07:46:11,674 | Val: [4/9]\tTime  8.729 ( 9.376)\tLoss (MSE) 260.131 (226.234)\tLoss (L1) 12.569 (11.919)\n",
      "2022-01-20 07:46:20,412 | Val: [5/9]\tTime  8.737 ( 9.269)\tLoss (MSE) 246.963 (229.689)\tLoss (L1) 12.603 (12.033)\n",
      "2022-01-20 07:46:29,162 | Val: [6/9]\tTime  8.751 ( 9.195)\tLoss (MSE) 222.694 (228.690)\tLoss (L1) 11.553 (11.965)\n",
      "2022-01-20 07:46:37,947 | Val: [7/9]\tTime  8.785 ( 9.144)\tLoss (MSE) 262.035 (232.858)\tLoss (L1) 12.841 (12.074)\n",
      "2022-01-20 07:46:40,945 | Val: [8/9]\tTime  2.998 ( 8.461)\tLoss (MSE) 204.906 (231.656)\tLoss (L1) 11.175 (12.036)\n",
      "2022-01-20 07:46:41,089 |  * Overall: MSE 231.656\tL1 12.036\tG-Mean 8.112\n",
      "2022-01-20 07:46:41,090 |  * Many: MSE 189.225\tL1 10.842\tG-Mean 7.293\n",
      "2022-01-20 07:46:41,090 |  * Median: MSE 291.219\tL1 13.717\tG-Mean 9.266\n",
      "2022-01-20 07:46:41,091 |  * Low: MSE 467.680\tL1 18.662\tG-Mean 15.340\n",
      "2022-01-20 07:46:41,096 | Best L1 Loss: 12.036\n",
      "2022-01-20 07:46:42,320 | ===> Saving current best checkpoint...\n",
      "2022-01-20 07:46:43,484 | Epoch #5: Train loss [11.4031]; Val loss: MSE [231.6562], L1 [12.0356], G-Mean [8.1116]\n",
      "2022-01-20 07:46:43,489 | Training...\n",
      "2022-01-20 07:46:43,490 | Load train loader\n",
      "2022-01-20 07:46:44,708 | ===> Batch : 1\n",
      "2022-01-20 07:46:44,710 | FDS disable\n",
      "2022-01-20 07:46:59,335 | Calculate Loss\n",
      "2022-01-20 07:46:59,337 | Update Loss\n",
      "2022-01-20 07:46:59,470 | Backward\n",
      "2022-01-20 07:47:21,442 | Epoch: [6][ 1/48]\tTime  37.95 ( 37.95)\tData 1.2180 (1.2180)\tLoss (L1) 13.187 (13.187)\n",
      "2022-01-20 07:47:21,458 | ===> Batch : 2\n",
      "2022-01-20 07:47:21,459 | FDS disable\n",
      "2022-01-20 07:47:33,152 | Calculate Loss\n",
      "2022-01-20 07:47:33,153 | Update Loss\n",
      "2022-01-20 07:47:33,157 | Backward\n",
      "2022-01-20 07:47:54,592 | Epoch: [6][ 2/48]\tTime  33.15 ( 35.55)\tData 0.0169 (0.6175)\tLoss (L1) 13.352 (13.270)\n",
      "2022-01-20 07:47:54,607 | ===> Batch : 3\n",
      "2022-01-20 07:47:54,608 | FDS disable\n",
      "2022-01-20 07:48:06,280 | Calculate Loss\n",
      "2022-01-20 07:48:06,282 | Update Loss\n",
      "2022-01-20 07:48:06,286 | Backward\n",
      "2022-01-20 07:48:27,233 | Epoch: [6][ 3/48]\tTime  32.64 ( 34.58)\tData 0.0147 (0.4166)\tLoss (L1) 10.886 (12.475)\n",
      "2022-01-20 07:48:27,246 | ===> Batch : 4\n",
      "2022-01-20 07:48:27,247 | FDS disable\n",
      "2022-01-20 07:48:38,992 | Calculate Loss\n",
      "2022-01-20 07:48:38,993 | Update Loss\n",
      "2022-01-20 07:48:38,997 | Backward\n",
      "2022-01-20 07:48:59,751 | Epoch: [6][ 4/48]\tTime  32.52 ( 34.07)\tData 0.0132 (0.3157)\tLoss (L1) 11.647 (12.268)\n",
      "2022-01-20 07:48:59,765 | ===> Batch : 5\n",
      "2022-01-20 07:48:59,766 | FDS disable\n",
      "2022-01-20 07:49:11,507 | Calculate Loss\n",
      "2022-01-20 07:49:11,508 | Update Loss\n",
      "2022-01-20 07:49:11,512 | Backward\n",
      "2022-01-20 07:49:33,017 | Epoch: [6][ 5/48]\tTime  33.27 ( 33.91)\tData 0.0132 (0.2552)\tLoss (L1) 11.082 (12.031)\n",
      "2022-01-20 07:49:33,033 | ===> Batch : 6\n",
      "2022-01-20 07:49:33,033 | FDS disable\n",
      "2022-01-20 07:49:44,774 | Calculate Loss\n",
      "2022-01-20 07:49:44,776 | Update Loss\n",
      "2022-01-20 07:49:44,779 | Backward\n",
      "2022-01-20 07:50:05,530 | Epoch: [6][ 6/48]\tTime  32.51 ( 33.67)\tData 0.0152 (0.2152)\tLoss (L1) 11.939 (12.015)\n",
      "2022-01-20 07:50:05,542 | ===> Batch : 7\n",
      "2022-01-20 07:50:05,543 | FDS disable\n",
      "2022-01-20 07:50:17,256 | Calculate Loss\n",
      "2022-01-20 07:50:17,258 | Update Loss\n",
      "2022-01-20 07:50:17,261 | Backward\n",
      "2022-01-20 07:50:37,869 | Epoch: [6][ 7/48]\tTime  32.34 ( 33.48)\tData 0.0127 (0.1863)\tLoss (L1) 10.703 (11.828)\n",
      "2022-01-20 07:50:37,882 | ===> Batch : 8\n",
      "2022-01-20 07:50:37,883 | FDS disable\n",
      "2022-01-20 07:50:49,527 | Calculate Loss\n",
      "2022-01-20 07:50:49,529 | Update Loss\n",
      "2022-01-20 07:50:49,532 | Backward\n",
      "2022-01-20 07:51:10,361 | Epoch: [6][ 8/48]\tTime  32.49 ( 33.36)\tData 0.0130 (0.1646)\tLoss (L1) 10.908 (11.713)\n",
      "2022-01-20 07:51:10,379 | ===> Batch : 9\n",
      "2022-01-20 07:51:10,380 | FDS disable\n",
      "2022-01-20 07:51:22,226 | Calculate Loss\n",
      "2022-01-20 07:51:22,228 | Update Loss\n",
      "2022-01-20 07:51:22,231 | Backward\n",
      "2022-01-20 07:51:43,171 | Epoch: [6][ 9/48]\tTime  32.81 ( 33.30)\tData 0.0181 (0.1483)\tLoss (L1) 10.937 (11.627)\n",
      "2022-01-20 07:51:43,187 | ===> Batch : 10\n",
      "2022-01-20 07:51:43,188 | FDS disable\n",
      "2022-01-20 07:51:54,952 | Calculate Loss\n",
      "2022-01-20 07:51:54,953 | Update Loss\n",
      "2022-01-20 07:51:54,957 | Backward\n",
      "2022-01-20 07:52:15,963 | Epoch: [6][10/48]\tTime  32.79 ( 33.25)\tData 0.0163 (0.1351)\tLoss (L1) 10.190 (11.483)\n",
      "2022-01-20 07:52:15,977 | ===> Batch : 11\n",
      "2022-01-20 07:52:15,978 | FDS disable\n",
      "2022-01-20 07:52:27,696 | Calculate Loss\n",
      "2022-01-20 07:52:27,698 | Update Loss\n",
      "2022-01-20 07:52:27,701 | Backward\n",
      "2022-01-20 07:52:48,613 | Epoch: [6][11/48]\tTime  32.65 ( 33.19)\tData 0.0141 (0.1241)\tLoss (L1) 9.894 (11.339)\n",
      "2022-01-20 07:52:48,626 | ===> Batch : 12\n",
      "2022-01-20 07:52:48,626 | FDS disable\n",
      "2022-01-20 07:53:00,312 | Calculate Loss\n",
      "2022-01-20 07:53:00,313 | Update Loss\n",
      "2022-01-20 07:53:00,317 | Backward\n",
      "2022-01-20 07:53:21,052 | Epoch: [6][12/48]\tTime  32.44 ( 33.13)\tData 0.0124 (0.1148)\tLoss (L1) 10.962 (11.307)\n",
      "2022-01-20 07:53:21,065 | ===> Batch : 13\n",
      "2022-01-20 07:53:21,066 | FDS disable\n",
      "2022-01-20 07:53:32,751 | Calculate Loss\n",
      "2022-01-20 07:53:32,753 | Update Loss\n",
      "2022-01-20 07:53:32,756 | Backward\n",
      "2022-01-20 07:53:54,087 | Epoch: [6][13/48]\tTime  33.03 ( 33.12)\tData 0.0130 (0.1070)\tLoss (L1) 10.272 (11.228)\n",
      "2022-01-20 07:53:54,104 | ===> Batch : 14\n",
      "2022-01-20 07:53:54,105 | FDS disable\n",
      "2022-01-20 07:54:05,825 | Calculate Loss\n",
      "2022-01-20 07:54:05,826 | Update Loss\n",
      "2022-01-20 07:54:05,830 | Backward\n",
      "2022-01-20 07:54:26,984 | Epoch: [6][14/48]\tTime  32.90 ( 33.11)\tData 0.0168 (0.1005)\tLoss (L1) 10.024 (11.142)\n",
      "2022-01-20 07:54:26,996 | ===> Batch : 15\n",
      "2022-01-20 07:54:26,997 | FDS disable\n",
      "2022-01-20 07:54:38,824 | Calculate Loss\n",
      "2022-01-20 07:54:38,826 | Update Loss\n",
      "2022-01-20 07:54:38,829 | Backward\n",
      "2022-01-20 07:54:58,941 | Epoch: [6][15/48]\tTime  31.96 ( 33.03)\tData 0.0127 (0.0947)\tLoss (L1) 10.446 (11.095)\n",
      "2022-01-20 07:54:58,954 | ===> Batch : 16\n",
      "2022-01-20 07:54:58,954 | FDS disable\n",
      "2022-01-20 07:55:10,685 | Calculate Loss\n",
      "2022-01-20 07:55:10,686 | Update Loss\n",
      "2022-01-20 07:55:10,690 | Backward\n",
      "2022-01-20 07:55:31,967 | Epoch: [6][16/48]\tTime  33.03 ( 33.03)\tData 0.0129 (0.0896)\tLoss (L1) 11.259 (11.105)\n",
      "2022-01-20 07:55:31,981 | ===> Batch : 17\n",
      "2022-01-20 07:55:31,982 | FDS disable\n",
      "2022-01-20 07:55:43,788 | Calculate Loss\n",
      "2022-01-20 07:55:43,790 | Update Loss\n",
      "2022-01-20 07:55:43,794 | Backward\n",
      "2022-01-20 07:56:03,797 | Epoch: [6][17/48]\tTime  31.83 ( 32.96)\tData 0.0143 (0.0851)\tLoss (L1) 8.489 (10.952)\n",
      "2022-01-20 07:56:03,811 | ===> Batch : 18\n",
      "2022-01-20 07:56:03,812 | FDS disable\n",
      "2022-01-20 07:56:15,632 | Calculate Loss\n",
      "2022-01-20 07:56:15,634 | Update Loss\n",
      "2022-01-20 07:56:15,637 | Backward\n",
      "2022-01-20 07:56:36,825 | Epoch: [6][18/48]\tTime  33.03 ( 32.96)\tData 0.0142 (0.0812)\tLoss (L1) 10.438 (10.923)\n",
      "2022-01-20 07:56:36,839 | ===> Batch : 19\n",
      "2022-01-20 07:56:36,840 | FDS disable\n",
      "2022-01-20 07:56:48,584 | Calculate Loss\n",
      "2022-01-20 07:56:48,586 | Update Loss\n",
      "2022-01-20 07:56:48,590 | Backward\n",
      "2022-01-20 07:57:09,753 | Epoch: [6][19/48]\tTime  32.93 ( 32.96)\tData 0.0141 (0.0777)\tLoss (L1) 9.196 (10.832)\n",
      "2022-01-20 07:57:09,768 | ===> Batch : 20\n",
      "2022-01-20 07:57:09,768 | FDS disable\n",
      "2022-01-20 07:57:21,430 | Calculate Loss\n",
      "2022-01-20 07:57:21,431 | Update Loss\n",
      "2022-01-20 07:57:21,435 | Backward\n",
      "2022-01-20 07:57:42,470 | Epoch: [6][20/48]\tTime  32.72 ( 32.95)\tData 0.0144 (0.0745)\tLoss (L1) 12.703 (10.926)\n",
      "2022-01-20 07:57:42,483 | ===> Batch : 21\n",
      "2022-01-20 07:57:42,484 | FDS disable\n",
      "2022-01-20 07:57:54,216 | Calculate Loss\n",
      "2022-01-20 07:57:54,217 | Update Loss\n",
      "2022-01-20 07:57:54,221 | Backward\n",
      "2022-01-20 07:58:15,214 | Epoch: [6][21/48]\tTime  32.74 ( 32.94)\tData 0.0135 (0.0716)\tLoss (L1) 9.331 (10.850)\n",
      "2022-01-20 07:58:15,228 | ===> Batch : 22\n",
      "2022-01-20 07:58:15,228 | FDS disable\n",
      "2022-01-20 07:58:26,917 | Calculate Loss\n",
      "2022-01-20 07:58:26,918 | Update Loss\n",
      "2022-01-20 07:58:26,922 | Backward\n",
      "2022-01-20 07:58:48,012 | Epoch: [6][22/48]\tTime  32.80 ( 32.93)\tData 0.0141 (0.0690)\tLoss (L1) 14.615 (11.021)\n",
      "2022-01-20 07:58:48,028 | ===> Batch : 23\n",
      "2022-01-20 07:58:48,028 | FDS disable\n",
      "2022-01-20 07:58:59,736 | Calculate Loss\n",
      "2022-01-20 07:58:59,738 | Update Loss\n",
      "2022-01-20 07:58:59,742 | Backward\n",
      "2022-01-20 07:59:20,737 | Epoch: [6][23/48]\tTime  32.72 ( 32.92)\tData 0.0154 (0.0667)\tLoss (L1) 12.471 (11.084)\n",
      "2022-01-20 07:59:20,752 | ===> Batch : 24\n",
      "2022-01-20 07:59:20,753 | FDS disable\n",
      "2022-01-20 07:59:32,494 | Calculate Loss\n",
      "2022-01-20 07:59:32,496 | Update Loss\n",
      "2022-01-20 07:59:32,499 | Backward\n",
      "2022-01-20 07:59:53,209 | Epoch: [6][24/48]\tTime  32.47 ( 32.90)\tData 0.0153 (0.0645)\tLoss (L1) 10.382 (11.055)\n",
      "2022-01-20 07:59:53,223 | ===> Batch : 25\n",
      "2022-01-20 07:59:53,224 | FDS disable\n",
      "2022-01-20 08:00:04,937 | Calculate Loss\n",
      "2022-01-20 08:00:04,938 | Update Loss\n",
      "2022-01-20 08:00:04,941 | Backward\n",
      "2022-01-20 08:00:25,705 | Epoch: [6][25/48]\tTime  32.50 ( 32.89)\tData 0.0138 (0.0625)\tLoss (L1) 10.406 (11.029)\n",
      "2022-01-20 08:00:25,720 | ===> Batch : 26\n",
      "2022-01-20 08:00:25,720 | FDS disable\n",
      "2022-01-20 08:00:37,458 | Calculate Loss\n",
      "2022-01-20 08:00:37,460 | Update Loss\n",
      "2022-01-20 08:00:37,463 | Backward\n",
      "2022-01-20 08:00:58,151 | Epoch: [6][26/48]\tTime  32.45 ( 32.87)\tData 0.0153 (0.0607)\tLoss (L1) 11.273 (11.038)\n",
      "2022-01-20 08:00:58,165 | ===> Batch : 27\n",
      "2022-01-20 08:00:58,166 | FDS disable\n",
      "2022-01-20 08:01:09,887 | Calculate Loss\n",
      "2022-01-20 08:01:09,888 | Update Loss\n",
      "2022-01-20 08:01:09,892 | Backward\n",
      "2022-01-20 08:01:31,378 | Epoch: [6][27/48]\tTime  33.23 ( 32.88)\tData 0.0146 (0.0590)\tLoss (L1) 12.297 (11.085)\n",
      "2022-01-20 08:01:31,392 | ===> Batch : 28\n",
      "2022-01-20 08:01:31,392 | FDS disable\n",
      "2022-01-20 08:01:43,128 | Calculate Loss\n",
      "2022-01-20 08:01:43,130 | Update Loss\n",
      "2022-01-20 08:01:43,134 | Backward\n",
      "2022-01-20 08:02:03,869 | Epoch: [6][28/48]\tTime  32.49 ( 32.87)\tData 0.0140 (0.0574)\tLoss (L1) 12.543 (11.137)\n",
      "2022-01-20 08:02:03,884 | ===> Batch : 29\n",
      "2022-01-20 08:02:03,884 | FDS disable\n",
      "2022-01-20 08:02:15,636 | Calculate Loss\n",
      "2022-01-20 08:02:15,638 | Update Loss\n",
      "2022-01-20 08:02:15,642 | Backward\n",
      "2022-01-20 08:02:36,907 | Epoch: [6][29/48]\tTime  33.04 ( 32.88)\tData 0.0144 (0.0559)\tLoss (L1) 12.748 (11.192)\n",
      "2022-01-20 08:02:36,921 | ===> Batch : 30\n",
      "2022-01-20 08:02:36,922 | FDS disable\n",
      "2022-01-20 08:02:48,628 | Calculate Loss\n",
      "2022-01-20 08:02:48,630 | Update Loss\n",
      "2022-01-20 08:02:48,634 | Backward\n",
      "2022-01-20 08:03:09,564 | Epoch: [6][30/48]\tTime  32.66 ( 32.87)\tData 0.0148 (0.0545)\tLoss (L1) 10.845 (11.181)\n",
      "2022-01-20 08:03:09,579 | ===> Batch : 31\n",
      "2022-01-20 08:03:09,580 | FDS disable\n",
      "2022-01-20 08:03:21,312 | Calculate Loss\n",
      "2022-01-20 08:03:21,314 | Update Loss\n",
      "2022-01-20 08:03:21,318 | Backward\n",
      "2022-01-20 08:03:42,126 | Epoch: [6][31/48]\tTime  32.56 ( 32.86)\tData 0.0151 (0.0532)\tLoss (L1) 10.472 (11.158)\n",
      "2022-01-20 08:03:42,140 | ===> Batch : 32\n",
      "2022-01-20 08:03:42,141 | FDS disable\n",
      "2022-01-20 08:03:53,789 | Calculate Loss\n",
      "2022-01-20 08:03:53,790 | Update Loss\n",
      "2022-01-20 08:03:53,794 | Backward\n",
      "2022-01-20 08:04:14,899 | Epoch: [6][32/48]\tTime  32.77 ( 32.86)\tData 0.0141 (0.0520)\tLoss (L1) 12.010 (11.185)\n",
      "2022-01-20 08:04:14,913 | ===> Batch : 33\n",
      "2022-01-20 08:04:14,914 | FDS disable\n",
      "2022-01-20 08:04:26,387 | Calculate Loss\n",
      "2022-01-20 08:04:26,388 | Update Loss\n",
      "2022-01-20 08:04:26,392 | Backward\n",
      "2022-01-20 08:04:47,856 | Epoch: [6][33/48]\tTime  32.96 ( 32.86)\tData 0.0143 (0.0509)\tLoss (L1) 12.057 (11.211)\n",
      "2022-01-20 08:04:47,870 | ===> Batch : 34\n",
      "2022-01-20 08:04:47,871 | FDS disable\n",
      "2022-01-20 08:04:59,433 | Calculate Loss\n",
      "2022-01-20 08:04:59,436 | Update Loss\n",
      "2022-01-20 08:04:59,440 | Backward\n",
      "2022-01-20 08:05:20,774 | Epoch: [6][34/48]\tTime  32.92 ( 32.86)\tData 0.0138 (0.0498)\tLoss (L1) 11.261 (11.212)\n",
      "2022-01-20 08:05:20,787 | ===> Batch : 35\n",
      "2022-01-20 08:05:20,788 | FDS disable\n",
      "2022-01-20 08:05:32,361 | Calculate Loss\n",
      "2022-01-20 08:05:32,363 | Update Loss\n",
      "2022-01-20 08:05:32,366 | Backward\n",
      "2022-01-20 08:05:53,519 | Epoch: [6][35/48]\tTime  32.75 ( 32.86)\tData 0.0138 (0.0488)\tLoss (L1) 12.088 (11.237)\n",
      "2022-01-20 08:05:53,534 | ===> Batch : 36\n",
      "2022-01-20 08:05:53,535 | FDS disable\n",
      "2022-01-20 08:06:05,084 | Calculate Loss\n",
      "2022-01-20 08:06:05,085 | Update Loss\n",
      "2022-01-20 08:06:05,089 | Backward\n",
      "2022-01-20 08:06:26,011 | Epoch: [6][36/48]\tTime  32.49 ( 32.85)\tData 0.0156 (0.0478)\tLoss (L1) 11.706 (11.250)\n",
      "2022-01-20 08:06:26,025 | ===> Batch : 37\n",
      "2022-01-20 08:06:26,026 | FDS disable\n",
      "2022-01-20 08:06:37,596 | Calculate Loss\n",
      "2022-01-20 08:06:37,598 | Update Loss\n",
      "2022-01-20 08:06:37,601 | Backward\n",
      "2022-01-20 08:06:58,801 | Epoch: [6][37/48]\tTime  32.79 ( 32.85)\tData 0.0140 (0.0469)\tLoss (L1) 10.115 (11.220)\n",
      "2022-01-20 08:06:58,815 | ===> Batch : 38\n",
      "2022-01-20 08:06:58,815 | FDS disable\n",
      "2022-01-20 08:07:10,421 | Calculate Loss\n",
      "2022-01-20 08:07:10,422 | Update Loss\n",
      "2022-01-20 08:07:10,426 | Backward\n",
      "2022-01-20 08:07:30,897 | Epoch: [6][38/48]\tTime  32.10 ( 32.83)\tData 0.0143 (0.0461)\tLoss (L1) 10.533 (11.202)\n",
      "2022-01-20 08:07:30,911 | ===> Batch : 39\n",
      "2022-01-20 08:07:30,912 | FDS disable\n",
      "2022-01-20 08:07:42,519 | Calculate Loss\n",
      "2022-01-20 08:07:42,520 | Update Loss\n",
      "2022-01-20 08:07:42,524 | Backward\n",
      "2022-01-20 08:08:03,574 | Epoch: [6][39/48]\tTime  32.68 ( 32.82)\tData 0.0141 (0.0452)\tLoss (L1) 12.543 (11.236)\n",
      "2022-01-20 08:08:03,588 | ===> Batch : 40\n",
      "2022-01-20 08:08:03,588 | FDS disable\n",
      "2022-01-20 08:08:15,179 | Calculate Loss\n",
      "2022-01-20 08:08:15,180 | Update Loss\n",
      "2022-01-20 08:08:15,184 | Backward\n",
      "2022-01-20 08:08:36,068 | Epoch: [6][40/48]\tTime  32.49 ( 32.81)\tData 0.0135 (0.0445)\tLoss (L1) 10.401 (11.215)\n",
      "2022-01-20 08:08:36,082 | ===> Batch : 41\n",
      "2022-01-20 08:08:36,083 | FDS disable\n",
      "2022-01-20 08:08:47,644 | Calculate Loss\n",
      "2022-01-20 08:08:47,646 | Update Loss\n",
      "2022-01-20 08:08:47,649 | Backward\n",
      "2022-01-20 08:09:08,103 | Epoch: [6][41/48]\tTime  32.03 ( 32.80)\tData 0.0142 (0.0437)\tLoss (L1) 9.257 (11.167)\n",
      "2022-01-20 08:09:08,116 | ===> Batch : 42\n",
      "2022-01-20 08:09:08,117 | FDS disable\n",
      "2022-01-20 08:09:19,719 | Calculate Loss\n",
      "2022-01-20 08:09:19,721 | Update Loss\n",
      "2022-01-20 08:09:19,724 | Backward\n",
      "2022-01-20 08:09:40,952 | Epoch: [6][42/48]\tTime  32.85 ( 32.80)\tData 0.0136 (0.0430)\tLoss (L1) 11.515 (11.176)\n",
      "2022-01-20 08:09:40,966 | ===> Batch : 43\n",
      "2022-01-20 08:09:40,966 | FDS disable\n",
      "2022-01-20 08:09:52,536 | Calculate Loss\n",
      "2022-01-20 08:09:52,537 | Update Loss\n",
      "2022-01-20 08:09:52,540 | Backward\n",
      "2022-01-20 08:10:13,217 | Epoch: [6][43/48]\tTime  32.27 ( 32.78)\tData 0.0137 (0.0423)\tLoss (L1) 11.014 (11.172)\n",
      "2022-01-20 08:10:13,231 | ===> Batch : 44\n",
      "2022-01-20 08:10:13,231 | FDS disable\n",
      "2022-01-20 08:10:24,748 | Calculate Loss\n",
      "2022-01-20 08:10:24,749 | Update Loss\n",
      "2022-01-20 08:10:24,753 | Backward\n",
      "2022-01-20 08:10:46,221 | Epoch: [6][44/48]\tTime  33.00 ( 32.79)\tData 0.0137 (0.0417)\tLoss (L1) 9.855 (11.142)\n",
      "2022-01-20 08:10:46,235 | ===> Batch : 45\n",
      "2022-01-20 08:10:46,236 | FDS disable\n",
      "2022-01-20 08:10:57,728 | Calculate Loss\n",
      "2022-01-20 08:10:57,729 | Update Loss\n",
      "2022-01-20 08:10:57,733 | Backward\n",
      "2022-01-20 08:11:18,940 | Epoch: [6][45/48]\tTime  32.72 ( 32.79)\tData 0.0141 (0.0411)\tLoss (L1) 12.808 (11.179)\n",
      "2022-01-20 08:11:18,953 | ===> Batch : 46\n",
      "2022-01-20 08:11:18,954 | FDS disable\n",
      "2022-01-20 08:11:30,536 | Calculate Loss\n",
      "2022-01-20 08:11:30,537 | Update Loss\n",
      "2022-01-20 08:11:30,540 | Backward\n",
      "2022-01-20 08:11:51,408 | Epoch: [6][46/48]\tTime  32.47 ( 32.78)\tData 0.0134 (0.0405)\tLoss (L1) 9.220 (11.136)\n",
      "2022-01-20 08:11:51,423 | ===> Batch : 47\n",
      "2022-01-20 08:11:51,424 | FDS disable\n",
      "2022-01-20 08:12:02,973 | Calculate Loss\n",
      "2022-01-20 08:12:02,974 | Update Loss\n",
      "2022-01-20 08:12:02,978 | Backward\n",
      "2022-01-20 08:12:24,271 | Epoch: [6][47/48]\tTime  32.86 ( 32.78)\tData 0.0154 (0.0399)\tLoss (L1) 10.932 (11.132)\n",
      "2022-01-20 08:12:24,285 | ===> Batch : 48\n",
      "2022-01-20 08:12:24,286 | FDS disable\n",
      "2022-01-20 08:12:32,344 | Calculate Loss\n",
      "2022-01-20 08:12:32,346 | Update Loss\n",
      "2022-01-20 08:12:32,349 | Backward\n",
      "2022-01-20 08:12:46,609 | Epoch: [6][48/48]\tTime  22.34 ( 32.56)\tData 0.0139 (0.0394)\tLoss (L1) 12.550 (11.153)\n",
      "2022-01-20 08:12:58,517 | Val: [0/9]\tTime 11.733 (11.733)\tLoss (MSE) 258.534 (258.534)\tLoss (L1) 12.544 (12.544)\n",
      "2022-01-20 08:13:07,660 | Val: [1/9]\tTime  9.144 (10.438)\tLoss (MSE) 218.043 (238.288)\tLoss (L1) 11.487 (12.016)\n",
      "2022-01-20 08:13:16,791 | Val: [2/9]\tTime  9.132 (10.003)\tLoss (MSE) 228.805 (235.127)\tLoss (L1) 12.045 (12.026)\n",
      "2022-01-20 08:13:25,871 | Val: [3/9]\tTime  9.079 ( 9.772)\tLoss (MSE) 248.019 (238.350)\tLoss (L1) 12.440 (12.129)\n",
      "2022-01-20 08:13:34,979 | Val: [4/9]\tTime  9.108 ( 9.639)\tLoss (MSE) 321.842 (255.049)\tLoss (L1) 14.378 (12.579)\n",
      "2022-01-20 08:13:44,057 | Val: [5/9]\tTime  9.078 ( 9.546)\tLoss (MSE) 287.067 (260.385)\tLoss (L1) 13.509 (12.734)\n",
      "2022-01-20 08:13:53,261 | Val: [6/9]\tTime  9.204 ( 9.497)\tLoss (MSE) 250.525 (258.977)\tLoss (L1) 12.292 (12.671)\n",
      "2022-01-20 08:14:02,397 | Val: [7/9]\tTime  9.136 ( 9.452)\tLoss (MSE) 288.399 (262.654)\tLoss (L1) 13.008 (12.713)\n",
      "2022-01-20 08:14:05,669 | Val: [8/9]\tTime  3.272 ( 8.765)\tLoss (MSE) 263.280 (262.681)\tLoss (L1) 12.466 (12.702)\n",
      "2022-01-20 08:14:05,801 |  * Overall: MSE 262.681\tL1 12.702\tG-Mean 8.316\n",
      "2022-01-20 08:14:05,802 |  * Many: MSE 227.236\tL1 11.863\tG-Mean 7.776\n",
      "2022-01-20 08:14:05,803 |  * Median: MSE 323.117\tL1 14.040\tG-Mean 9.144\n",
      "2022-01-20 08:14:05,803 |  * Low: MSE 430.315\tL1 16.934\tG-Mean 12.056\n",
      "2022-01-20 08:14:05,809 | Best L1 Loss: 12.036\n",
      "2022-01-20 08:14:07,045 | Epoch #6: Train loss [11.1525]; Val loss: MSE [262.6813], L1 [12.7022], G-Mean [8.3162]\n",
      "2022-01-20 08:14:07,047 | Training...\n",
      "2022-01-20 08:14:07,048 | Load train loader\n",
      "2022-01-20 08:14:08,285 | ===> Batch : 1\n",
      "2022-01-20 08:14:08,287 | FDS disable\n",
      "2022-01-20 08:14:22,916 | Calculate Loss\n",
      "2022-01-20 08:14:22,918 | Update Loss\n",
      "2022-01-20 08:14:23,049 | Backward\n",
      "2022-01-20 08:14:44,892 | Epoch: [7][ 1/48]\tTime  37.84 ( 37.84)\tData 1.2372 (1.2372)\tLoss (L1) 10.018 (10.018)\n",
      "2022-01-20 08:14:44,905 | ===> Batch : 2\n",
      "2022-01-20 08:14:44,906 | FDS disable\n",
      "2022-01-20 08:14:56,590 | Calculate Loss\n",
      "2022-01-20 08:14:56,592 | Update Loss\n",
      "2022-01-20 08:14:56,595 | Backward\n",
      "2022-01-20 08:15:17,585 | Epoch: [7][ 2/48]\tTime  32.69 ( 35.27)\tData 0.0130 (0.6251)\tLoss (L1) 11.761 (10.889)\n",
      "2022-01-20 08:15:17,598 | ===> Batch : 3\n",
      "2022-01-20 08:15:17,599 | FDS disable\n",
      "2022-01-20 08:15:29,276 | Calculate Loss\n",
      "2022-01-20 08:15:29,277 | Update Loss\n",
      "2022-01-20 08:15:29,281 | Backward\n",
      "2022-01-20 08:15:50,103 | Epoch: [7][ 3/48]\tTime  32.52 ( 34.35)\tData 0.0131 (0.4211)\tLoss (L1) 11.189 (10.989)\n",
      "2022-01-20 08:15:50,120 | ===> Batch : 4\n",
      "2022-01-20 08:15:50,120 | FDS disable\n",
      "2022-01-20 08:16:01,837 | Calculate Loss\n",
      "2022-01-20 08:16:01,839 | Update Loss\n",
      "2022-01-20 08:16:01,842 | Backward\n",
      "2022-01-20 08:16:22,516 | Epoch: [7][ 4/48]\tTime  32.41 ( 33.87)\tData 0.0170 (0.3200)\tLoss (L1) 10.860 (10.957)\n",
      "2022-01-20 08:16:22,528 | ===> Batch : 5\n",
      "2022-01-20 08:16:22,529 | FDS disable\n",
      "2022-01-20 08:16:34,174 | Calculate Loss\n",
      "2022-01-20 08:16:34,175 | Update Loss\n",
      "2022-01-20 08:16:34,179 | Backward\n",
      "2022-01-20 08:16:54,973 | Epoch: [7][ 5/48]\tTime  32.46 ( 33.59)\tData 0.0127 (0.2586)\tLoss (L1) 11.440 (11.054)\n",
      "2022-01-20 08:16:54,986 | ===> Batch : 6\n",
      "2022-01-20 08:16:54,987 | FDS disable\n",
      "2022-01-20 08:17:06,637 | Calculate Loss\n",
      "2022-01-20 08:17:06,638 | Update Loss\n",
      "2022-01-20 08:17:06,641 | Backward\n",
      "2022-01-20 08:17:27,172 | Epoch: [7][ 6/48]\tTime  32.20 ( 33.35)\tData 0.0131 (0.2177)\tLoss (L1) 10.112 (10.897)\n",
      "2022-01-20 08:17:27,191 | ===> Batch : 7\n",
      "2022-01-20 08:17:27,192 | FDS disable\n",
      "2022-01-20 08:17:38,887 | Calculate Loss\n",
      "2022-01-20 08:17:38,889 | Update Loss\n",
      "2022-01-20 08:17:38,892 | Backward\n",
      "2022-01-20 08:18:00,401 | Epoch: [7][ 7/48]\tTime  33.23 ( 33.34)\tData 0.0196 (0.1894)\tLoss (L1) 12.248 (11.090)\n",
      "2022-01-20 08:18:00,414 | ===> Batch : 8\n",
      "2022-01-20 08:18:00,415 | FDS disable\n",
      "2022-01-20 08:18:12,119 | Calculate Loss\n",
      "2022-01-20 08:18:12,121 | Update Loss\n",
      "2022-01-20 08:18:12,125 | Backward\n",
      "2022-01-20 08:18:32,301 | Epoch: [7][ 8/48]\tTime  31.90 ( 33.16)\tData 0.0129 (0.1673)\tLoss (L1) 10.387 (11.002)\n",
      "2022-01-20 08:18:32,320 | ===> Batch : 9\n",
      "2022-01-20 08:18:32,321 | FDS disable\n",
      "2022-01-20 08:18:44,061 | Calculate Loss\n",
      "2022-01-20 08:18:44,063 | Update Loss\n",
      "2022-01-20 08:18:44,066 | Backward\n",
      "2022-01-20 08:19:05,504 | Epoch: [7][ 9/48]\tTime  33.20 ( 33.16)\tData 0.0196 (0.1509)\tLoss (L1) 9.533 (10.839)\n",
      "2022-01-20 08:19:05,518 | ===> Batch : 10\n",
      "2022-01-20 08:19:05,519 | FDS disable\n",
      "2022-01-20 08:19:17,209 | Calculate Loss\n",
      "2022-01-20 08:19:17,211 | Update Loss\n",
      "2022-01-20 08:19:17,215 | Backward\n",
      "2022-01-20 08:19:37,996 | Epoch: [7][10/48]\tTime  32.49 ( 33.09)\tData 0.0141 (0.1372)\tLoss (L1) 11.694 (10.924)\n",
      "2022-01-20 08:19:38,009 | ===> Batch : 11\n",
      "2022-01-20 08:19:38,009 | FDS disable\n",
      "2022-01-20 08:19:49,623 | Calculate Loss\n",
      "2022-01-20 08:19:49,624 | Update Loss\n",
      "2022-01-20 08:19:49,628 | Backward\n",
      "2022-01-20 08:20:10,189 | Epoch: [7][11/48]\tTime  32.19 ( 33.01)\tData 0.0128 (0.1259)\tLoss (L1) 13.005 (11.113)\n",
      "2022-01-20 08:20:10,204 | ===> Batch : 12\n",
      "2022-01-20 08:20:10,205 | FDS disable\n",
      "2022-01-20 08:20:21,873 | Calculate Loss\n",
      "2022-01-20 08:20:21,874 | Update Loss\n",
      "2022-01-20 08:20:21,877 | Backward\n",
      "2022-01-20 08:20:42,151 | Epoch: [7][12/48]\tTime  31.96 ( 32.93)\tData 0.0149 (0.1167)\tLoss (L1) 10.886 (11.094)\n",
      "2022-01-20 08:20:42,164 | ===> Batch : 13\n",
      "2022-01-20 08:20:42,164 | FDS disable\n",
      "2022-01-20 08:20:53,878 | Calculate Loss\n",
      "2022-01-20 08:20:53,880 | Update Loss\n",
      "2022-01-20 08:20:53,883 | Backward\n",
      "2022-01-20 08:21:15,379 | Epoch: [7][13/48]\tTime  33.23 ( 32.95)\tData 0.0124 (0.1086)\tLoss (L1) 10.004 (11.011)\n",
      "2022-01-20 08:21:15,392 | ===> Batch : 14\n",
      "2022-01-20 08:21:15,392 | FDS disable\n",
      "2022-01-20 08:21:27,128 | Calculate Loss\n",
      "2022-01-20 08:21:27,129 | Update Loss\n",
      "2022-01-20 08:21:27,133 | Backward\n",
      "2022-01-20 08:21:47,512 | Epoch: [7][14/48]\tTime  32.13 ( 32.89)\tData 0.0127 (0.1018)\tLoss (L1) 9.211 (10.882)\n",
      "2022-01-20 08:21:47,525 | ===> Batch : 15\n",
      "2022-01-20 08:21:47,526 | FDS disable\n",
      "2022-01-20 08:21:59,275 | Calculate Loss\n",
      "2022-01-20 08:21:59,276 | Update Loss\n",
      "2022-01-20 08:21:59,280 | Backward\n",
      "2022-01-20 08:22:20,734 | Epoch: [7][15/48]\tTime  33.22 ( 32.91)\tData 0.0130 (0.0959)\tLoss (L1) 11.676 (10.935)\n",
      "2022-01-20 08:22:20,747 | ===> Batch : 16\n",
      "2022-01-20 08:22:20,748 | FDS disable\n",
      "2022-01-20 08:22:32,424 | Calculate Loss\n",
      "2022-01-20 08:22:32,426 | Update Loss\n",
      "2022-01-20 08:22:32,430 | Backward\n",
      "2022-01-20 08:22:53,255 | Epoch: [7][16/48]\tTime  32.52 ( 32.89)\tData 0.0129 (0.0907)\tLoss (L1) 11.979 (11.000)\n",
      "2022-01-20 08:22:53,269 | ===> Batch : 17\n",
      "2022-01-20 08:22:53,269 | FDS disable\n",
      "2022-01-20 08:23:04,966 | Calculate Loss\n",
      "2022-01-20 08:23:04,967 | Update Loss\n",
      "2022-01-20 08:23:04,970 | Backward\n",
      "2022-01-20 08:23:26,510 | Epoch: [7][17/48]\tTime  33.25 ( 32.91)\tData 0.0139 (0.0862)\tLoss (L1) 11.495 (11.029)\n",
      "2022-01-20 08:23:26,524 | ===> Batch : 18\n",
      "2022-01-20 08:23:26,524 | FDS disable\n",
      "2022-01-20 08:23:38,252 | Calculate Loss\n",
      "2022-01-20 08:23:38,253 | Update Loss\n",
      "2022-01-20 08:23:38,257 | Backward\n",
      "2022-01-20 08:23:59,075 | Epoch: [7][18/48]\tTime  32.57 ( 32.89)\tData 0.0137 (0.0821)\tLoss (L1) 10.356 (10.992)\n",
      "2022-01-20 08:23:59,089 | ===> Batch : 19\n",
      "2022-01-20 08:23:59,089 | FDS disable\n",
      "2022-01-20 08:24:10,835 | Calculate Loss\n",
      "2022-01-20 08:24:10,836 | Update Loss\n",
      "2022-01-20 08:24:10,839 | Backward\n",
      "2022-01-20 08:24:31,857 | Epoch: [7][19/48]\tTime  32.78 ( 32.88)\tData 0.0141 (0.0786)\tLoss (L1) 13.395 (11.118)\n",
      "2022-01-20 08:24:31,871 | ===> Batch : 20\n",
      "2022-01-20 08:24:31,872 | FDS disable\n",
      "2022-01-20 08:24:43,561 | Calculate Loss\n",
      "2022-01-20 08:24:43,562 | Update Loss\n",
      "2022-01-20 08:24:43,566 | Backward\n",
      "2022-01-20 08:25:04,876 | Epoch: [7][20/48]\tTime  33.02 ( 32.89)\tData 0.0142 (0.0753)\tLoss (L1) 11.209 (11.123)\n",
      "2022-01-20 08:25:04,890 | ===> Batch : 21\n",
      "2022-01-20 08:25:04,891 | FDS disable\n",
      "2022-01-20 08:25:16,585 | Calculate Loss\n",
      "2022-01-20 08:25:16,587 | Update Loss\n",
      "2022-01-20 08:25:16,591 | Backward\n",
      "2022-01-20 08:25:37,840 | Epoch: [7][21/48]\tTime  32.96 ( 32.89)\tData 0.0144 (0.0724)\tLoss (L1) 10.920 (11.113)\n",
      "2022-01-20 08:25:37,855 | ===> Batch : 22\n",
      "2022-01-20 08:25:37,856 | FDS disable\n",
      "2022-01-20 08:25:49,552 | Calculate Loss\n",
      "2022-01-20 08:25:49,553 | Update Loss\n",
      "2022-01-20 08:25:49,557 | Backward\n",
      "2022-01-20 08:26:10,293 | Epoch: [7][22/48]\tTime  32.45 ( 32.87)\tData 0.0152 (0.0698)\tLoss (L1) 12.824 (11.191)\n",
      "2022-01-20 08:26:10,307 | ===> Batch : 23\n",
      "2022-01-20 08:26:10,307 | FDS disable\n",
      "2022-01-20 08:26:22,066 | Calculate Loss\n",
      "2022-01-20 08:26:22,068 | Update Loss\n",
      "2022-01-20 08:26:22,071 | Backward\n",
      "2022-01-20 08:26:43,474 | Epoch: [7][23/48]\tTime  33.18 ( 32.89)\tData 0.0136 (0.0674)\tLoss (L1) 8.999 (11.096)\n",
      "2022-01-20 08:26:43,488 | ===> Batch : 24\n",
      "2022-01-20 08:26:43,488 | FDS disable\n",
      "2022-01-20 08:26:55,096 | Calculate Loss\n",
      "2022-01-20 08:26:55,097 | Update Loss\n",
      "2022-01-20 08:26:55,101 | Backward\n",
      "2022-01-20 08:27:16,416 | Epoch: [7][24/48]\tTime  32.94 ( 32.89)\tData 0.0138 (0.0652)\tLoss (L1) 10.814 (11.084)\n",
      "2022-01-20 08:27:16,430 | ===> Batch : 25\n",
      "2022-01-20 08:27:16,430 | FDS disable\n",
      "2022-01-20 08:27:28,103 | Calculate Loss\n",
      "2022-01-20 08:27:28,104 | Update Loss\n",
      "2022-01-20 08:27:28,109 | Backward\n",
      "2022-01-20 08:27:48,917 | Epoch: [7][25/48]\tTime  32.50 ( 32.87)\tData 0.0140 (0.0631)\tLoss (L1) 9.782 (11.032)\n",
      "2022-01-20 08:27:48,936 | ===> Batch : 26\n",
      "2022-01-20 08:27:48,937 | FDS disable\n",
      "2022-01-20 08:28:00,666 | Calculate Loss\n",
      "2022-01-20 08:28:00,668 | Update Loss\n",
      "2022-01-20 08:28:00,672 | Backward\n",
      "2022-01-20 08:28:21,345 | Epoch: [7][26/48]\tTime  32.43 ( 32.86)\tData 0.0197 (0.0614)\tLoss (L1) 11.643 (11.055)\n",
      "2022-01-20 08:28:21,359 | ===> Batch : 27\n",
      "2022-01-20 08:28:21,359 | FDS disable\n",
      "2022-01-20 08:28:33,079 | Calculate Loss\n",
      "2022-01-20 08:28:33,080 | Update Loss\n",
      "2022-01-20 08:28:33,084 | Backward\n",
      "2022-01-20 08:28:54,269 | Epoch: [7][27/48]\tTime  32.92 ( 32.86)\tData 0.0137 (0.0597)\tLoss (L1) 10.214 (11.024)\n",
      "2022-01-20 08:28:54,283 | ===> Batch : 28\n",
      "2022-01-20 08:28:54,284 | FDS disable\n",
      "2022-01-20 08:29:06,045 | Calculate Loss\n",
      "2022-01-20 08:29:06,047 | Update Loss\n",
      "2022-01-20 08:29:06,051 | Backward\n",
      "2022-01-20 08:29:26,895 | Epoch: [7][28/48]\tTime  32.63 ( 32.85)\tData 0.0141 (0.0580)\tLoss (L1) 10.981 (11.023)\n",
      "2022-01-20 08:29:26,909 | ===> Batch : 29\n",
      "2022-01-20 08:29:26,909 | FDS disable\n",
      "2022-01-20 08:29:38,612 | Calculate Loss\n",
      "2022-01-20 08:29:38,613 | Update Loss\n",
      "2022-01-20 08:29:38,616 | Backward\n",
      "2022-01-20 08:29:59,391 | Epoch: [7][29/48]\tTime  32.50 ( 32.84)\tData 0.0139 (0.0565)\tLoss (L1) 8.866 (10.948)\n",
      "2022-01-20 08:29:59,405 | ===> Batch : 30\n",
      "2022-01-20 08:29:59,406 | FDS disable\n",
      "2022-01-20 08:30:11,101 | Calculate Loss\n",
      "2022-01-20 08:30:11,102 | Update Loss\n",
      "2022-01-20 08:30:11,106 | Backward\n",
      "2022-01-20 08:30:31,557 | Epoch: [7][30/48]\tTime  32.17 ( 32.82)\tData 0.0141 (0.0551)\tLoss (L1) 11.345 (10.962)\n",
      "2022-01-20 08:30:31,571 | ===> Batch : 31\n",
      "2022-01-20 08:30:31,572 | FDS disable\n",
      "2022-01-20 08:30:43,313 | Calculate Loss\n",
      "2022-01-20 08:30:43,315 | Update Loss\n",
      "2022-01-20 08:30:43,319 | Backward\n",
      "2022-01-20 08:31:04,447 | Epoch: [7][31/48]\tTime  32.89 ( 32.82)\tData 0.0144 (0.0538)\tLoss (L1) 11.010 (10.963)\n",
      "2022-01-20 08:31:04,463 | ===> Batch : 32\n",
      "2022-01-20 08:31:04,463 | FDS disable\n",
      "2022-01-20 08:31:16,095 | Calculate Loss\n",
      "2022-01-20 08:31:16,096 | Update Loss\n",
      "2022-01-20 08:31:16,100 | Backward\n",
      "2022-01-20 08:31:36,901 | Epoch: [7][32/48]\tTime  32.45 ( 32.81)\tData 0.0161 (0.0526)\tLoss (L1) 12.073 (10.998)\n",
      "2022-01-20 08:31:36,915 | ===> Batch : 33\n",
      "2022-01-20 08:31:36,915 | FDS disable\n",
      "2022-01-20 08:31:48,531 | Calculate Loss\n",
      "2022-01-20 08:31:48,532 | Update Loss\n",
      "2022-01-20 08:31:48,536 | Backward\n",
      "2022-01-20 08:32:09,986 | Epoch: [7][33/48]\tTime  33.09 ( 32.82)\tData 0.0143 (0.0515)\tLoss (L1) 10.540 (10.984)\n",
      "2022-01-20 08:32:10,000 | ===> Batch : 34\n",
      "2022-01-20 08:32:10,001 | FDS disable\n",
      "2022-01-20 08:32:21,666 | Calculate Loss\n",
      "2022-01-20 08:32:21,667 | Update Loss\n",
      "2022-01-20 08:32:21,671 | Backward\n",
      "2022-01-20 08:32:41,977 | Epoch: [7][34/48]\tTime  31.99 ( 32.79)\tData 0.0139 (0.0504)\tLoss (L1) 10.926 (10.982)\n",
      "2022-01-20 08:32:41,990 | ===> Batch : 35\n",
      "2022-01-20 08:32:41,991 | FDS disable\n",
      "2022-01-20 08:32:53,616 | Calculate Loss\n",
      "2022-01-20 08:32:53,618 | Update Loss\n",
      "2022-01-20 08:32:53,621 | Backward\n",
      "2022-01-20 08:33:14,339 | Epoch: [7][35/48]\tTime  32.36 ( 32.78)\tData 0.0134 (0.0493)\tLoss (L1) 13.765 (11.062)\n",
      "2022-01-20 08:33:14,354 | ===> Batch : 36\n",
      "2022-01-20 08:33:14,354 | FDS disable\n",
      "2022-01-20 08:33:25,976 | Calculate Loss\n",
      "2022-01-20 08:33:25,977 | Update Loss\n",
      "2022-01-20 08:33:25,981 | Backward\n",
      "2022-01-20 08:33:47,496 | Epoch: [7][36/48]\tTime  33.16 ( 32.79)\tData 0.0143 (0.0483)\tLoss (L1) 12.053 (11.089)\n",
      "2022-01-20 08:33:47,510 | ===> Batch : 37\n",
      "2022-01-20 08:33:47,511 | FDS disable\n",
      "2022-01-20 08:33:59,143 | Calculate Loss\n",
      "2022-01-20 08:33:59,144 | Update Loss\n",
      "2022-01-20 08:33:59,149 | Backward\n",
      "2022-01-20 08:34:19,438 | Epoch: [7][37/48]\tTime  31.94 ( 32.77)\tData 0.0139 (0.0474)\tLoss (L1) 10.911 (11.084)\n",
      "2022-01-20 08:34:19,452 | ===> Batch : 38\n",
      "2022-01-20 08:34:19,453 | FDS disable\n",
      "2022-01-20 08:34:31,015 | Calculate Loss\n",
      "2022-01-20 08:34:31,016 | Update Loss\n",
      "2022-01-20 08:34:31,020 | Backward\n",
      "2022-01-20 08:34:51,686 | Epoch: [7][38/48]\tTime  32.25 ( 32.75)\tData 0.0140 (0.0465)\tLoss (L1) 16.111 (11.217)\n",
      "2022-01-20 08:34:51,703 | ===> Batch : 39\n",
      "2022-01-20 08:34:51,703 | FDS disable\n",
      "2022-01-20 08:35:03,300 | Calculate Loss\n",
      "2022-01-20 08:35:03,301 | Update Loss\n",
      "2022-01-20 08:35:03,305 | Backward\n",
      "2022-01-20 08:35:24,117 | Epoch: [7][39/48]\tTime  32.43 ( 32.75)\tData 0.0171 (0.0458)\tLoss (L1) 10.719 (11.204)\n",
      "2022-01-20 08:35:24,131 | ===> Batch : 40\n",
      "2022-01-20 08:35:24,131 | FDS disable\n",
      "2022-01-20 08:35:35,686 | Calculate Loss\n",
      "2022-01-20 08:35:35,688 | Update Loss\n",
      "2022-01-20 08:35:35,691 | Backward\n",
      "2022-01-20 08:35:56,865 | Epoch: [7][40/48]\tTime  32.75 ( 32.75)\tData 0.0141 (0.0450)\tLoss (L1) 10.167 (11.178)\n",
      "2022-01-20 08:35:56,881 | ===> Batch : 41\n",
      "2022-01-20 08:35:56,882 | FDS disable\n",
      "2022-01-20 08:36:08,433 | Calculate Loss\n",
      "2022-01-20 08:36:08,435 | Update Loss\n",
      "2022-01-20 08:36:08,439 | Backward\n",
      "2022-01-20 08:36:29,456 | Epoch: [7][41/48]\tTime  32.59 ( 32.74)\tData 0.0160 (0.0443)\tLoss (L1) 11.926 (11.196)\n",
      "2022-01-20 08:36:29,470 | ===> Batch : 42\n",
      "2022-01-20 08:36:29,471 | FDS disable\n",
      "2022-01-20 08:36:41,075 | Calculate Loss\n",
      "2022-01-20 08:36:41,076 | Update Loss\n",
      "2022-01-20 08:36:41,080 | Backward\n",
      "2022-01-20 08:37:01,821 | Epoch: [7][42/48]\tTime  32.36 ( 32.73)\tData 0.0139 (0.0435)\tLoss (L1) 12.386 (11.225)\n",
      "2022-01-20 08:37:01,836 | ===> Batch : 43\n",
      "2022-01-20 08:37:01,836 | FDS disable\n",
      "2022-01-20 08:37:13,379 | Calculate Loss\n",
      "2022-01-20 08:37:13,380 | Update Loss\n",
      "2022-01-20 08:37:13,383 | Backward\n",
      "2022-01-20 08:37:34,307 | Epoch: [7][43/48]\tTime  32.49 ( 32.73)\tData 0.0142 (0.0429)\tLoss (L1) 11.606 (11.234)\n",
      "2022-01-20 08:37:34,321 | ===> Batch : 44\n",
      "2022-01-20 08:37:34,322 | FDS disable\n",
      "2022-01-20 08:37:45,893 | Calculate Loss\n",
      "2022-01-20 08:37:45,895 | Update Loss\n",
      "2022-01-20 08:37:45,898 | Backward\n",
      "2022-01-20 08:38:07,291 | Epoch: [7][44/48]\tTime  32.98 ( 32.73)\tData 0.0138 (0.0422)\tLoss (L1) 11.258 (11.234)\n",
      "2022-01-20 08:38:07,304 | ===> Batch : 45\n",
      "2022-01-20 08:38:07,305 | FDS disable\n",
      "2022-01-20 08:38:18,921 | Calculate Loss\n",
      "2022-01-20 08:38:18,922 | Update Loss\n",
      "2022-01-20 08:38:18,926 | Backward\n",
      "2022-01-20 08:38:39,728 | Epoch: [7][45/48]\tTime  32.44 ( 32.73)\tData 0.0134 (0.0416)\tLoss (L1) 11.835 (11.247)\n",
      "2022-01-20 08:38:39,742 | ===> Batch : 46\n",
      "2022-01-20 08:38:39,742 | FDS disable\n",
      "2022-01-20 08:38:51,247 | Calculate Loss\n",
      "2022-01-20 08:38:51,249 | Update Loss\n",
      "2022-01-20 08:38:51,253 | Backward\n",
      "2022-01-20 08:39:12,634 | Epoch: [7][46/48]\tTime  32.91 ( 32.73)\tData 0.0141 (0.0410)\tLoss (L1) 9.709 (11.214)\n",
      "2022-01-20 08:39:12,648 | ===> Batch : 47\n",
      "2022-01-20 08:39:12,648 | FDS disable\n",
      "2022-01-20 08:39:24,236 | Calculate Loss\n",
      "2022-01-20 08:39:24,238 | Update Loss\n",
      "2022-01-20 08:39:24,242 | Backward\n",
      "2022-01-20 08:39:45,202 | Epoch: [7][47/48]\tTime  32.57 ( 32.73)\tData 0.0138 (0.0404)\tLoss (L1) 10.527 (11.199)\n",
      "2022-01-20 08:39:45,217 | ===> Batch : 48\n",
      "2022-01-20 08:39:45,218 | FDS disable\n",
      "2022-01-20 08:39:53,303 | Calculate Loss\n",
      "2022-01-20 08:39:53,304 | Update Loss\n",
      "2022-01-20 08:39:53,307 | Backward\n",
      "2022-01-20 08:40:07,626 | Epoch: [7][48/48]\tTime  22.42 ( 32.51)\tData 0.0150 (0.0399)\tLoss (L1) 10.083 (11.183)\n",
      "2022-01-20 08:40:19,561 | Val: [0/9]\tTime 11.768 (11.768)\tLoss (MSE) 230.798 (230.798)\tLoss (L1) 12.037 (12.037)\n",
      "2022-01-20 08:40:28,630 | Val: [1/9]\tTime  9.069 (10.419)\tLoss (MSE) 223.333 (227.066)\tLoss (L1) 11.807 (11.922)\n",
      "2022-01-20 08:40:37,696 | Val: [2/9]\tTime  9.066 ( 9.968)\tLoss (MSE) 231.793 (228.642)\tLoss (L1) 12.324 (12.056)\n",
      "2022-01-20 08:40:46,840 | Val: [3/9]\tTime  9.145 ( 9.762)\tLoss (MSE) 244.243 (232.542)\tLoss (L1) 12.534 (12.176)\n",
      "2022-01-20 08:40:55,944 | Val: [4/9]\tTime  9.104 ( 9.630)\tLoss (MSE) 251.903 (236.414)\tLoss (L1) 12.829 (12.306)\n",
      "2022-01-20 08:41:05,072 | Val: [5/9]\tTime  9.127 ( 9.547)\tLoss (MSE) 280.732 (243.800)\tLoss (L1) 13.403 (12.489)\n",
      "2022-01-20 08:41:14,202 | Val: [6/9]\tTime  9.131 ( 9.487)\tLoss (MSE) 217.992 (240.113)\tLoss (L1) 11.923 (12.408)\n",
      "2022-01-20 08:41:23,298 | Val: [7/9]\tTime  9.096 ( 9.438)\tLoss (MSE) 270.666 (243.933)\tLoss (L1) 12.709 (12.446)\n",
      "2022-01-20 08:41:26,558 | Val: [8/9]\tTime  3.260 ( 8.752)\tLoss (MSE) 236.497 (243.613)\tLoss (L1) 11.862 (12.421)\n",
      "2022-01-20 08:41:26,696 |  * Overall: MSE 243.613\tL1 12.421\tG-Mean 8.256\n",
      "2022-01-20 08:41:26,697 |  * Many: MSE 148.894\tL1 9.601\tG-Mean 6.285\n",
      "2022-01-20 08:41:26,697 |  * Median: MSE 445.576\tL1 18.439\tG-Mean 15.030\n",
      "2022-01-20 08:41:26,697 |  * Low: MSE 579.667\tL1 22.408\tG-Mean 20.715\n",
      "2022-01-20 08:41:26,702 | Best L1 Loss: 12.036\n",
      "2022-01-20 08:41:27,967 | Epoch #7: Train loss [11.1832]; Val loss: MSE [243.6129], L1 [12.4208], G-Mean [8.2560]\n",
      "2022-01-20 08:41:27,971 | Training...\n",
      "2022-01-20 08:41:27,972 | Load train loader\n",
      "2022-01-20 08:41:29,256 | ===> Batch : 1\n",
      "2022-01-20 08:41:29,259 | FDS disable\n",
      "2022-01-20 08:41:43,973 | Calculate Loss\n",
      "2022-01-20 08:41:43,975 | Update Loss\n",
      "2022-01-20 08:41:44,107 | Backward\n",
      "2022-01-20 08:42:06,066 | Epoch: [8][ 1/48]\tTime  38.09 ( 38.09)\tData 1.2841 (1.2841)\tLoss (L1) 10.780 (10.780)\n",
      "2022-01-20 08:42:06,080 | ===> Batch : 2\n",
      "2022-01-20 08:42:06,081 | FDS disable\n",
      "2022-01-20 08:42:17,809 | Calculate Loss\n",
      "2022-01-20 08:42:17,811 | Update Loss\n",
      "2022-01-20 08:42:17,814 | Backward\n",
      "2022-01-20 08:42:38,511 | Epoch: [8][ 2/48]\tTime  32.45 ( 35.27)\tData 0.0147 (0.6494)\tLoss (L1) 12.034 (11.407)\n",
      "2022-01-20 08:42:38,524 | ===> Batch : 3\n",
      "2022-01-20 08:42:38,525 | FDS disable\n",
      "2022-01-20 08:42:50,259 | Calculate Loss\n",
      "2022-01-20 08:42:50,260 | Update Loss\n",
      "2022-01-20 08:42:50,264 | Backward\n",
      "2022-01-20 08:43:10,967 | Epoch: [8][ 3/48]\tTime  32.46 ( 34.33)\tData 0.0129 (0.4372)\tLoss (L1) 9.748 (10.854)\n",
      "2022-01-20 08:43:10,980 | ===> Batch : 4\n",
      "2022-01-20 08:43:10,981 | FDS disable\n",
      "2022-01-20 08:43:22,703 | Calculate Loss\n",
      "2022-01-20 08:43:22,704 | Update Loss\n",
      "2022-01-20 08:43:22,708 | Backward\n",
      "2022-01-20 08:43:43,960 | Epoch: [8][ 4/48]\tTime  32.99 ( 34.00)\tData 0.0131 (0.3312)\tLoss (L1) 12.054 (11.154)\n",
      "2022-01-20 08:43:43,976 | ===> Batch : 5\n",
      "2022-01-20 08:43:43,977 | FDS disable\n",
      "2022-01-20 08:43:55,677 | Calculate Loss\n",
      "2022-01-20 08:43:55,678 | Update Loss\n",
      "2022-01-20 08:43:55,682 | Backward\n",
      "2022-01-20 08:44:16,927 | Epoch: [8][ 5/48]\tTime  32.97 ( 33.79)\tData 0.0161 (0.2682)\tLoss (L1) 9.708 (10.865)\n",
      "2022-01-20 08:44:16,940 | ===> Batch : 6\n",
      "2022-01-20 08:44:16,941 | FDS disable\n",
      "2022-01-20 08:44:28,609 | Calculate Loss\n",
      "2022-01-20 08:44:28,610 | Update Loss\n",
      "2022-01-20 08:44:28,613 | Backward\n",
      "2022-01-20 08:44:48,692 | Epoch: [8][ 6/48]\tTime  31.77 ( 33.45)\tData 0.0132 (0.2257)\tLoss (L1) 11.586 (10.985)\n",
      "2022-01-20 08:44:48,706 | ===> Batch : 7\n",
      "2022-01-20 08:44:48,707 | FDS disable\n",
      "2022-01-20 08:45:00,379 | Calculate Loss\n",
      "2022-01-20 08:45:00,382 | Update Loss\n",
      "2022-01-20 08:45:00,386 | Backward\n",
      "2022-01-20 08:45:21,403 | Epoch: [8][ 7/48]\tTime  32.71 ( 33.35)\tData 0.0142 (0.1955)\tLoss (L1) 8.900 (10.687)\n",
      "2022-01-20 08:45:21,416 | ===> Batch : 8\n",
      "2022-01-20 08:45:21,417 | FDS disable\n",
      "2022-01-20 08:45:33,058 | Calculate Loss\n",
      "2022-01-20 08:45:33,059 | Update Loss\n",
      "2022-01-20 08:45:33,062 | Backward\n",
      "2022-01-20 08:45:53,586 | Epoch: [8][ 8/48]\tTime  32.18 ( 33.20)\tData 0.0134 (0.1727)\tLoss (L1) 10.674 (10.686)\n",
      "2022-01-20 08:45:53,612 | ===> Batch : 9\n",
      "2022-01-20 08:45:53,612 | FDS disable\n",
      "2022-01-20 08:46:05,287 | Calculate Loss\n",
      "2022-01-20 08:46:05,289 | Update Loss\n",
      "2022-01-20 08:46:05,292 | Backward\n",
      "2022-01-20 08:46:26,443 | Epoch: [8][ 9/48]\tTime  32.86 ( 33.16)\tData 0.0259 (0.1564)\tLoss (L1) 9.427 (10.546)\n",
      "2022-01-20 08:46:26,457 | ===> Batch : 10\n",
      "2022-01-20 08:46:26,458 | FDS disable\n",
      "2022-01-20 08:46:38,203 | Calculate Loss\n",
      "2022-01-20 08:46:38,205 | Update Loss\n",
      "2022-01-20 08:46:38,209 | Backward\n",
      "2022-01-20 08:46:59,159 | Epoch: [8][10/48]\tTime  32.72 ( 33.12)\tData 0.0143 (0.1422)\tLoss (L1) 10.312 (10.522)\n",
      "2022-01-20 08:46:59,174 | ===> Batch : 11\n",
      "2022-01-20 08:46:59,175 | FDS disable\n",
      "2022-01-20 08:47:10,844 | Calculate Loss\n",
      "2022-01-20 08:47:10,845 | Update Loss\n",
      "2022-01-20 08:47:10,849 | Backward\n",
      "2022-01-20 08:47:32,085 | Epoch: [8][11/48]\tTime  32.93 ( 33.10)\tData 0.0154 (0.1306)\tLoss (L1) 10.883 (10.555)\n",
      "2022-01-20 08:47:32,097 | ===> Batch : 12\n",
      "2022-01-20 08:47:32,098 | FDS disable\n",
      "2022-01-20 08:47:43,856 | Calculate Loss\n",
      "2022-01-20 08:47:43,857 | Update Loss\n",
      "2022-01-20 08:47:43,860 | Backward\n",
      "2022-01-20 08:48:04,128 | Epoch: [8][12/48]\tTime  32.04 ( 33.01)\tData 0.0124 (0.1208)\tLoss (L1) 9.823 (10.494)\n",
      "2022-01-20 08:48:04,141 | ===> Batch : 13\n",
      "2022-01-20 08:48:04,141 | FDS disable\n",
      "2022-01-20 08:48:15,844 | Calculate Loss\n",
      "2022-01-20 08:48:15,845 | Update Loss\n",
      "2022-01-20 08:48:15,848 | Backward\n",
      "2022-01-20 08:48:36,489 | Epoch: [8][13/48]\tTime  32.36 ( 32.96)\tData 0.0125 (0.1125)\tLoss (L1) 12.752 (10.668)\n",
      "2022-01-20 08:48:36,502 | ===> Batch : 14\n",
      "2022-01-20 08:48:36,503 | FDS disable\n",
      "2022-01-20 08:48:48,316 | Calculate Loss\n",
      "2022-01-20 08:48:48,317 | Update Loss\n",
      "2022-01-20 08:48:48,321 | Backward\n",
      "2022-01-20 08:49:09,541 | Epoch: [8][14/48]\tTime  33.05 ( 32.97)\tData 0.0127 (0.1053)\tLoss (L1) 10.938 (10.687)\n",
      "2022-01-20 08:49:09,554 | ===> Batch : 15\n",
      "2022-01-20 08:49:09,555 | FDS disable\n",
      "2022-01-20 08:49:21,354 | Calculate Loss\n",
      "2022-01-20 08:49:21,355 | Update Loss\n",
      "2022-01-20 08:49:21,359 | Backward\n",
      "2022-01-20 08:49:42,608 | Epoch: [8][15/48]\tTime  33.07 ( 32.98)\tData 0.0131 (0.0992)\tLoss (L1) 10.828 (10.696)\n",
      "2022-01-20 08:49:42,623 | ===> Batch : 16\n",
      "2022-01-20 08:49:42,624 | FDS disable\n",
      "2022-01-20 08:49:54,347 | Calculate Loss\n",
      "2022-01-20 08:49:54,348 | Update Loss\n",
      "2022-01-20 08:49:54,352 | Backward\n",
      "2022-01-20 08:50:15,042 | Epoch: [8][16/48]\tTime  32.43 ( 32.94)\tData 0.0145 (0.0939)\tLoss (L1) 11.344 (10.737)\n",
      "2022-01-20 08:50:15,056 | ===> Batch : 17\n",
      "2022-01-20 08:50:15,056 | FDS disable\n",
      "2022-01-20 08:50:26,763 | Calculate Loss\n",
      "2022-01-20 08:50:26,765 | Update Loss\n",
      "2022-01-20 08:50:26,769 | Backward\n",
      "2022-01-20 08:50:48,105 | Epoch: [8][17/48]\tTime  33.06 ( 32.95)\tData 0.0138 (0.0892)\tLoss (L1) 11.358 (10.773)\n",
      "2022-01-20 08:50:48,119 | ===> Batch : 18\n",
      "2022-01-20 08:50:48,120 | FDS disable\n",
      "2022-01-20 08:50:59,819 | Calculate Loss\n",
      "2022-01-20 08:50:59,820 | Update Loss\n",
      "2022-01-20 08:50:59,824 | Backward\n",
      "2022-01-20 08:51:20,732 | Epoch: [8][18/48]\tTime  32.63 ( 32.93)\tData 0.0140 (0.0850)\tLoss (L1) 11.686 (10.824)\n",
      "2022-01-20 08:51:20,747 | ===> Batch : 19\n",
      "2022-01-20 08:51:20,748 | FDS disable\n",
      "2022-01-20 08:51:32,466 | Calculate Loss\n",
      "2022-01-20 08:51:32,468 | Update Loss\n",
      "2022-01-20 08:51:32,472 | Backward\n",
      "2022-01-20 08:51:53,394 | Epoch: [8][19/48]\tTime  32.66 ( 32.92)\tData 0.0150 (0.0813)\tLoss (L1) 10.892 (10.828)\n",
      "2022-01-20 08:51:53,409 | ===> Batch : 20\n",
      "2022-01-20 08:51:53,409 | FDS disable\n",
      "2022-01-20 08:52:05,104 | Calculate Loss\n",
      "2022-01-20 08:52:05,105 | Update Loss\n",
      "2022-01-20 08:52:05,109 | Backward\n",
      "2022-01-20 08:52:25,123 | Epoch: [8][20/48]\tTime  31.73 ( 32.86)\tData 0.0150 (0.0780)\tLoss (L1) 9.204 (10.747)\n",
      "2022-01-20 08:52:25,137 | ===> Batch : 21\n",
      "2022-01-20 08:52:25,138 | FDS disable\n",
      "2022-01-20 08:52:36,961 | Calculate Loss\n",
      "2022-01-20 08:52:36,963 | Update Loss\n",
      "2022-01-20 08:52:36,967 | Backward\n",
      "2022-01-20 08:52:58,463 | Epoch: [8][21/48]\tTime  33.34 ( 32.88)\tData 0.0144 (0.0750)\tLoss (L1) 11.080 (10.762)\n",
      "2022-01-20 08:52:58,477 | ===> Batch : 22\n",
      "2022-01-20 08:52:58,478 | FDS disable\n",
      "2022-01-20 08:53:10,247 | Calculate Loss\n",
      "2022-01-20 08:53:10,248 | Update Loss\n",
      "2022-01-20 08:53:10,252 | Backward\n",
      "2022-01-20 08:53:30,669 | Epoch: [8][22/48]\tTime  32.21 ( 32.85)\tData 0.0143 (0.0722)\tLoss (L1) 9.933 (10.725)\n",
      "2022-01-20 08:53:30,684 | ===> Batch : 23\n",
      "2022-01-20 08:53:30,684 | FDS disable\n",
      "2022-01-20 08:53:42,410 | Calculate Loss\n",
      "2022-01-20 08:53:42,412 | Update Loss\n",
      "2022-01-20 08:53:42,415 | Backward\n",
      "2022-01-20 08:54:03,901 | Epoch: [8][23/48]\tTime  33.23 ( 32.87)\tData 0.0146 (0.0697)\tLoss (L1) 8.460 (10.626)\n",
      "2022-01-20 08:54:03,915 | ===> Batch : 24\n",
      "2022-01-20 08:54:03,916 | FDS disable\n",
      "2022-01-20 08:54:15,582 | Calculate Loss\n",
      "2022-01-20 08:54:15,583 | Update Loss\n",
      "2022-01-20 08:54:15,587 | Backward\n",
      "2022-01-20 08:54:36,115 | Epoch: [8][24/48]\tTime  32.21 ( 32.84)\tData 0.0143 (0.0674)\tLoss (L1) 11.573 (10.666)\n",
      "2022-01-20 08:54:36,129 | ===> Batch : 25\n",
      "2022-01-20 08:54:36,129 | FDS disable\n",
      "2022-01-20 08:54:47,846 | Calculate Loss\n",
      "2022-01-20 08:54:47,847 | Update Loss\n",
      "2022-01-20 08:54:47,851 | Backward\n",
      "2022-01-20 08:55:08,773 | Epoch: [8][25/48]\tTime  32.66 ( 32.83)\tData 0.0139 (0.0653)\tLoss (L1) 10.126 (10.644)\n",
      "2022-01-20 08:55:08,787 | ===> Batch : 26\n",
      "2022-01-20 08:55:08,787 | FDS disable\n",
      "2022-01-20 08:55:20,453 | Calculate Loss\n",
      "2022-01-20 08:55:20,455 | Update Loss\n",
      "2022-01-20 08:55:20,459 | Backward\n",
      "2022-01-20 08:55:41,175 | Epoch: [8][26/48]\tTime  32.40 ( 32.82)\tData 0.0138 (0.0633)\tLoss (L1) 10.865 (10.653)\n",
      "2022-01-20 08:55:41,191 | ===> Batch : 27\n",
      "2022-01-20 08:55:41,191 | FDS disable\n",
      "2022-01-20 08:55:52,952 | Calculate Loss\n",
      "2022-01-20 08:55:52,954 | Update Loss\n",
      "2022-01-20 08:55:52,957 | Backward\n",
      "2022-01-20 08:56:13,779 | Epoch: [8][27/48]\tTime  32.60 ( 32.81)\tData 0.0156 (0.0615)\tLoss (L1) 9.013 (10.592)\n",
      "2022-01-20 08:56:13,793 | ===> Batch : 28\n",
      "2022-01-20 08:56:13,793 | FDS disable\n",
      "2022-01-20 09:00:00,696 | Epoch: [8][34/48]\tTime  31.68 ( 32.73)\tData 0.0149 (0.0518)\tLoss (L1) 12.383 (10.665)\n",
      "2022-01-20 09:00:00,711 | ===> Batch : 35\n",
      "2022-01-20 09:00:00,712 | FDS disable\n",
      "2022-01-20 09:00:12,435 | Calculate Loss\n",
      "2022-01-20 09:00:12,436 | Update Loss\n",
      "2022-01-20 09:00:12,439 | Backward\n",
      "2022-01-20 09:00:33,367 | Epoch: [8][35/48]\tTime  32.67 ( 32.73)\tData 0.0150 (0.0508)\tLoss (L1) 12.448 (10.716)\n",
      "2022-01-20 09:00:33,380 | ===> Batch : 36\n",
      "2022-01-20 09:00:33,381 | FDS disable\n",
      "2022-01-20 09:00:44,962 | Calculate Loss\n",
      "2022-01-20 09:00:44,963 | Update Loss\n",
      "2022-01-20 09:00:44,967 | Backward\n",
      "2022-01-20 09:01:06,129 | Epoch: [8][36/48]\tTime  32.76 ( 32.73)\tData 0.0135 (0.0497)\tLoss (L1) 9.600 (10.685)\n",
      "2022-01-20 09:01:06,143 | ===> Batch : 37\n",
      "2022-01-20 09:01:06,144 | FDS disable\n",
      "2022-01-20 09:01:17,727 | Calculate Loss\n",
      "2022-01-20 09:01:17,728 | Update Loss\n",
      "2022-01-20 09:01:17,732 | Backward\n",
      "2022-01-20 09:01:38,453 | Epoch: [8][37/48]\tTime  32.32 ( 32.72)\tData 0.0147 (0.0488)\tLoss (L1) 10.486 (10.679)\n",
      "2022-01-20 09:01:38,467 | ===> Batch : 38\n",
      "2022-01-20 09:01:38,468 | FDS disable\n",
      "2022-01-20 09:01:50,065 | Calculate Loss\n",
      "2022-01-20 09:01:50,067 | Update Loss\n",
      "2022-01-20 09:01:50,071 | Backward\n",
      "2022-01-20 09:02:11,067 | Epoch: [8][38/48]\tTime  32.61 ( 32.71)\tData 0.0143 (0.0479)\tLoss (L1) 9.847 (10.657)\n",
      "2022-01-20 09:02:11,081 | ===> Batch : 39\n",
      "2022-01-20 09:02:11,081 | FDS disable\n",
      "2022-01-20 09:02:22,549 | Calculate Loss\n",
      "2022-01-20 09:02:22,551 | Update Loss\n",
      "2022-01-20 09:02:22,554 | Backward\n",
      "2022-01-20 09:02:43,771 | Epoch: [8][39/48]\tTime  32.70 ( 32.71)\tData 0.0137 (0.0470)\tLoss (L1) 10.544 (10.654)\n",
      "2022-01-20 09:02:43,785 | ===> Batch : 40\n",
      "2022-01-20 09:02:43,785 | FDS disable\n",
      "2022-01-20 09:02:55,334 | Calculate Loss\n",
      "2022-01-20 09:02:55,336 | Update Loss\n",
      "2022-01-20 09:02:55,339 | Backward\n",
      "2022-01-20 09:03:16,623 | Epoch: [8][40/48]\tTime  32.85 ( 32.72)\tData 0.0138 (0.0462)\tLoss (L1) 12.215 (10.693)\n",
      "2022-01-20 09:03:16,638 | ===> Batch : 41\n",
      "2022-01-20 09:03:16,639 | FDS disable\n",
      "2022-01-20 09:03:28,232 | Calculate Loss\n",
      "2022-01-20 09:03:28,233 | Update Loss\n",
      "2022-01-20 09:03:28,237 | Backward\n",
      "2022-01-20 09:03:49,195 | Epoch: [8][41/48]\tTime  32.57 ( 32.71)\tData 0.0149 (0.0454)\tLoss (L1) 12.586 (10.740)\n",
      "2022-01-20 09:03:49,209 | ===> Batch : 42\n",
      "2022-01-20 09:03:49,209 | FDS disable\n",
      "2022-01-20 09:04:00,739 | Calculate Loss\n",
      "2022-01-20 09:04:00,740 | Update Loss\n",
      "2022-01-20 09:04:00,744 | Backward\n",
      "2022-01-20 09:04:21,673 | Epoch: [8][42/48]\tTime  32.48 ( 32.71)\tData 0.0137 (0.0446)\tLoss (L1) 10.302 (10.729)\n",
      "2022-01-20 09:04:21,688 | ===> Batch : 43\n",
      "2022-01-20 09:04:21,689 | FDS disable\n",
      "2022-01-20 09:04:33,252 | Calculate Loss\n",
      "2022-01-20 09:04:33,253 | Update Loss\n",
      "2022-01-20 09:04:33,257 | Backward\n",
      "2022-01-20 09:04:54,346 | Epoch: [8][43/48]\tTime  32.67 ( 32.71)\tData 0.0149 (0.0440)\tLoss (L1) 11.556 (10.748)\n",
      "2022-01-20 09:04:54,360 | ===> Batch : 44\n",
      "2022-01-20 09:04:54,360 | FDS disable\n",
      "2022-01-20 09:05:05,909 | Calculate Loss\n",
      "2022-01-20 09:05:05,910 | Update Loss\n",
      "2022-01-20 09:05:05,913 | Backward\n",
      "2022-01-20 09:05:26,639 | Epoch: [8][44/48]\tTime  32.29 ( 32.70)\tData 0.0138 (0.0433)\tLoss (L1) 9.315 (10.716)\n",
      "2022-01-20 09:05:26,652 | ===> Batch : 45\n",
      "2022-01-20 09:05:26,652 | FDS disable\n",
      "2022-01-20 09:05:38,172 | Calculate Loss\n",
      "2022-01-20 09:05:38,173 | Update Loss\n",
      "2022-01-20 09:05:38,177 | Backward\n",
      "2022-01-20 09:05:59,084 | Epoch: [8][45/48]\tTime  32.45 ( 32.69)\tData 0.0134 (0.0426)\tLoss (L1) 11.214 (10.727)\n",
      "2022-01-20 09:05:59,097 | ===> Batch : 46\n",
      "2022-01-20 09:05:59,098 | FDS disable\n",
      "2022-01-20 09:06:10,663 | Calculate Loss\n",
      "2022-01-20 09:06:10,664 | Update Loss\n",
      "2022-01-20 09:06:10,668 | Backward\n",
      "2022-01-20 09:06:31,564 | Epoch: [8][46/48]\tTime  32.48 ( 32.69)\tData 0.0135 (0.0420)\tLoss (L1) 10.998 (10.733)\n",
      "2022-01-20 09:06:31,578 | ===> Batch : 47\n",
      "2022-01-20 09:06:31,578 | FDS disable\n",
      "2022-01-20 09:06:43,077 | Calculate Loss\n",
      "2022-01-20 09:06:43,078 | Update Loss\n",
      "2022-01-20 09:06:43,082 | Backward\n",
      "2022-01-20 09:07:04,018 | Epoch: [8][47/48]\tTime  32.45 ( 32.68)\tData 0.0143 (0.0414)\tLoss (L1) 11.232 (10.743)\n",
      "2022-01-20 09:07:04,032 | ===> Batch : 48\n",
      "2022-01-20 09:07:04,033 | FDS disable\n",
      "2022-01-20 09:07:12,151 | Calculate Loss\n",
      "2022-01-20 09:07:12,152 | Update Loss\n",
      "2022-01-20 09:07:12,156 | Backward\n",
      "2022-01-20 09:07:26,437 | Epoch: [8][48/48]\tTime  22.42 ( 32.47)\tData 0.0143 (0.0408)\tLoss (L1) 9.390 (10.724)\n",
      "2022-01-20 09:07:38,370 | Val: [0/9]\tTime 11.756 (11.756)\tLoss (MSE) 231.441 (231.441)\tLoss (L1) 11.833 (11.833)\n",
      "2022-01-20 09:07:47,659 | Val: [1/9]\tTime  9.290 (10.523)\tLoss (MSE) 211.893 (221.667)\tLoss (L1) 11.372 (11.602)\n",
      "2022-01-20 09:07:56,807 | Val: [2/9]\tTime  9.147 (10.064)\tLoss (MSE) 209.536 (217.623)\tLoss (L1) 11.528 (11.578)\n",
      "2022-01-20 09:08:05,889 | Val: [3/9]\tTime  9.082 ( 9.819)\tLoss (MSE) 238.729 (222.900)\tLoss (L1) 12.365 (11.774)\n",
      "2022-01-20 09:08:15,012 | Val: [4/9]\tTime  9.123 ( 9.680)\tLoss (MSE) 233.201 (224.960)\tLoss (L1) 12.276 (11.875)\n",
      "2022-01-20 09:08:24,144 | Val: [5/9]\tTime  9.132 ( 9.588)\tLoss (MSE) 258.291 (230.515)\tLoss (L1) 12.781 (12.026)\n",
      "2022-01-20 09:08:33,222 | Val: [6/9]\tTime  9.078 ( 9.515)\tLoss (MSE) 231.622 (230.673)\tLoss (L1) 12.221 (12.054)\n",
      "2022-01-20 09:08:42,301 | Val: [7/9]\tTime  9.080 ( 9.461)\tLoss (MSE) 258.541 (234.157)\tLoss (L1) 12.441 (12.102)\n",
      "2022-01-20 09:08:45,570 | Val: [8/9]\tTime  3.269 ( 8.773)\tLoss (MSE) 240.837 (234.444)\tLoss (L1) 11.769 (12.088)\n",
      "2022-01-20 09:08:45,719 |  * Overall: MSE 234.444\tL1 12.088\tG-Mean 7.998\n",
      "2022-01-20 09:08:45,720 |  * Many: MSE 152.895\tL1 9.785\tG-Mean 6.495\n",
      "2022-01-20 09:08:45,721 |  * Median: MSE 409.637\tL1 17.029\tG-Mean 12.430\n",
      "2022-01-20 09:08:45,721 |  * Low: MSE 520.148\tL1 20.169\tG-Mean 16.891\n",
      "2022-01-20 09:08:45,726 | Best L1 Loss: 12.036\n",
      "2022-01-20 09:08:46,989 | Epoch #8: Train loss [10.7239]; Val loss: MSE [234.4440], L1 [12.0877], G-Mean [7.9984]\n",
      "2022-01-20 09:08:46,991 | Training...\n",
      "2022-01-20 09:08:46,992 | Load train loader\n",
      "2022-01-20 09:08:48,169 | ===> Batch : 1\n",
      "2022-01-20 09:08:48,171 | FDS disable\n",
      "2022-01-20 09:09:02,778 | Calculate Loss\n",
      "2022-01-20 09:09:02,780 | Update Loss\n",
      "2022-01-20 09:09:02,914 | Backward\n",
      "2022-01-20 09:09:24,813 | Epoch: [9][ 1/48]\tTime  37.82 ( 37.82)\tData 1.1775 (1.1775)\tLoss (L1) 10.006 (10.006)\n",
      "2022-01-20 09:09:24,827 | ===> Batch : 2\n",
      "2022-01-20 09:09:24,827 | FDS disable\n",
      "2022-01-20 09:09:36,479 | Calculate Loss\n",
      "2022-01-20 09:09:36,480 | Update Loss\n",
      "2022-01-20 09:09:36,484 | Backward\n",
      "2022-01-20 09:09:57,856 | Epoch: [9][ 2/48]\tTime  33.04 ( 35.43)\tData 0.0136 (0.5955)\tLoss (L1) 10.654 (10.330)\n",
      "2022-01-20 09:09:57,873 | ===> Batch : 3\n",
      "2022-01-20 09:09:57,873 | FDS disable\n",
      "2022-01-20 09:10:09,566 | Calculate Loss\n",
      "2022-01-20 09:10:09,567 | Update Loss\n",
      "2022-01-20 09:10:09,570 | Backward\n",
      "2022-01-20 09:10:30,805 | Epoch: [9][ 3/48]\tTime  32.95 ( 34.60)\tData 0.0166 (0.4026)\tLoss (L1) 9.287 (9.982)\n",
      "2022-01-20 09:10:30,821 | ===> Batch : 4\n",
      "2022-01-20 09:10:30,822 | FDS disable\n",
      "2022-01-20 09:10:42,563 | Calculate Loss\n",
      "2022-01-20 09:10:42,564 | Update Loss\n",
      "2022-01-20 09:10:42,568 | Backward\n",
      "2022-01-20 09:11:03,201 | Epoch: [9][ 4/48]\tTime  32.40 ( 34.05)\tData 0.0166 (0.3061)\tLoss (L1) 10.307 (10.063)\n",
      "2022-01-20 09:11:03,214 | ===> Batch : 5\n",
      "2022-01-20 09:11:03,214 | FDS disable\n",
      "2022-01-20 09:11:14,925 | Calculate Loss\n",
      "2022-01-20 09:11:14,926 | Update Loss\n",
      "2022-01-20 09:11:14,930 | Backward\n",
      "2022-01-20 09:11:35,982 | Epoch: [9][ 5/48]\tTime  32.78 ( 33.80)\tData 0.0126 (0.2474)\tLoss (L1) 9.261 (9.903)\n",
      "2022-01-20 09:11:35,995 | ===> Batch : 6\n",
      "2022-01-20 09:11:35,996 | FDS disable\n",
      "2022-01-20 09:11:47,654 | Calculate Loss\n",
      "2022-01-20 09:11:47,655 | Update Loss\n",
      "2022-01-20 09:11:47,659 | Backward\n",
      "2022-01-20 09:12:08,141 | Epoch: [9][ 6/48]\tTime  32.16 ( 33.52)\tData 0.0130 (0.2083)\tLoss (L1) 10.577 (10.015)\n",
      "2022-01-20 09:12:08,154 | ===> Batch : 7\n",
      "2022-01-20 09:12:08,155 | FDS disable\n",
      "2022-01-20 09:12:19,869 | Calculate Loss\n",
      "2022-01-20 09:12:19,870 | Update Loss\n",
      "2022-01-20 09:12:19,874 | Backward\n",
      "2022-01-20 09:12:40,445 | Epoch: [9][ 7/48]\tTime  32.30 ( 33.35)\tData 0.0132 (0.1805)\tLoss (L1) 11.531 (10.232)\n",
      "2022-01-20 09:12:40,459 | ===> Batch : 8\n",
      "2022-01-20 09:12:40,460 | FDS disable\n",
      "2022-01-20 09:12:52,161 | Calculate Loss\n",
      "2022-01-20 09:12:52,162 | Update Loss\n",
      "2022-01-20 09:12:52,166 | Backward\n",
      "2022-01-20 09:13:13,127 | Epoch: [9][ 8/48]\tTime  32.68 ( 33.27)\tData 0.0142 (0.1597)\tLoss (L1) 9.679 (10.163)\n",
      "2022-01-20 09:13:13,147 | ===> Batch : 9\n",
      "2022-01-20 09:13:13,147 | FDS disable\n",
      "2022-01-20 09:13:24,805 | Calculate Loss\n",
      "2022-01-20 09:13:24,807 | Update Loss\n",
      "2022-01-20 09:13:24,811 | Backward\n",
      "2022-01-20 09:13:45,697 | Epoch: [9][ 9/48]\tTime  32.57 ( 33.19)\tData 0.0198 (0.1441)\tLoss (L1) 13.316 (10.513)\n",
      "2022-01-20 09:13:45,710 | ===> Batch : 10\n",
      "2022-01-20 09:13:45,710 | FDS disable\n",
      "2022-01-20 09:13:57,427 | Calculate Loss\n",
      "2022-01-20 09:13:57,428 | Update Loss\n",
      "2022-01-20 09:13:57,432 | Backward\n",
      "2022-01-20 09:14:17,654 | Epoch: [9][10/48]\tTime  31.96 ( 33.07)\tData 0.0126 (0.1310)\tLoss (L1) 9.660 (10.428)\n",
      "2022-01-20 09:14:17,666 | ===> Batch : 11\n",
      "2022-01-20 09:14:17,667 | FDS disable\n",
      "2022-01-20 09:14:29,341 | Calculate Loss\n",
      "2022-01-20 09:14:29,342 | Update Loss\n",
      "2022-01-20 09:14:29,346 | Backward\n",
      "2022-01-20 09:14:50,311 | Epoch: [9][11/48]\tTime  32.66 ( 33.03)\tData 0.0125 (0.1202)\tLoss (L1) 9.838 (10.374)\n",
      "2022-01-20 09:14:50,327 | ===> Batch : 12\n",
      "2022-01-20 09:14:50,327 | FDS disable\n",
      "2022-01-20 09:15:02,019 | Calculate Loss\n",
      "2022-01-20 09:15:02,020 | Update Loss\n",
      "2022-01-20 09:15:02,024 | Backward\n",
      "2022-01-20 09:15:23,471 | Epoch: [9][12/48]\tTime  33.16 ( 33.04)\tData 0.0157 (0.1115)\tLoss (L1) 10.698 (10.401)\n",
      "2022-01-20 09:15:23,484 | ===> Batch : 13\n",
      "2022-01-20 09:15:23,484 | FDS disable\n",
      "2022-01-20 09:15:35,201 | Calculate Loss\n",
      "2022-01-20 09:15:35,203 | Update Loss\n",
      "2022-01-20 09:15:35,207 | Backward\n",
      "2022-01-20 09:15:55,431 | Epoch: [9][13/48]\tTime  31.96 ( 32.96)\tData 0.0132 (0.1039)\tLoss (L1) 9.065 (10.298)\n",
      "2022-01-20 09:15:55,443 | ===> Batch : 14\n",
      "2022-01-20 09:15:55,444 | FDS disable\n",
      "2022-01-20 09:16:07,053 | Calculate Loss\n",
      "2022-01-20 09:16:07,054 | Update Loss\n",
      "2022-01-20 09:16:07,058 | Backward\n",
      "2022-01-20 09:16:28,469 | Epoch: [9][14/48]\tTime  33.04 ( 32.96)\tData 0.0128 (0.0974)\tLoss (L1) 10.604 (10.320)\n",
      "2022-01-20 09:16:28,482 | ===> Batch : 15\n",
      "2022-01-20 09:16:28,482 | FDS disable\n",
      "2022-01-20 09:16:40,171 | Calculate Loss\n",
      "2022-01-20 09:16:40,173 | Update Loss\n",
      "2022-01-20 09:16:40,176 | Backward\n",
      "2022-01-20 09:17:00,577 | Epoch: [9][15/48]\tTime  32.11 ( 32.91)\tData 0.0128 (0.0918)\tLoss (L1) 11.933 (10.428)\n",
      "2022-01-20 09:17:00,589 | ===> Batch : 16\n",
      "2022-01-20 09:17:00,589 | FDS disable\n",
      "2022-01-20 09:17:12,227 | Calculate Loss\n",
      "2022-01-20 09:17:12,229 | Update Loss\n",
      "2022-01-20 09:17:12,232 | Backward\n",
      "2022-01-20 09:17:33,131 | Epoch: [9][16/48]\tTime  32.55 ( 32.88)\tData 0.0123 (0.0868)\tLoss (L1) 10.719 (10.446)\n",
      "2022-01-20 09:17:33,145 | ===> Batch : 17\n",
      "2022-01-20 09:17:33,146 | FDS disable\n",
      "2022-01-20 09:17:44,804 | Calculate Loss\n",
      "2022-01-20 09:17:44,806 | Update Loss\n",
      "2022-01-20 09:17:44,810 | Backward\n",
      "2022-01-20 09:18:05,893 | Epoch: [9][17/48]\tTime  32.76 ( 32.88)\tData 0.0144 (0.0826)\tLoss (L1) 11.737 (10.522)\n",
      "2022-01-20 09:18:05,907 | ===> Batch : 18\n",
      "2022-01-20 09:18:05,908 | FDS disable\n",
      "2022-01-20 09:18:17,606 | Calculate Loss\n",
      "2022-01-20 09:18:17,608 | Update Loss\n",
      "2022-01-20 09:18:17,611 | Backward\n",
      "2022-01-20 09:18:38,866 | Epoch: [9][18/48]\tTime  32.97 ( 32.88)\tData 0.0141 (0.0788)\tLoss (L1) 9.646 (10.473)\n",
      "2022-01-20 09:18:38,880 | ===> Batch : 19\n",
      "2022-01-20 09:18:38,881 | FDS disable\n",
      "2022-01-20 09:18:50,612 | Calculate Loss\n",
      "2022-01-20 09:18:50,614 | Update Loss\n",
      "2022-01-20 09:18:50,617 | Backward\n",
      "2022-01-20 09:19:11,694 | Epoch: [9][19/48]\tTime  32.83 ( 32.88)\tData 0.0149 (0.0754)\tLoss (L1) 10.776 (10.489)\n",
      "2022-01-20 09:19:11,709 | ===> Batch : 20\n",
      "2022-01-20 09:19:11,709 | FDS disable\n",
      "2022-01-20 09:19:23,365 | Calculate Loss\n",
      "2022-01-20 09:19:23,366 | Update Loss\n",
      "2022-01-20 09:19:23,370 | Backward\n",
      "2022-01-20 09:19:44,683 | Epoch: [9][20/48]\tTime  32.99 ( 32.88)\tData 0.0142 (0.0723)\tLoss (L1) 10.064 (10.468)\n",
      "2022-01-20 09:19:44,698 | ===> Batch : 21\n",
      "2022-01-20 09:19:44,698 | FDS disable\n",
      "2022-01-20 09:19:56,401 | Calculate Loss\n",
      "2022-01-20 09:19:56,402 | Update Loss\n",
      "2022-01-20 09:19:56,405 | Backward\n",
      "2022-01-20 09:20:16,648 | Epoch: [9][21/48]\tTime  31.96 ( 32.84)\tData 0.0142 (0.0696)\tLoss (L1) 11.491 (10.517)\n",
      "2022-01-20 09:20:16,662 | ===> Batch : 22\n",
      "2022-01-20 09:20:16,663 | FDS disable\n",
      "2022-01-20 09:20:28,278 | Calculate Loss\n",
      "2022-01-20 09:20:28,279 | Update Loss\n",
      "2022-01-20 09:20:28,283 | Backward\n",
      "2022-01-20 09:20:49,247 | Epoch: [9][22/48]\tTime  32.60 ( 32.83)\tData 0.0144 (0.0671)\tLoss (L1) 11.466 (10.560)\n",
      "2022-01-20 09:20:49,260 | ===> Batch : 23\n",
      "2022-01-20 09:20:49,261 | FDS disable\n",
      "2022-01-20 09:21:00,978 | Calculate Loss\n",
      "2022-01-20 09:21:00,980 | Update Loss\n",
      "2022-01-20 09:21:00,983 | Backward\n",
      "2022-01-20 09:21:20,987 | Epoch: [9][23/48]\tTime  31.74 ( 32.78)\tData 0.0137 (0.0647)\tLoss (L1) 12.610 (10.649)\n",
      "2022-01-20 09:21:21,002 | ===> Batch : 24\n",
      "2022-01-20 09:21:21,002 | FDS disable\n",
      "2022-01-20 09:21:32,604 | Calculate Loss\n",
      "2022-01-20 09:21:32,605 | Update Loss\n",
      "2022-01-20 09:21:32,608 | Backward\n",
      "2022-01-20 09:21:53,762 | Epoch: [9][24/48]\tTime  32.77 ( 32.78)\tData 0.0144 (0.0626)\tLoss (L1) 11.214 (10.672)\n",
      "2022-01-20 09:21:53,776 | ===> Batch : 25\n",
      "2022-01-20 09:21:53,776 | FDS disable\n",
      "2022-01-20 09:22:05,389 | Calculate Loss\n",
      "2022-01-20 09:22:05,390 | Update Loss\n",
      "2022-01-20 09:22:05,394 | Backward\n",
      "2022-01-20 09:22:26,353 | Epoch: [9][25/48]\tTime  32.59 ( 32.77)\tData 0.0139 (0.0607)\tLoss (L1) 9.756 (10.636)\n",
      "2022-01-20 09:22:26,367 | ===> Batch : 26\n",
      "2022-01-20 09:22:26,367 | FDS disable\n",
      "2022-01-20 09:22:38,147 | Calculate Loss\n",
      "2022-01-20 09:22:38,148 | Update Loss\n",
      "2022-01-20 09:22:38,152 | Backward\n",
      "2022-01-20 09:22:59,052 | Epoch: [9][26/48]\tTime  32.70 ( 32.77)\tData 0.0144 (0.0589)\tLoss (L1) 9.606 (10.596)\n",
      "2022-01-20 09:22:59,066 | ===> Batch : 27\n",
      "2022-01-20 09:22:59,066 | FDS disable\n",
      "2022-01-20 09:23:10,627 | Calculate Loss\n",
      "2022-01-20 09:23:10,628 | Update Loss\n",
      "2022-01-20 09:23:10,631 | Backward\n",
      "2022-01-20 09:23:32,163 | Epoch: [9][27/48]\tTime  33.11 ( 32.78)\tData 0.0136 (0.0572)\tLoss (L1) 11.397 (10.626)\n",
      "2022-01-20 09:23:32,177 | ===> Batch : 28\n",
      "2022-01-20 09:23:32,178 | FDS disable\n",
      "2022-01-20 09:23:43,815 | Calculate Loss\n",
      "2022-01-20 09:23:43,817 | Update Loss\n",
      "2022-01-20 09:23:43,820 | Backward\n",
      "2022-01-20 09:24:04,638 | Epoch: [9][28/48]\tTime  32.47 ( 32.77)\tData 0.0138 (0.0557)\tLoss (L1) 9.645 (10.591)\n",
      "2022-01-20 09:24:04,652 | ===> Batch : 29\n",
      "2022-01-20 09:24:04,653 | FDS disable\n",
      "2022-01-20 09:24:16,366 | Calculate Loss\n",
      "2022-01-20 09:24:16,368 | Update Loss\n",
      "2022-01-20 09:24:16,372 | Backward\n",
      "2022-01-20 09:24:37,489 | Epoch: [9][29/48]\tTime  32.85 ( 32.78)\tData 0.0142 (0.0543)\tLoss (L1) 11.446 (10.620)\n",
      "2022-01-20 09:24:37,503 | ===> Batch : 30\n",
      "2022-01-20 09:24:37,504 | FDS disable\n",
      "2022-01-20 09:24:49,145 | Calculate Loss\n",
      "2022-01-20 09:24:49,146 | Update Loss\n",
      "2022-01-20 09:24:49,150 | Backward\n",
      "2022-01-20 09:25:10,201 | Epoch: [9][30/48]\tTime  32.71 ( 32.77)\tData 0.0141 (0.0529)\tLoss (L1) 12.967 (10.699)\n",
      "2022-01-20 09:25:10,215 | ===> Batch : 31\n",
      "2022-01-20 09:25:10,216 | FDS disable\n",
      "2022-01-20 09:25:21,856 | Calculate Loss\n",
      "2022-01-20 09:25:21,857 | Update Loss\n",
      "2022-01-20 09:25:21,860 | Backward\n",
      "2022-01-20 09:25:42,738 | Epoch: [9][31/48]\tTime  32.54 ( 32.77)\tData 0.0140 (0.0517)\tLoss (L1) 11.300 (10.718)\n",
      "2022-01-20 09:25:42,754 | ===> Batch : 32\n",
      "2022-01-20 09:25:42,755 | FDS disable\n",
      "2022-01-20 09:25:54,445 | Calculate Loss\n",
      "2022-01-20 09:25:54,446 | Update Loss\n",
      "2022-01-20 09:25:54,450 | Backward\n",
      "2022-01-20 09:26:15,367 | Epoch: [9][32/48]\tTime  32.63 ( 32.76)\tData 0.0168 (0.0506)\tLoss (L1) 12.200 (10.764)\n",
      "2022-01-20 09:26:15,382 | ===> Batch : 33\n",
      "2022-01-20 09:26:15,382 | FDS disable\n",
      "2022-01-20 09:26:26,925 | Calculate Loss\n",
      "2022-01-20 09:26:26,926 | Update Loss\n",
      "2022-01-20 09:26:26,929 | Backward\n",
      "2022-01-20 09:26:47,884 | Epoch: [9][33/48]\tTime  32.52 ( 32.75)\tData 0.0148 (0.0495)\tLoss (L1) 13.173 (10.837)\n",
      "2022-01-20 09:26:47,900 | ===> Batch : 34\n",
      "2022-01-20 09:26:47,900 | FDS disable\n",
      "2022-01-20 09:26:59,478 | Calculate Loss\n",
      "2022-01-20 09:26:59,480 | Update Loss\n",
      "2022-01-20 09:26:59,484 | Backward\n",
      "2022-01-20 09:27:20,943 | Epoch: [9][34/48]\tTime  33.06 ( 32.76)\tData 0.0153 (0.0485)\tLoss (L1) 9.580 (10.800)\n",
      "2022-01-20 09:27:20,957 | ===> Batch : 35\n",
      "2022-01-20 09:27:20,958 | FDS disable\n",
      "2022-01-20 09:27:32,551 | Calculate Loss\n",
      "2022-01-20 09:27:32,552 | Update Loss\n",
      "2022-01-20 09:27:32,556 | Backward\n",
      "2022-01-20 09:27:53,563 | Epoch: [9][35/48]\tTime  32.62 ( 32.76)\tData 0.0140 (0.0475)\tLoss (L1) 11.931 (10.833)\n",
      "2022-01-20 09:27:53,577 | ===> Batch : 36\n",
      "2022-01-20 09:27:53,577 | FDS disable\n",
      "2022-01-20 09:28:05,154 | Calculate Loss\n",
      "2022-01-20 09:28:05,155 | Update Loss\n",
      "2022-01-20 09:28:05,159 | Backward\n",
      "2022-01-20 09:28:25,218 | Epoch: [9][36/48]\tTime  31.65 ( 32.73)\tData 0.0143 (0.0466)\tLoss (L1) 9.938 (10.808)\n",
      "2022-01-20 09:28:25,231 | ===> Batch : 37\n",
      "2022-01-20 09:28:25,232 | FDS disable\n",
      "2022-01-20 09:28:36,768 | Calculate Loss\n",
      "2022-01-20 09:28:36,770 | Update Loss\n",
      "2022-01-20 09:28:36,775 | Backward\n",
      "2022-01-20 09:28:58,053 | Epoch: [9][37/48]\tTime  32.84 ( 32.73)\tData 0.0139 (0.0457)\tLoss (L1) 11.295 (10.821)\n",
      "2022-01-20 09:28:58,069 | ===> Batch : 38\n",
      "2022-01-20 09:28:58,069 | FDS disable\n",
      "2022-01-20 09:29:09,622 | Calculate Loss\n",
      "2022-01-20 09:29:09,624 | Update Loss\n",
      "2022-01-20 09:29:09,627 | Backward\n",
      "2022-01-20 09:29:30,947 | Epoch: [9][38/48]\tTime  32.89 ( 32.74)\tData 0.0154 (0.0449)\tLoss (L1) 9.949 (10.798)\n",
      "2022-01-20 09:29:30,962 | ===> Batch : 39\n",
      "2022-01-20 09:29:30,962 | FDS disable\n",
      "2022-01-20 09:29:42,563 | Calculate Loss\n",
      "2022-01-20 09:29:42,564 | Update Loss\n",
      "2022-01-20 09:29:42,568 | Backward\n",
      "2022-01-20 09:30:03,469 | Epoch: [9][39/48]\tTime  32.52 ( 32.73)\tData 0.0154 (0.0441)\tLoss (L1) 12.911 (10.852)\n",
      "2022-01-20 09:30:03,483 | ===> Batch : 40\n",
      "2022-01-20 09:30:03,484 | FDS disable\n",
      "2022-01-20 09:30:15,075 | Calculate Loss\n",
      "2022-01-20 09:30:15,077 | Update Loss\n",
      "2022-01-20 09:30:15,081 | Backward\n",
      "2022-01-20 09:30:35,658 | Epoch: [9][40/48]\tTime  32.19 ( 32.72)\tData 0.0138 (0.0434)\tLoss (L1) 8.558 (10.795)\n",
      "2022-01-20 09:30:35,672 | ===> Batch : 41\n",
      "2022-01-20 09:30:35,673 | FDS disable\n",
      "2022-01-20 09:30:47,231 | Calculate Loss\n",
      "2022-01-20 09:30:47,232 | Update Loss\n",
      "2022-01-20 09:30:47,236 | Backward\n",
      "2022-01-20 09:31:08,789 | Epoch: [9][41/48]\tTime  33.13 ( 32.73)\tData 0.0144 (0.0427)\tLoss (L1) 10.071 (10.777)\n",
      "2022-01-20 09:31:08,803 | ===> Batch : 42\n",
      "2022-01-20 09:31:08,804 | FDS disable\n",
      "2022-01-20 09:31:20,420 | Calculate Loss\n",
      "2022-01-20 09:31:20,421 | Update Loss\n",
      "2022-01-20 09:31:20,426 | Backward\n",
      "2022-01-20 09:31:41,292 | Epoch: [9][42/48]\tTime  32.50 ( 32.72)\tData 0.0138 (0.0420)\tLoss (L1) 11.282 (10.789)\n",
      "2022-01-20 09:31:41,308 | ===> Batch : 43\n",
      "2022-01-20 09:31:41,308 | FDS disable\n",
      "2022-01-20 09:31:52,899 | Calculate Loss\n",
      "2022-01-20 09:31:52,901 | Update Loss\n",
      "2022-01-20 09:31:52,904 | Backward\n",
      "2022-01-20 09:32:13,605 | Epoch: [9][43/48]\tTime  32.31 ( 32.71)\tData 0.0158 (0.0414)\tLoss (L1) 9.066 (10.749)\n",
      "2022-01-20 09:32:13,619 | ===> Batch : 44\n",
      "2022-01-20 09:32:13,620 | FDS disable\n",
      "2022-01-20 09:32:25,219 | Calculate Loss\n",
      "2022-01-20 09:32:25,220 | Update Loss\n",
      "2022-01-20 09:32:25,224 | Backward\n",
      "2022-01-20 09:32:46,394 | Epoch: [9][44/48]\tTime  32.79 ( 32.71)\tData 0.0143 (0.0408)\tLoss (L1) 11.477 (10.766)\n",
      "2022-01-20 09:32:46,409 | ===> Batch : 45\n",
      "2022-01-20 09:32:46,409 | FDS disable\n",
      "2022-01-20 09:32:57,974 | Calculate Loss\n",
      "2022-01-20 09:32:57,975 | Update Loss\n",
      "2022-01-20 09:32:57,978 | Backward\n",
      "2022-01-20 09:33:18,855 | Epoch: [9][45/48]\tTime  32.46 ( 32.71)\tData 0.0143 (0.0402)\tLoss (L1) 9.733 (10.743)\n",
      "2022-01-20 09:33:18,869 | ===> Batch : 46\n",
      "2022-01-20 09:33:18,870 | FDS disable\n",
      "2022-01-20 09:33:30,454 | Calculate Loss\n",
      "2022-01-20 09:33:30,455 | Update Loss\n",
      "2022-01-20 09:33:30,459 | Backward\n",
      "2022-01-20 09:33:51,374 | Epoch: [9][46/48]\tTime  32.52 ( 32.70)\tData 0.0143 (0.0396)\tLoss (L1) 10.835 (10.745)\n",
      "2022-01-20 09:33:51,387 | ===> Batch : 47\n",
      "2022-01-20 09:33:51,388 | FDS disable\n",
      "2022-01-20 09:34:02,941 | Calculate Loss\n",
      "2022-01-20 09:34:02,943 | Update Loss\n",
      "2022-01-20 09:34:02,946 | Backward\n",
      "2022-01-20 09:34:24,233 | Epoch: [9][47/48]\tTime  32.86 ( 32.71)\tData 0.0138 (0.0391)\tLoss (L1) 10.830 (10.746)\n",
      "2022-01-20 09:34:24,247 | ===> Batch : 48\n",
      "2022-01-20 09:34:24,247 | FDS disable\n",
      "2022-01-20 09:34:32,283 | Calculate Loss\n",
      "2022-01-20 09:34:32,285 | Update Loss\n",
      "2022-01-20 09:34:32,288 | Backward\n",
      "2022-01-20 09:34:46,707 | Epoch: [9][48/48]\tTime  22.47 ( 32.49)\tData 0.0138 (0.0385)\tLoss (L1) 9.439 (10.728)\n",
      "2022-01-20 09:34:58,666 | Val: [0/9]\tTime 11.778 (11.778)\tLoss (MSE) 233.275 (233.275)\tLoss (L1) 12.536 (12.536)\n",
      "2022-01-20 09:35:07,834 | Val: [1/9]\tTime  9.168 (10.473)\tLoss (MSE) 202.329 (217.802)\tLoss (L1) 11.508 (12.022)\n",
      "2022-01-20 09:35:16,907 | Val: [2/9]\tTime  9.073 (10.006)\tLoss (MSE) 187.149 (207.584)\tLoss (L1) 11.284 (11.776)\n",
      "2022-01-20 09:35:26,057 | Val: [3/9]\tTime  9.150 ( 9.792)\tLoss (MSE) 204.947 (206.925)\tLoss (L1) 11.707 (11.759)\n",
      "2022-01-20 09:35:35,069 | Val: [4/9]\tTime  9.012 ( 9.636)\tLoss (MSE) 232.840 (212.108)\tLoss (L1) 12.445 (11.896)\n",
      "2022-01-20 09:35:44,208 | Val: [5/9]\tTime  9.139 ( 9.553)\tLoss (MSE) 222.325 (213.811)\tLoss (L1) 12.031 (11.919)\n",
      "2022-01-20 09:35:53,323 | Val: [6/9]\tTime  9.115 ( 9.491)\tLoss (MSE) 199.240 (211.729)\tLoss (L1) 11.366 (11.840)\n",
      "2022-01-20 09:36:02,382 | Val: [7/9]\tTime  9.059 ( 9.437)\tLoss (MSE) 260.132 (217.780)\tLoss (L1) 12.456 (11.917)\n",
      "2022-01-20 09:36:05,658 | Val: [8/9]\tTime  3.276 ( 8.752)\tLoss (MSE) 203.059 (217.147)\tLoss (L1) 11.008 (11.878)\n",
      "2022-01-20 09:36:05,805 |  * Overall: MSE 217.147\tL1 11.878\tG-Mean 8.108\n",
      "2022-01-20 09:36:05,806 |  * Many: MSE 144.101\tL1 9.666\tG-Mean 6.565\n",
      "2022-01-20 09:36:05,806 |  * Median: MSE 328.867\tL1 15.559\tG-Mean 11.880\n",
      "2022-01-20 09:36:05,806 |  * Low: MSE 598.070\tL1 22.588\tG-Mean 20.700\n",
      "2022-01-20 09:36:05,812 | Best L1 Loss: 11.878\n",
      "2022-01-20 09:36:07,081 | ===> Saving current best checkpoint...\n",
      "2022-01-20 09:36:08,201 | Epoch #9: Train loss [10.7276]; Val loss: MSE [217.1467], L1 [11.8776], G-Mean [8.1077]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "        tb_logger.log_value('train_loss', train_loss, epoch)\n",
    "        tb_logger.log_value('val_loss_mse', val_loss_mse, epoch)\n",
    "        tb_logger.log_value('val_loss_l1', val_loss_l1, epoch)\n",
    "        tb_logger.log_value('val_loss_gmean', val_loss_gmean, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"checkpoints/agedb_resnet50_agedb_resnet50_Model_with_sqrt_inv_sqrt_inv_adam_l1_0.001_256_sqrt_inv_adam_l1_0.001_256/ckpt.best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 15:19:09,650 | ========================================================================================================================\n",
      "2022-01-20 15:19:09,651 | Test best model on testset...\n",
      "2022-01-20 15:19:09,677 | Loaded best model, epoch 10, best val loss 11.8776\n",
      "2022-01-20 15:19:21,422 | Test: [0/9]\tTime 11.743 (11.743)\tLoss (MSE) 220.644 (220.644)\tLoss (L1) 11.771 (11.771)\n",
      "2022-01-20 15:19:30,233 | Test: [1/9]\tTime  8.811 (10.277)\tLoss (MSE) 189.506 (205.075)\tLoss (L1) 11.093 (11.432)\n",
      "2022-01-20 15:19:38,944 | Test: [2/9]\tTime  8.711 ( 9.755)\tLoss (MSE) 212.870 (207.673)\tLoss (L1) 11.625 (11.497)\n",
      "2022-01-20 15:19:47,802 | Test: [3/9]\tTime  8.858 ( 9.531)\tLoss (MSE) 213.382 (209.100)\tLoss (L1) 11.437 (11.482)\n",
      "2022-01-20 15:19:56,518 | Test: [4/9]\tTime  8.716 ( 9.368)\tLoss (MSE) 221.300 (211.540)\tLoss (L1) 12.108 (11.607)\n",
      "2022-01-20 15:20:05,308 | Test: [5/9]\tTime  8.790 ( 9.272)\tLoss (MSE) 201.398 (209.850)\tLoss (L1) 11.087 (11.520)\n",
      "2022-01-20 15:20:14,010 | Test: [6/9]\tTime  8.702 ( 9.190)\tLoss (MSE) 208.828 (209.704)\tLoss (L1) 11.595 (11.531)\n",
      "2022-01-20 15:20:22,813 | Test: [7/9]\tTime  8.803 ( 9.142)\tLoss (MSE) 206.619 (209.318)\tLoss (L1) 11.559 (11.534)\n",
      "2022-01-20 15:20:25,767 | Test: [8/9]\tTime  2.954 ( 8.454)\tLoss (MSE) 255.668 (211.311)\tLoss (L1) 12.752 (11.587)\n",
      "2022-01-20 15:20:25,926 |  * Overall: MSE 211.311\tL1 11.587\tG-Mean 7.614\n",
      "2022-01-20 15:20:25,926 |  * Many: MSE 139.453\tL1 9.265\tG-Mean 5.983\n",
      "2022-01-20 15:20:25,927 |  * Median: MSE 311.865\tL1 15.500\tG-Mean 12.143\n",
      "2022-01-20 15:20:25,927 |  * Low: MSE 611.890\tL1 22.699\tG-Mean 20.415\n",
      "2022-01-20 15:20:25,933 | Test loss: MSE [211.3108], L1 [11.5868], G-Mean [7.6145]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# test with best checkpoint\n",
    "print(\"=\" * 120)\n",
    "print(\"Test best model on testset...\")\n",
    "#checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plotModel(vals,title,xtitle,ytitle,grid = False):\n",
    "    batch = np.arange(len(vals)) + 1\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xtitle)\n",
    "    plt.ylabel(ytitle)\n",
    "    plt.grid(grid)\n",
    "    plt.plot(batch,vals)\n",
    "    plt.show()\n",
    "    \n",
    "def getL1Losses(Lines, mod = \"train\"):\n",
    "    l1_values = []\n",
    "    for line in Lines:\n",
    "        if \"(L1)\" in line:\n",
    "            strr = re.sub(r\"\\s+\", \"\", line)\n",
    "            if mod == \"Val\":\n",
    "                if mod in strr:\n",
    "                    start = strr.find('(L1)')\n",
    "                    end = strr.rfind(\"(\")\n",
    "                    l1 = strr[start+4:end]\n",
    "                    l1_values.append(float(l1))\n",
    "            else:\n",
    "                if \"Val\" not in strr:\n",
    "                    start = strr.find('(L1)')\n",
    "                    end = strr.rfind(\"(\")\n",
    "                    l1 = strr[start+4:end]\n",
    "                    l1_values.append(float(l1))\n",
    "                \n",
    "    return l1_values\n",
    "\n",
    "def getLinesFromTextFile(path):\n",
    "    all_path = '/home/candoga01/' + path\n",
    "    file1 = open(all_path, 'r')\n",
    "    return file1.readlines()\n",
    "\n",
    "\n",
    "path = 'blg_561e_project/agedb-dir/checkpoints/agedb_resnet50_agedb_resnet50_Model_with_sqrt_inv_sqrt_inv_adam_l1_0.001_256_sqrt_inv_adam_l1_0.001_256/training.log'\n",
    "\n",
    "\n",
    "# Data for Val\n",
    "lines = getLinesFromTextFile(path)\n",
    "l1_values_val = getL1Losses(lines,mod=\"Val\")\n",
    "\n",
    "# Data for Train\n",
    "lines = getLinesFromTextFile(path)\n",
    "l1_values_train = getL1Losses(lines)\n",
    "\n",
    "plotModel(l1_values_val,\"Validation Loss\",\"Batch\",\"L1 Loss\",True)\n",
    "plotModel(l1_values_train,\"Train Loss\",\"Batch\",\"L1 Loss\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
