{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gmean\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboard_logger import Logger\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from resnet import resnet50\n",
    "from loss import *\n",
    "from datasets import AgeDB\n",
    "from utils import *\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "imbalanced_related = {\"dataset\": \"agedb\", \"start_epoch\": 0, \"epoch\": 10, \"best_loss\": 1e5, \"lr\": 1e-3,\n",
    "                      \"data_dir\": \"./data\", \"schedule\": [60, 80], \"loss\": \"l1\", \"print_freq\" : 1, \"evaluate\": \"\",\n",
    "                      \"fds\": True, \"bucket_num\": 100, \"bucket_start\": 3, \"start_update\": 0, \"resume\": \"\",\n",
    "                      \"start_smooth\": 1, \"fds_kernel\": \"gaussian\", \"fds_ks\": 5, \"fds_sigma\": 2, \"pretrained\": \"\",\n",
    "                      \"fds_mmt\": 0.9, \"retrain_fc\": False, \"img_size\": 224, \"reweight\": \"sqrt_inv\", \"optimizer\": \"adam\",\n",
    "                      \"lds\": False, \"lds_kernel\": \"gaussian\", \"lds_ks\": 5, \"lds_sigma\": 2, \"model\": \"resnet50\",\n",
    "                      \"batch_size\": 256, \"workers\": 8, \"store_root\": \"./checkpoints\", \"store_name\": \"fds_model\"\n",
    "                     }\n",
    "args = SimpleNamespace(**imbalanced_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:56:10,434 | Args: namespace(batch_size=256, best_loss=100000.0, bucket_num=100, bucket_start=3, data_dir='./data', dataset='agedb', epoch=10, evaluate='', fds=True, fds_kernel='gaussian', fds_ks=5, fds_mmt=0.9, fds_sigma=2, img_size=224, lds=False, lds_kernel='gaussian', lds_ks=5, lds_sigma=2, loss='l1', lr=0.001, model='resnet50', optimizer='adam', pretrained='', print_freq=1, resume='', retrain_fc=False, reweight='sqrt_inv', schedule=[60, 80], start_epoch=0, start_smooth=1, start_update=0, store_name='agedb_resnet50_fds_model_sqrt_inv_fds_gau_5_2_0_1_0.9_adam_l1_0.001_256', store_root='./checkpoints', workers=8)\n",
      "2022-01-21 15:56:10,434 | Store name: agedb_resnet50_fds_model_sqrt_inv_fds_gau_5_2_0_1_0.9_adam_l1_0.001_256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Creating folder: ./checkpoints/agedb_resnet50_fds_model_sqrt_inv_fds_gau_5_2_0_1_0.9_adam_l1_0.001_256\n"
     ]
    }
   ],
   "source": [
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.fds:\n",
    "    args.store_name += f'_fds_{args.fds_kernel[:3]}_{args.fds_ks}'\n",
    "    if args.fds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.fds_sigma}'\n",
    "    args.store_name += f'_{args.start_update}_{args.start_smooth}_{args.fds_mmt}'\n",
    "if args.retrain_fc:\n",
    "    args.store_name += f'_retrain_fc'\n",
    "args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "logging.root.handlers = []\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(args.store_root, args.store_name, 'training.log')),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "print = logging.info\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "tb_logger = Logger(logdir=os.path.join(args.store_root, args.store_name), flush_secs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:56:11,713 | ===> Resume is disabled\n"
     ]
    }
   ],
   "source": [
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "        checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "            torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        args.best_loss = checkpoint['best_loss']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "    else:\n",
    "        print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "        \n",
    "else:\n",
    "    print(\"===> Resume is disabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution helper function\n",
    "\n",
    "def get_hist_of_age(bins,ages):\n",
    "    plt.style.use('ggplot')\n",
    "    fig=plt.figure(figsize=(16,6))\n",
    "\n",
    "    plt.hist(ages, bins = bins, edgecolor = 'black')\n",
    "\n",
    "    plt.xlabel(\"Age\")\n",
    "    plt.ylabel(\"Age Disribution\")\n",
    "    plt.title(\"Age Distribution Histogram\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:56:13,230 | =====> Preparing data...\n",
      "2022-01-21 15:56:13,231 | File (.csv): agedb.csv\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('=====> Preparing data...')\n",
    "print(f\"File (.csv): {args.dataset}.csv\")\n",
    "df = pd.read_csv(os.path.join(args.data_dir, f\"{args.dataset}.csv\"))\n",
    "df_train, df_val, df_test = df[df['split'] == 'train'], df[df['split'] == 'val'], df[df['split'] == 'test']\n",
    "train_labels = df_train['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:56:13,944 |    age                                path  split\n",
      "0   31   AgeDB/11706_OliviaHussey_31_f.jpg  train\n",
      "1   59   AgeDB/11684_MireilleDarc_59_f.jpg    val\n",
      "2   44   AgeDB/7955_GilbertRoland_44_m.jpg  train\n",
      "3   61  AgeDB/9352_GeorgesMarchal_61_m.jpg    val\n",
      "4   28     AgeDB/3888_TomasMilian_28_m.jpg    val\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train Data for Age\n",
    "get_hist_of_age(bins=100, ages = train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:56:15,805 | Using re-weighting: [SQRT_INV]\n"
     ]
    }
   ],
   "source": [
    "# Load Data Sets and See the Kernel and Effective Label Density after convolution to make the Emprical LD smoothe\n",
    "train_dataset = AgeDB(data_dir=args.data_dir, df=df_train, img_size=args.img_size, split='train',reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "val_dataset = AgeDB(data_dir=args.data_dir, df=df_val, img_size=args.img_size, split='val')\n",
    "test_dataset = AgeDB(data_dir=args.data_dir, df=df_test, img_size=args.img_size, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                          num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                        num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                         num_workers=args.workers, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:56:18,259 | Training data size: 12208\n",
      "2022-01-21 15:56:18,260 | Validation data size: 2140\n",
      "2022-01-21 15:56:18,260 | Test data size: 2140\n"
     ]
    }
   ],
   "source": [
    "# Observe sizes\n",
    "print(f\"Training data size: {len(train_dataset)}\")\n",
    "print(f\"Validation data size: {len(val_dataset)}\")\n",
    "print(f\"Test data size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:56:18,858 | =====> Building model...\n",
      "2022-01-21 15:56:19,235 | Using FDS: [GAUSSIAN] (5/2)\n",
      "2022-01-21 15:56:19,588 | DataParallel(\n",
      "  (module): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "    (linear): Linear(in_features=2048, out_features=1, bias=True)\n",
      "    (FDS): FDS()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model\n",
    "print('=====> Building model...')\n",
    "model = resnet50(fds=args.fds, bucket_num=args.bucket_num, bucket_start=args.bucket_start,\n",
    "                 start_update=args.start_update, start_smooth=args.start_smooth,\n",
    "                 kernel=args.fds_kernel, ks=args.fds_ks, sigma=args.fds_sigma, momentum=args.fds_mmt)\n",
    "model = torch.nn.DataParallel(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    print(\"Load train loader\")\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        print(\"===> Batch : \" + str(idx+1))\n",
    "        #inputs, targets, weights = \\\n",
    "        #    inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        if args.fds:\n",
    "            print(\"FDS enable\")\n",
    "            outputs, _ = model(inputs, targets, epoch)\n",
    "        else:\n",
    "            print(\"FDS disable\")\n",
    "            outputs = model(inputs, targets, epoch)\n",
    "\n",
    "        print(\"Calculate Loss\")\n",
    "        loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        print(\"Update Loss\")\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Backward\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx+1)\n",
    "\n",
    "    if args.fds and epoch >= args.start_update:\n",
    "        print(f\"Create Epoch [{epoch}] features of all training data...\")\n",
    "        encodings, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets, _) in tqdm(train_loader):\n",
    "                #inputs = inputs.cuda(non_blocking=True)\n",
    "                outputs, feature = model(inputs, targets, epoch)\n",
    "                encodings.extend(feature.data.squeeze().cpu().numpy())\n",
    "                labels.extend(targets.data.squeeze().cpu().numpy())\n",
    "\n",
    "        encodings, labels = torch.from_numpy(np.vstack(encodings)), torch.from_numpy(np.hstack(labels))\n",
    "        model.module.FDS.update_last_epoch_stats(epoch)\n",
    "        model.module.FDS.update_running_stats(encodings, labels, epoch)\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, train_labels=None, prefix='Val'):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            #inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=100, low_shot_thr=20):\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['gmean'] = gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "    return shot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 15:56:36,683 | Training...\n",
      "2022-01-21 15:56:36,685 | Load train loader\n",
      "2022-01-21 15:56:37,894 | ===> Batch : 1\n",
      "2022-01-21 15:56:37,896 | FDS enable\n",
      "2022-01-21 15:56:50,508 | Calculate Loss\n",
      "2022-01-21 15:56:50,511 | Update Loss\n",
      "2022-01-21 15:56:50,512 | Backward\n",
      "2022-01-21 15:57:11,987 | Epoch: [0][ 1/48]\tTime  35.30 ( 35.30)\tData 1.2095 (1.2095)\tLoss (L1) 43.560 (43.560)\n",
      "2022-01-21 15:57:11,999 | ===> Batch : 2\n",
      "2022-01-21 15:57:12,000 | FDS enable\n",
      "2022-01-21 15:57:23,946 | Calculate Loss\n",
      "2022-01-21 15:57:23,947 | Update Loss\n",
      "2022-01-21 15:57:23,952 | Backward\n",
      "2022-01-21 15:57:45,161 | Epoch: [0][ 2/48]\tTime  33.17 ( 34.24)\tData 0.0127 (0.6111)\tLoss (L1) 44.162 (43.861)\n",
      "2022-01-21 15:57:45,175 | ===> Batch : 3\n",
      "2022-01-21 15:57:45,176 | FDS enable\n",
      "2022-01-21 15:57:57,084 | Calculate Loss\n",
      "2022-01-21 15:57:57,086 | Update Loss\n",
      "2022-01-21 15:57:57,090 | Backward\n",
      "2022-01-21 15:58:17,909 | Epoch: [0][ 3/48]\tTime  32.75 ( 33.74)\tData 0.0142 (0.4121)\tLoss (L1) 39.783 (42.502)\n",
      "2022-01-21 15:58:17,927 | ===> Batch : 4\n",
      "2022-01-21 15:58:17,927 | FDS enable\n",
      "2022-01-21 15:58:29,830 | Calculate Loss\n",
      "2022-01-21 15:58:29,831 | Update Loss\n",
      "2022-01-21 15:58:29,836 | Backward\n",
      "2022-01-21 15:58:50,817 | Epoch: [0][ 4/48]\tTime  32.91 ( 33.53)\tData 0.0175 (0.3135)\tLoss (L1) 31.645 (39.787)\n",
      "2022-01-21 15:58:50,831 | ===> Batch : 5\n",
      "2022-01-21 15:58:50,831 | FDS enable\n",
      "2022-01-21 15:59:02,757 | Calculate Loss\n",
      "2022-01-21 15:59:02,758 | Update Loss\n",
      "2022-01-21 15:59:02,763 | Backward\n",
      "2022-01-21 15:59:23,313 | Epoch: [0][ 5/48]\tTime  32.50 ( 33.33)\tData 0.0141 (0.2536)\tLoss (L1) 28.634 (37.557)\n",
      "2022-01-21 15:59:23,325 | ===> Batch : 6\n",
      "2022-01-21 15:59:23,326 | FDS enable\n",
      "2022-01-21 15:59:35,252 | Calculate Loss\n",
      "2022-01-21 15:59:35,253 | Update Loss\n",
      "2022-01-21 15:59:35,257 | Backward\n",
      "2022-01-21 15:59:55,927 | Epoch: [0][ 6/48]\tTime  32.61 ( 33.21)\tData 0.0126 (0.2134)\tLoss (L1) 26.899 (35.780)\n",
      "2022-01-21 15:59:55,940 | ===> Batch : 7\n",
      "2022-01-21 15:59:55,941 | FDS enable\n",
      "2022-01-21 16:00:07,799 | Calculate Loss\n",
      "2022-01-21 16:00:07,801 | Update Loss\n",
      "2022-01-21 16:00:07,805 | Backward\n",
      "2022-01-21 16:00:28,472 | Epoch: [0][ 7/48]\tTime  32.54 ( 33.11)\tData 0.0128 (0.1848)\tLoss (L1) 20.130 (33.545)\n",
      "2022-01-21 16:00:28,485 | ===> Batch : 8\n",
      "2022-01-21 16:00:28,485 | FDS enable\n",
      "2022-01-21 16:00:40,316 | Calculate Loss\n",
      "2022-01-21 16:00:40,317 | Update Loss\n",
      "2022-01-21 16:00:40,322 | Backward\n",
      "2022-01-21 16:01:01,507 | Epoch: [0][ 8/48]\tTime  33.03 ( 33.10)\tData 0.0125 (0.1632)\tLoss (L1) 19.103 (31.739)\n",
      "2022-01-21 16:01:01,525 | ===> Batch : 9\n",
      "2022-01-21 16:01:01,526 | FDS enable\n",
      "2022-01-21 16:01:13,351 | Calculate Loss\n",
      "2022-01-21 16:01:13,352 | Update Loss\n",
      "2022-01-21 16:01:13,357 | Backward\n",
      "2022-01-21 16:01:34,062 | Epoch: [0][ 9/48]\tTime  32.55 ( 33.04)\tData 0.0179 (0.1471)\tLoss (L1) 19.331 (30.361)\n",
      "2022-01-21 16:01:34,075 | ===> Batch : 10\n",
      "2022-01-21 16:01:34,076 | FDS enable\n",
      "2022-01-21 16:01:45,927 | Calculate Loss\n",
      "2022-01-21 16:01:45,928 | Update Loss\n",
      "2022-01-21 16:01:45,933 | Backward\n",
      "2022-01-21 16:02:06,788 | Epoch: [0][10/48]\tTime  32.73 ( 33.01)\tData 0.0134 (0.1337)\tLoss (L1) 17.384 (29.063)\n",
      "2022-01-21 16:02:06,804 | ===> Batch : 11\n",
      "2022-01-21 16:02:06,805 | FDS enable\n",
      "2022-01-21 16:02:18,600 | Calculate Loss\n",
      "2022-01-21 16:02:18,601 | Update Loss\n",
      "2022-01-21 16:02:18,605 | Backward\n",
      "2022-01-21 16:02:39,296 | Epoch: [0][11/48]\tTime  32.51 ( 32.96)\tData 0.0156 (0.1230)\tLoss (L1) 18.203 (28.076)\n",
      "2022-01-21 16:02:39,308 | ===> Batch : 12\n",
      "2022-01-21 16:02:39,309 | FDS enable\n",
      "2022-01-21 16:02:51,079 | Calculate Loss\n",
      "2022-01-21 16:02:51,080 | Update Loss\n",
      "2022-01-21 16:02:51,085 | Backward\n",
      "2022-01-21 16:03:12,076 | Epoch: [0][12/48]\tTime  32.78 ( 32.95)\tData 0.0125 (0.1138)\tLoss (L1) 13.417 (26.854)\n",
      "2022-01-21 16:03:12,089 | ===> Batch : 13\n",
      "2022-01-21 16:03:12,090 | FDS enable\n",
      "2022-01-21 16:03:23,877 | Calculate Loss\n",
      "2022-01-21 16:03:23,878 | Update Loss\n",
      "2022-01-21 16:03:23,882 | Backward\n",
      "2022-01-21 16:03:44,735 | Epoch: [0][13/48]\tTime  32.66 ( 32.93)\tData 0.0136 (0.1061)\tLoss (L1) 12.351 (25.739)\n",
      "2022-01-21 16:03:44,747 | ===> Batch : 14\n",
      "2022-01-21 16:03:44,748 | FDS enable\n",
      "2022-01-21 16:03:56,507 | Calculate Loss\n",
      "2022-01-21 16:03:56,508 | Update Loss\n",
      "2022-01-21 16:03:56,513 | Backward\n",
      "2022-01-21 16:04:17,134 | Epoch: [0][14/48]\tTime  32.40 ( 32.89)\tData 0.0125 (0.0994)\tLoss (L1) 16.224 (25.059)\n",
      "2022-01-21 16:04:17,146 | ===> Batch : 15\n",
      "2022-01-21 16:04:17,147 | FDS enable\n",
      "2022-01-21 16:04:28,890 | Calculate Loss\n",
      "2022-01-21 16:04:28,891 | Update Loss\n",
      "2022-01-21 16:04:28,896 | Backward\n",
      "2022-01-21 16:04:49,628 | Epoch: [0][15/48]\tTime  32.49 ( 32.86)\tData 0.0123 (0.0936)\tLoss (L1) 16.136 (24.464)\n",
      "2022-01-21 16:04:49,641 | ===> Batch : 16\n",
      "2022-01-21 16:04:49,642 | FDS enable\n",
      "2022-01-21 16:05:01,369 | Calculate Loss\n",
      "2022-01-21 16:05:01,370 | Update Loss\n",
      "2022-01-21 16:05:01,375 | Backward\n",
      "2022-01-21 16:05:22,137 | Epoch: [0][16/48]\tTime  32.51 ( 32.84)\tData 0.0133 (0.0886)\tLoss (L1) 17.521 (24.030)\n",
      "2022-01-21 16:05:22,151 | ===> Batch : 17\n",
      "2022-01-21 16:05:22,152 | FDS enable\n",
      "2022-01-21 16:05:33,943 | Calculate Loss\n",
      "2022-01-21 16:05:33,944 | Update Loss\n",
      "2022-01-21 16:05:33,949 | Backward\n",
      "2022-01-21 16:05:54,833 | Epoch: [0][17/48]\tTime  32.70 ( 32.83)\tData 0.0142 (0.0842)\tLoss (L1) 13.123 (23.389)\n",
      "2022-01-21 16:05:54,849 | ===> Batch : 18\n",
      "2022-01-21 16:05:54,850 | FDS enable\n",
      "2022-01-21 16:06:06,620 | Calculate Loss\n",
      "2022-01-21 16:06:06,621 | Update Loss\n",
      "2022-01-21 16:06:06,626 | Backward\n",
      "2022-01-21 16:06:27,415 | Epoch: [0][18/48]\tTime  32.58 ( 32.82)\tData 0.0157 (0.0804)\tLoss (L1) 15.304 (22.939)\n",
      "2022-01-21 16:06:27,428 | ===> Batch : 19\n",
      "2022-01-21 16:06:27,428 | FDS enable\n",
      "2022-01-21 16:06:39,097 | Calculate Loss\n",
      "2022-01-21 16:06:39,098 | Update Loss\n",
      "2022-01-21 16:06:39,103 | Backward\n",
      "2022-01-21 16:06:59,318 | Epoch: [0][19/48]\tTime  31.90 ( 32.77)\tData 0.0135 (0.0769)\tLoss (L1) 14.372 (22.488)\n",
      "2022-01-21 16:06:59,332 | ===> Batch : 20\n",
      "2022-01-21 16:06:59,332 | FDS enable\n",
      "2022-01-21 16:07:11,044 | Calculate Loss\n",
      "2022-01-21 16:07:11,046 | Update Loss\n",
      "2022-01-21 16:07:11,050 | Backward\n",
      "2022-01-21 16:07:31,754 | Epoch: [0][20/48]\tTime  32.44 ( 32.75)\tData 0.0140 (0.0737)\tLoss (L1) 13.855 (22.057)\n",
      "2022-01-21 16:07:31,767 | ===> Batch : 21\n",
      "2022-01-21 16:07:31,768 | FDS enable\n",
      "2022-01-21 16:07:43,441 | Calculate Loss\n",
      "2022-01-21 16:07:43,442 | Update Loss\n",
      "2022-01-21 16:07:43,447 | Backward\n",
      "2022-01-21 16:08:04,120 | Epoch: [0][21/48]\tTime  32.37 ( 32.73)\tData 0.0135 (0.0709)\tLoss (L1) 11.850 (21.571)\n",
      "2022-01-21 16:08:04,134 | ===> Batch : 22\n",
      "2022-01-21 16:08:04,135 | FDS enable\n",
      "2022-01-21 16:08:15,805 | Calculate Loss\n",
      "2022-01-21 16:08:15,806 | Update Loss\n",
      "2022-01-21 16:08:15,811 | Backward\n",
      "2022-01-21 16:08:36,237 | Epoch: [0][22/48]\tTime  32.12 ( 32.71)\tData 0.0148 (0.0683)\tLoss (L1) 12.411 (21.154)\n",
      "2022-01-21 16:08:36,251 | ===> Batch : 23\n",
      "2022-01-21 16:08:36,251 | FDS enable\n",
      "2022-01-21 16:08:47,941 | Calculate Loss\n",
      "2022-01-21 16:08:47,942 | Update Loss\n",
      "2022-01-21 16:08:47,946 | Backward\n",
      "2022-01-21 16:09:08,999 | Epoch: [0][23/48]\tTime  32.76 ( 32.71)\tData 0.0139 (0.0659)\tLoss (L1) 16.756 (20.963)\n",
      "2022-01-21 16:09:09,012 | ===> Batch : 24\n",
      "2022-01-21 16:09:09,013 | FDS enable\n",
      "2022-01-21 16:09:20,717 | Calculate Loss\n",
      "2022-01-21 16:09:20,718 | Update Loss\n",
      "2022-01-21 16:09:20,723 | Backward\n",
      "2022-01-21 16:09:41,335 | Epoch: [0][24/48]\tTime  32.34 ( 32.69)\tData 0.0136 (0.0638)\tLoss (L1) 16.741 (20.787)\n",
      "2022-01-21 16:09:41,349 | ===> Batch : 25\n",
      "2022-01-21 16:09:41,349 | FDS enable\n",
      "2022-01-21 16:09:53,065 | Calculate Loss\n",
      "2022-01-21 16:09:53,066 | Update Loss\n",
      "2022-01-21 16:09:53,071 | Backward\n",
      "2022-01-21 16:10:13,866 | Epoch: [0][25/48]\tTime  32.53 ( 32.69)\tData 0.0144 (0.0618)\tLoss (L1) 16.034 (20.597)\n",
      "2022-01-21 16:10:13,882 | ===> Batch : 26\n",
      "2022-01-21 16:10:13,882 | FDS enable\n",
      "2022-01-21 16:10:25,508 | Calculate Loss\n",
      "2022-01-21 16:10:25,510 | Update Loss\n",
      "2022-01-21 16:10:25,515 | Backward\n",
      "2022-01-21 16:10:46,327 | Epoch: [0][26/48]\tTime  32.46 ( 32.68)\tData 0.0152 (0.0600)\tLoss (L1) 14.540 (20.364)\n",
      "2022-01-21 16:10:46,342 | ===> Batch : 27\n",
      "2022-01-21 16:10:46,342 | FDS enable\n",
      "2022-01-21 16:10:58,062 | Calculate Loss\n",
      "2022-01-21 16:10:58,064 | Update Loss\n",
      "2022-01-21 16:10:58,068 | Backward\n",
      "2022-01-21 16:11:18,540 | Epoch: [0][27/48]\tTime  32.21 ( 32.66)\tData 0.0148 (0.0583)\tLoss (L1) 15.909 (20.199)\n",
      "2022-01-21 16:11:18,554 | ===> Batch : 28\n",
      "2022-01-21 16:11:18,555 | FDS enable\n",
      "2022-01-21 16:11:30,194 | Calculate Loss\n",
      "2022-01-21 16:11:30,195 | Update Loss\n",
      "2022-01-21 16:11:30,200 | Backward\n",
      "2022-01-21 16:11:51,033 | Epoch: [0][28/48]\tTime  32.49 ( 32.66)\tData 0.0142 (0.0567)\tLoss (L1) 13.847 (19.972)\n",
      "2022-01-21 16:11:51,049 | ===> Batch : 29\n",
      "2022-01-21 16:11:51,049 | FDS enable\n",
      "2022-01-21 16:12:02,727 | Calculate Loss\n",
      "2022-01-21 16:12:02,729 | Update Loss\n",
      "2022-01-21 16:12:02,733 | Backward\n",
      "2022-01-21 16:12:23,487 | Epoch: [0][29/48]\tTime  32.45 ( 32.65)\tData 0.0160 (0.0553)\tLoss (L1) 16.092 (19.838)\n",
      "2022-01-21 16:12:23,500 | ===> Batch : 30\n",
      "2022-01-21 16:12:23,501 | FDS enable\n",
      "2022-01-21 16:12:35,201 | Calculate Loss\n",
      "2022-01-21 16:12:35,203 | Update Loss\n",
      "2022-01-21 16:12:35,208 | Backward\n",
      "2022-01-21 16:12:56,050 | Epoch: [0][30/48]\tTime  32.56 ( 32.65)\tData 0.0138 (0.0539)\tLoss (L1) 13.017 (19.611)\n",
      "2022-01-21 16:12:56,065 | ===> Batch : 31\n",
      "2022-01-21 16:12:56,066 | FDS enable\n",
      "2022-01-21 16:13:07,736 | Calculate Loss\n",
      "2022-01-21 16:13:07,738 | Update Loss\n",
      "2022-01-21 16:13:07,742 | Backward\n",
      "2022-01-21 16:13:28,237 | Epoch: [0][31/48]\tTime  32.19 ( 32.63)\tData 0.0147 (0.0527)\tLoss (L1) 13.805 (19.424)\n",
      "2022-01-21 16:13:28,252 | ===> Batch : 32\n",
      "2022-01-21 16:13:28,252 | FDS enable\n",
      "2022-01-21 16:13:39,861 | Calculate Loss\n",
      "2022-01-21 16:13:39,862 | Update Loss\n",
      "2022-01-21 16:13:39,867 | Backward\n",
      "2022-01-21 16:14:00,776 | Epoch: [0][32/48]\tTime  32.54 ( 32.63)\tData 0.0150 (0.0515)\tLoss (L1) 11.315 (19.170)\n",
      "2022-01-21 16:14:00,790 | ===> Batch : 33\n",
      "2022-01-21 16:14:00,791 | FDS enable\n",
      "2022-01-21 16:14:12,292 | Calculate Loss\n",
      "2022-01-21 16:14:12,293 | Update Loss\n",
      "2022-01-21 16:14:12,298 | Backward\n",
      "2022-01-21 16:14:32,584 | Epoch: [0][33/48]\tTime  31.81 ( 32.60)\tData 0.0146 (0.0504)\tLoss (L1) 14.344 (19.024)\n",
      "2022-01-21 16:14:32,597 | ===> Batch : 34\n",
      "2022-01-21 16:14:32,598 | FDS enable\n",
      "2022-01-21 16:14:44,081 | Calculate Loss\n",
      "2022-01-21 16:14:44,082 | Update Loss\n",
      "2022-01-21 16:14:44,086 | Backward\n",
      "2022-01-21 16:15:05,103 | Epoch: [0][34/48]\tTime  32.52 ( 32.60)\tData 0.0138 (0.0493)\tLoss (L1) 12.767 (18.840)\n",
      "2022-01-21 16:15:05,117 | ===> Batch : 35\n",
      "2022-01-21 16:15:05,117 | FDS enable\n",
      "2022-01-21 16:15:16,660 | Calculate Loss\n",
      "2022-01-21 16:15:16,662 | Update Loss\n",
      "2022-01-21 16:15:16,666 | Backward\n",
      "2022-01-21 16:15:37,703 | Epoch: [0][35/48]\tTime  32.60 ( 32.60)\tData 0.0140 (0.0483)\tLoss (L1) 13.648 (18.692)\n",
      "2022-01-21 16:15:37,717 | ===> Batch : 36\n",
      "2022-01-21 16:15:37,718 | FDS enable\n",
      "2022-01-21 16:15:49,276 | Calculate Loss\n",
      "2022-01-21 16:15:49,278 | Update Loss\n",
      "2022-01-21 16:15:49,282 | Backward\n",
      "2022-01-21 16:16:10,204 | Epoch: [0][36/48]\tTime  32.50 ( 32.60)\tData 0.0140 (0.0473)\tLoss (L1) 12.126 (18.509)\n",
      "2022-01-21 16:16:10,218 | ===> Batch : 37\n",
      "2022-01-21 16:16:10,219 | FDS enable\n",
      "2022-01-21 16:16:21,802 | Calculate Loss\n",
      "2022-01-21 16:16:21,803 | Update Loss\n",
      "2022-01-21 16:16:21,808 | Backward\n",
      "2022-01-21 16:16:42,663 | Epoch: [0][37/48]\tTime  32.46 ( 32.59)\tData 0.0147 (0.0465)\tLoss (L1) 16.400 (18.452)\n",
      "2022-01-21 16:16:42,677 | ===> Batch : 38\n",
      "2022-01-21 16:16:42,677 | FDS enable\n",
      "2022-01-21 16:16:54,157 | Calculate Loss\n",
      "2022-01-21 16:16:54,159 | Update Loss\n",
      "2022-01-21 16:16:54,163 | Backward\n",
      "2022-01-21 16:17:14,626 | Epoch: [0][38/48]\tTime  31.96 ( 32.58)\tData 0.0138 (0.0456)\tLoss (L1) 16.555 (18.402)\n",
      "2022-01-21 16:17:14,639 | ===> Batch : 39\n",
      "2022-01-21 16:17:14,640 | FDS enable\n",
      "2022-01-21 16:17:26,173 | Calculate Loss\n",
      "2022-01-21 16:17:26,175 | Update Loss\n",
      "2022-01-21 16:17:26,179 | Backward\n",
      "2022-01-21 16:17:47,371 | Epoch: [0][39/48]\tTime  32.75 ( 32.58)\tData 0.0137 (0.0448)\tLoss (L1) 13.538 (18.278)\n",
      "2022-01-21 16:17:47,385 | ===> Batch : 40\n",
      "2022-01-21 16:17:47,386 | FDS enable\n",
      "2022-01-21 16:17:58,932 | Calculate Loss\n",
      "2022-01-21 16:17:58,934 | Update Loss\n",
      "2022-01-21 16:17:58,938 | Backward\n",
      "2022-01-21 16:18:19,497 | Epoch: [0][40/48]\tTime  32.13 ( 32.57)\tData 0.0137 (0.0440)\tLoss (L1) 15.935 (18.219)\n",
      "2022-01-21 16:18:19,511 | ===> Batch : 41\n",
      "2022-01-21 16:18:19,512 | FDS enable\n",
      "2022-01-21 16:18:30,983 | Calculate Loss\n",
      "2022-01-21 16:18:30,985 | Update Loss\n",
      "2022-01-21 16:18:30,989 | Backward\n",
      "2022-01-21 16:18:51,803 | Epoch: [0][41/48]\tTime  32.31 ( 32.56)\tData 0.0136 (0.0433)\tLoss (L1) 15.037 (18.142)\n",
      "2022-01-21 16:18:51,816 | ===> Batch : 42\n",
      "2022-01-21 16:18:51,817 | FDS enable\n",
      "2022-01-21 16:19:03,242 | Calculate Loss\n",
      "2022-01-21 16:19:03,243 | Update Loss\n",
      "2022-01-21 16:19:03,248 | Backward\n",
      "2022-01-21 16:19:23,825 | Epoch: [0][42/48]\tTime  32.02 ( 32.55)\tData 0.0136 (0.0426)\tLoss (L1) 13.721 (18.036)\n",
      "2022-01-21 16:19:23,839 | ===> Batch : 43\n",
      "2022-01-21 16:19:23,839 | FDS enable\n",
      "2022-01-21 16:19:35,322 | Calculate Loss\n",
      "2022-01-21 16:19:35,324 | Update Loss\n",
      "2022-01-21 16:19:35,329 | Backward\n",
      "2022-01-21 16:19:56,097 | Epoch: [0][43/48]\tTime  32.27 ( 32.54)\tData 0.0140 (0.0419)\tLoss (L1) 13.705 (17.936)\n",
      "2022-01-21 16:19:56,111 | ===> Batch : 44\n",
      "2022-01-21 16:19:56,111 | FDS enable\n",
      "2022-01-21 16:20:07,578 | Calculate Loss\n",
      "2022-01-21 16:20:07,579 | Update Loss\n",
      "2022-01-21 16:20:07,584 | Backward\n",
      "2022-01-21 16:20:28,344 | Epoch: [0][44/48]\tTime  32.25 ( 32.54)\tData 0.0140 (0.0413)\tLoss (L1) 16.596 (17.905)\n",
      "2022-01-21 16:20:28,358 | ===> Batch : 45\n",
      "2022-01-21 16:20:28,358 | FDS enable\n",
      "2022-01-21 16:20:39,873 | Calculate Loss\n",
      "2022-01-21 16:20:39,874 | Update Loss\n",
      "2022-01-21 16:20:39,878 | Backward\n",
      "2022-01-21 16:21:00,411 | Epoch: [0][45/48]\tTime  32.07 ( 32.53)\tData 0.0136 (0.0407)\tLoss (L1) 13.092 (17.798)\n",
      "2022-01-21 16:21:00,424 | ===> Batch : 46\n",
      "2022-01-21 16:21:00,425 | FDS enable\n",
      "2022-01-21 16:21:11,914 | Calculate Loss\n",
      "2022-01-21 16:21:11,915 | Update Loss\n",
      "2022-01-21 16:21:11,919 | Backward\n",
      "2022-01-21 16:21:32,721 | Epoch: [0][46/48]\tTime  32.31 ( 32.52)\tData 0.0134 (0.0401)\tLoss (L1) 13.267 (17.700)\n",
      "2022-01-21 16:21:32,735 | ===> Batch : 47\n",
      "2022-01-21 16:21:32,736 | FDS enable\n",
      "2022-01-21 16:21:44,167 | Calculate Loss\n",
      "2022-01-21 16:21:44,168 | Update Loss\n",
      "2022-01-21 16:21:44,172 | Backward\n",
      "2022-01-21 16:22:04,717 | Epoch: [0][47/48]\tTime  32.00 ( 32.51)\tData 0.0149 (0.0395)\tLoss (L1) 12.765 (17.595)\n",
      "2022-01-21 16:22:04,731 | ===> Batch : 48\n",
      "2022-01-21 16:22:04,732 | FDS enable\n",
      "2022-01-21 16:22:12,510 | Calculate Loss\n",
      "2022-01-21 16:22:12,511 | Update Loss\n",
      "2022-01-21 16:22:12,516 | Backward\n",
      "2022-01-21 16:22:27,051 | Epoch: [0][48/48]\tTime  22.33 ( 32.30)\tData 0.0137 (0.0390)\tLoss (L1) 13.261 (17.532)\n",
      "2022-01-21 16:22:27,079 | Create Epoch [0] features of all training data...\n",
      "100%|██████████| 48/48 [08:36<00:00, 10.77s/it]\n",
      "2022-01-21 16:31:04,199 | Updated running statistics with Epoch [0] features!\n",
      "2022-01-21 16:31:16,018 | Val: [0/9]\tTime 11.808 (11.808)\tLoss (MSE) 289.913 (289.913)\tLoss (L1) 14.106 (14.106)\n",
      "2022-01-21 16:31:25,102 | Val: [1/9]\tTime  9.084 (10.446)\tLoss (MSE) 283.990 (286.952)\tLoss (L1) 13.919 (14.013)\n",
      "2022-01-21 16:31:34,157 | Val: [2/9]\tTime  9.055 ( 9.983)\tLoss (MSE) 326.592 (300.165)\tLoss (L1) 14.886 (14.304)\n",
      "2022-01-21 16:31:43,224 | Val: [3/9]\tTime  9.067 ( 9.754)\tLoss (MSE) 303.829 (301.081)\tLoss (L1) 14.273 (14.296)\n",
      "2022-01-21 16:31:52,312 | Val: [4/9]\tTime  9.088 ( 9.620)\tLoss (MSE) 300.099 (300.884)\tLoss (L1) 14.575 (14.352)\n",
      "2022-01-21 16:32:01,412 | Val: [5/9]\tTime  9.100 ( 9.534)\tLoss (MSE) 308.756 (302.196)\tLoss (L1) 14.251 (14.335)\n",
      "2022-01-21 16:32:10,491 | Val: [6/9]\tTime  9.078 ( 9.469)\tLoss (MSE) 269.971 (297.593)\tLoss (L1) 13.395 (14.201)\n",
      "2022-01-21 16:32:19,535 | Val: [7/9]\tTime  9.044 ( 9.416)\tLoss (MSE) 337.054 (302.525)\tLoss (L1) 15.026 (14.304)\n",
      "2022-01-21 16:32:23,402 | Val: [8/9]\tTime  3.867 ( 8.799)\tLoss (MSE) 298.937 (302.371)\tLoss (L1) 13.184 (14.256)\n",
      "2022-01-21 16:32:23,571 |  * Overall: MSE 302.371\tL1 14.256\tG-Mean 9.837\n",
      "2022-01-21 16:32:23,571 |  * Many: MSE 185.365\tL1 11.102\tG-Mean 7.588\n",
      "2022-01-21 16:32:23,572 |  * Median: MSE 498.843\tL1 20.097\tG-Mean 16.717\n",
      "2022-01-21 16:32:23,572 |  * Low: MSE 864.096\tL1 27.891\tG-Mean 26.349\n",
      "2022-01-21 16:32:23,578 | Best L1 Loss: 14.256\n",
      "2022-01-21 16:32:23,805 | ===> Saving current best checkpoint...\n",
      "2022-01-21 16:32:23,919 | Epoch #0: Train loss [17.5322]; Val loss: MSE [302.3712], L1 [14.2557], G-Mean [9.8375]\n",
      "2022-01-21 16:32:23,920 | Training...\n",
      "2022-01-21 16:32:23,921 | Load train loader\n",
      "2022-01-21 16:32:25,176 | ===> Batch : 1\n",
      "2022-01-21 16:32:25,178 | FDS enable\n",
      "2022-01-21 16:32:39,343 | Calculate Loss\n",
      "2022-01-21 16:32:39,344 | Update Loss\n",
      "2022-01-21 16:32:39,454 | Backward\n",
      "2022-01-21 16:33:00,973 | Epoch: [1][ 1/48]\tTime  37.05 ( 37.05)\tData 1.2552 (1.2552)\tLoss (L1) 12.557 (12.557)\n",
      "2022-01-21 16:33:00,986 | ===> Batch : 2\n",
      "2022-01-21 16:33:00,987 | FDS enable\n",
      "2022-01-21 16:33:12,685 | Calculate Loss\n",
      "2022-01-21 16:33:12,687 | Update Loss\n",
      "2022-01-21 16:33:12,691 | Backward\n",
      "2022-01-21 16:33:34,122 | Epoch: [1][ 2/48]\tTime  33.15 ( 35.10)\tData 0.0131 (0.6341)\tLoss (L1) 15.229 (13.893)\n",
      "2022-01-21 16:33:34,135 | ===> Batch : 3\n",
      "2022-01-21 16:33:34,136 | FDS enable\n",
      "2022-01-21 16:33:45,874 | Calculate Loss\n",
      "2022-01-21 16:33:45,876 | Update Loss\n",
      "2022-01-21 16:33:45,880 | Backward\n",
      "2022-01-21 16:34:06,472 | Epoch: [1][ 3/48]\tTime  32.35 ( 34.18)\tData 0.0132 (0.4272)\tLoss (L1) 16.278 (14.688)\n",
      "2022-01-21 16:34:06,486 | ===> Batch : 4\n",
      "2022-01-21 16:34:06,486 | FDS enable\n",
      "2022-01-21 16:34:18,213 | Calculate Loss\n",
      "2022-01-21 16:34:18,215 | Update Loss\n",
      "2022-01-21 16:34:18,219 | Backward\n",
      "2022-01-21 16:34:38,966 | Epoch: [1][ 4/48]\tTime  32.49 ( 33.76)\tData 0.0141 (0.3239)\tLoss (L1) 14.590 (14.664)\n",
      "2022-01-21 16:34:38,982 | ===> Batch : 5\n",
      "2022-01-21 16:34:38,983 | FDS enable\n",
      "2022-01-21 16:34:50,657 | Calculate Loss\n",
      "2022-01-21 16:34:50,659 | Update Loss\n",
      "2022-01-21 16:34:50,663 | Backward\n",
      "2022-01-21 16:35:11,810 | Epoch: [1][ 5/48]\tTime  32.84 ( 33.58)\tData 0.0166 (0.2624)\tLoss (L1) 14.862 (14.703)\n",
      "2022-01-21 16:35:11,825 | ===> Batch : 6\n",
      "2022-01-21 16:35:11,825 | FDS enable\n",
      "2022-01-21 16:35:23,564 | Calculate Loss\n",
      "2022-01-21 16:35:23,566 | Update Loss\n",
      "2022-01-21 16:35:23,571 | Backward\n",
      "2022-01-21 16:35:44,600 | Epoch: [1][ 6/48]\tTime  32.79 ( 33.45)\tData 0.0144 (0.2211)\tLoss (L1) 13.311 (14.471)\n",
      "2022-01-21 16:35:44,613 | ===> Batch : 7\n",
      "2022-01-21 16:35:44,614 | FDS enable\n",
      "2022-01-21 16:35:56,353 | Calculate Loss\n",
      "2022-01-21 16:35:56,355 | Update Loss\n",
      "2022-01-21 16:35:56,360 | Backward\n",
      "2022-01-21 16:36:17,455 | Epoch: [1][ 7/48]\tTime  32.86 ( 33.36)\tData 0.0129 (0.1914)\tLoss (L1) 13.084 (14.273)\n",
      "2022-01-21 16:36:17,474 | ===> Batch : 8\n",
      "2022-01-21 16:36:17,475 | FDS enable\n",
      "2022-01-21 16:36:29,217 | Calculate Loss\n",
      "2022-01-21 16:36:29,218 | Update Loss\n",
      "2022-01-21 16:36:29,223 | Backward\n",
      "2022-01-21 16:36:50,238 | Epoch: [1][ 8/48]\tTime  32.78 ( 33.29)\tData 0.0188 (0.1698)\tLoss (L1) 15.127 (14.380)\n",
      "2022-01-21 16:36:50,256 | ===> Batch : 9\n",
      "2022-01-21 16:36:50,257 | FDS enable\n",
      "2022-01-21 16:37:02,017 | Calculate Loss\n",
      "2022-01-21 16:37:02,019 | Update Loss\n",
      "2022-01-21 16:37:02,024 | Backward\n",
      "2022-01-21 16:37:23,047 | Epoch: [1][ 9/48]\tTime  32.81 ( 33.24)\tData 0.0184 (0.1530)\tLoss (L1) 15.071 (14.457)\n",
      "2022-01-21 16:37:23,063 | ===> Batch : 10\n",
      "2022-01-21 16:37:23,064 | FDS enable\n",
      "2022-01-21 16:37:34,838 | Calculate Loss\n",
      "2022-01-21 16:37:34,840 | Update Loss\n",
      "2022-01-21 16:37:34,845 | Backward\n",
      "2022-01-21 16:37:55,184 | Epoch: [1][10/48]\tTime  32.14 ( 33.13)\tData 0.0161 (0.1393)\tLoss (L1) 10.749 (14.086)\n",
      "2022-01-21 16:37:55,198 | ===> Batch : 11\n",
      "2022-01-21 16:37:55,199 | FDS enable\n",
      "2022-01-21 16:38:06,895 | Calculate Loss\n",
      "2022-01-21 16:38:06,897 | Update Loss\n",
      "2022-01-21 16:38:06,901 | Backward\n",
      "2022-01-21 16:38:27,989 | Epoch: [1][11/48]\tTime  32.80 ( 33.10)\tData 0.0141 (0.1279)\tLoss (L1) 15.062 (14.175)\n",
      "2022-01-21 16:38:28,001 | ===> Batch : 12\n",
      "2022-01-21 16:38:28,002 | FDS enable\n",
      "2022-01-21 16:38:39,709 | Calculate Loss\n",
      "2022-01-21 16:38:39,710 | Update Loss\n",
      "2022-01-21 16:38:39,715 | Backward\n",
      "2022-01-21 16:39:00,493 | Epoch: [1][12/48]\tTime  32.50 ( 33.05)\tData 0.0123 (0.1183)\tLoss (L1) 11.684 (13.967)\n",
      "2022-01-21 16:39:00,507 | ===> Batch : 13\n",
      "2022-01-21 16:39:00,507 | FDS enable\n",
      "2022-01-21 16:39:12,251 | Calculate Loss\n",
      "2022-01-21 16:39:12,252 | Update Loss\n",
      "2022-01-21 16:39:12,257 | Backward\n",
      "2022-01-21 16:39:33,309 | Epoch: [1][13/48]\tTime  32.82 ( 33.03)\tData 0.0140 (0.1102)\tLoss (L1) 14.109 (13.978)\n",
      "2022-01-21 16:39:33,322 | ===> Batch : 14\n",
      "2022-01-21 16:39:33,323 | FDS enable\n",
      "2022-01-21 16:39:45,066 | Calculate Loss\n",
      "2022-01-21 16:39:45,069 | Update Loss\n",
      "2022-01-21 16:39:45,073 | Backward\n",
      "2022-01-21 16:40:06,021 | Epoch: [1][14/48]\tTime  32.71 ( 33.01)\tData 0.0133 (0.1033)\tLoss (L1) 12.158 (13.848)\n",
      "2022-01-21 16:40:06,033 | ===> Batch : 15\n",
      "2022-01-21 16:40:06,034 | FDS enable\n",
      "2022-01-21 16:40:17,792 | Calculate Loss\n",
      "2022-01-21 16:40:17,794 | Update Loss\n",
      "2022-01-21 16:40:17,799 | Backward\n",
      "2022-01-21 16:40:38,611 | Epoch: [1][15/48]\tTime  32.59 ( 32.98)\tData 0.0125 (0.0973)\tLoss (L1) 12.510 (13.759)\n",
      "2022-01-21 16:40:38,624 | ===> Batch : 16\n",
      "2022-01-21 16:40:38,624 | FDS enable\n",
      "2022-01-21 16:40:50,372 | Calculate Loss\n",
      "2022-01-21 16:40:50,373 | Update Loss\n",
      "2022-01-21 16:40:50,377 | Backward\n",
      "2022-01-21 16:41:11,482 | Epoch: [1][16/48]\tTime  32.87 ( 32.97)\tData 0.0126 (0.0920)\tLoss (L1) 14.733 (13.820)\n",
      "2022-01-21 16:41:11,496 | ===> Batch : 17\n",
      "2022-01-21 16:41:11,497 | FDS enable\n",
      "2022-01-21 16:41:23,242 | Calculate Loss\n",
      "2022-01-21 16:41:23,243 | Update Loss\n",
      "2022-01-21 16:41:23,248 | Backward\n",
      "2022-01-21 16:41:43,421 | Epoch: [1][17/48]\tTime  31.94 ( 32.91)\tData 0.0142 (0.0874)\tLoss (L1) 14.392 (13.853)\n",
      "2022-01-21 16:41:43,435 | ===> Batch : 18\n",
      "2022-01-21 16:41:43,436 | FDS enable\n",
      "2022-01-21 16:41:55,185 | Calculate Loss\n",
      "2022-01-21 16:41:55,187 | Update Loss\n",
      "2022-01-21 16:41:55,191 | Backward\n",
      "2022-01-21 16:42:16,179 | Epoch: [1][18/48]\tTime  32.76 ( 32.90)\tData 0.0140 (0.0833)\tLoss (L1) 12.151 (13.759)\n",
      "2022-01-21 16:42:16,193 | ===> Batch : 19\n",
      "2022-01-21 16:42:16,193 | FDS enable\n",
      "2022-01-21 16:42:27,922 | Calculate Loss\n",
      "2022-01-21 16:42:27,924 | Update Loss\n",
      "2022-01-21 16:42:27,928 | Backward\n",
      "2022-01-21 16:42:48,663 | Epoch: [1][19/48]\tTime  32.48 ( 32.88)\tData 0.0136 (0.0797)\tLoss (L1) 10.985 (13.613)\n",
      "2022-01-21 16:42:48,677 | ===> Batch : 20\n",
      "2022-01-21 16:42:48,678 | FDS enable\n",
      "2022-01-21 16:43:00,394 | Calculate Loss\n",
      "2022-01-21 16:43:00,395 | Update Loss\n",
      "2022-01-21 16:43:00,400 | Backward\n",
      "2022-01-21 16:43:21,442 | Epoch: [1][20/48]\tTime  32.78 ( 32.88)\tData 0.0139 (0.0764)\tLoss (L1) 14.275 (13.646)\n",
      "2022-01-21 16:43:21,461 | ===> Batch : 21\n",
      "2022-01-21 16:43:21,462 | FDS enable\n",
      "2022-01-21 16:43:33,180 | Calculate Loss\n",
      "2022-01-21 16:43:33,182 | Update Loss\n",
      "2022-01-21 16:43:33,186 | Backward\n",
      "2022-01-21 16:43:53,833 | Epoch: [1][21/48]\tTime  32.39 ( 32.85)\tData 0.0196 (0.0737)\tLoss (L1) 17.471 (13.828)\n",
      "2022-01-21 16:43:53,848 | ===> Batch : 22\n",
      "2022-01-21 16:43:53,848 | FDS enable\n",
      "2022-01-21 16:44:05,587 | Calculate Loss\n",
      "2022-01-21 16:44:05,588 | Update Loss\n",
      "2022-01-21 16:44:05,593 | Backward\n",
      "2022-01-21 16:44:26,144 | Epoch: [1][22/48]\tTime  32.31 ( 32.83)\tData 0.0147 (0.0710)\tLoss (L1) 13.042 (13.792)\n",
      "2022-01-21 16:44:26,158 | ===> Batch : 23\n",
      "2022-01-21 16:44:26,158 | FDS enable\n",
      "2022-01-21 16:44:37,870 | Calculate Loss\n",
      "2022-01-21 16:44:37,871 | Update Loss\n",
      "2022-01-21 16:44:37,875 | Backward\n",
      "2022-01-21 16:44:58,980 | Epoch: [1][23/48]\tTime  32.84 ( 32.83)\tData 0.0137 (0.0685)\tLoss (L1) 12.512 (13.737)\n",
      "2022-01-21 16:44:58,993 | ===> Batch : 24\n",
      "2022-01-21 16:44:58,994 | FDS enable\n",
      "2022-01-21 16:45:10,691 | Calculate Loss\n",
      "2022-01-21 16:45:10,693 | Update Loss\n",
      "2022-01-21 16:45:10,697 | Backward\n",
      "2022-01-21 16:45:31,938 | Epoch: [1][24/48]\tTime  32.96 ( 32.83)\tData 0.0136 (0.0662)\tLoss (L1) 11.464 (13.642)\n",
      "2022-01-21 16:45:31,952 | ===> Batch : 25\n",
      "2022-01-21 16:45:31,952 | FDS enable\n",
      "2022-01-21 16:45:43,673 | Calculate Loss\n",
      "2022-01-21 16:45:43,675 | Update Loss\n",
      "2022-01-21 16:45:43,679 | Backward\n",
      "2022-01-21 16:46:04,654 | Epoch: [1][25/48]\tTime  32.72 ( 32.83)\tData 0.0134 (0.0641)\tLoss (L1) 13.801 (13.648)\n",
      "2022-01-21 16:46:04,668 | ===> Batch : 26\n",
      "2022-01-21 16:46:04,669 | FDS enable\n",
      "2022-01-21 16:46:16,400 | Calculate Loss\n",
      "2022-01-21 16:46:16,401 | Update Loss\n",
      "2022-01-21 16:46:16,406 | Backward\n",
      "2022-01-21 16:46:37,253 | Epoch: [1][26/48]\tTime  32.60 ( 32.82)\tData 0.0137 (0.0622)\tLoss (L1) 11.958 (13.583)\n",
      "2022-01-21 16:46:37,268 | ===> Batch : 27\n",
      "2022-01-21 16:46:37,268 | FDS enable\n",
      "2022-01-21 16:46:49,013 | Calculate Loss\n",
      "2022-01-21 16:46:49,014 | Update Loss\n",
      "2022-01-21 16:46:49,019 | Backward\n",
      "2022-01-21 16:47:09,661 | Epoch: [1][27/48]\tTime  32.41 ( 32.81)\tData 0.0147 (0.0604)\tLoss (L1) 13.341 (13.574)\n",
      "2022-01-21 16:47:09,676 | ===> Batch : 28\n",
      "2022-01-21 16:47:09,676 | FDS enable\n",
      "2022-01-21 16:47:21,385 | Calculate Loss\n",
      "2022-01-21 16:47:21,387 | Update Loss\n",
      "2022-01-21 16:47:21,391 | Backward\n",
      "2022-01-21 16:47:41,974 | Epoch: [1][28/48]\tTime  32.31 ( 32.79)\tData 0.0147 (0.0588)\tLoss (L1) 12.688 (13.543)\n",
      "2022-01-21 16:47:41,988 | ===> Batch : 29\n",
      "2022-01-21 16:47:41,989 | FDS enable\n",
      "2022-01-21 16:47:53,708 | Calculate Loss\n",
      "2022-01-21 16:47:53,709 | Update Loss\n",
      "2022-01-21 16:47:53,714 | Backward\n",
      "2022-01-21 16:48:14,558 | Epoch: [1][29/48]\tTime  32.58 ( 32.78)\tData 0.0148 (0.0572)\tLoss (L1) 10.900 (13.452)\n",
      "2022-01-21 16:48:14,577 | ===> Batch : 30\n",
      "2022-01-21 16:48:14,578 | FDS enable\n",
      "2022-01-21 16:48:26,320 | Calculate Loss\n",
      "2022-01-21 16:48:26,321 | Update Loss\n",
      "2022-01-21 16:48:26,326 | Backward\n",
      "2022-01-21 16:48:47,007 | Epoch: [1][30/48]\tTime  32.45 ( 32.77)\tData 0.0196 (0.0560)\tLoss (L1) 12.022 (13.404)\n",
      "2022-01-21 16:48:47,021 | ===> Batch : 31\n",
      "2022-01-21 16:48:47,022 | FDS enable\n",
      "2022-01-21 16:48:58,826 | Calculate Loss\n",
      "2022-01-21 16:48:58,827 | Update Loss\n",
      "2022-01-21 16:48:58,832 | Backward\n",
      "2022-01-21 16:49:19,643 | Epoch: [1][31/48]\tTime  32.64 ( 32.77)\tData 0.0140 (0.0546)\tLoss (L1) 14.425 (13.437)\n",
      "2022-01-21 16:49:19,657 | ===> Batch : 32\n",
      "2022-01-21 16:49:19,657 | FDS enable\n",
      "2022-01-21 16:49:31,293 | Calculate Loss\n",
      "2022-01-21 16:49:31,295 | Update Loss\n",
      "2022-01-21 16:49:31,299 | Backward\n",
      "2022-01-21 16:49:51,813 | Epoch: [1][32/48]\tTime  32.17 ( 32.75)\tData 0.0137 (0.0534)\tLoss (L1) 10.792 (13.354)\n",
      "2022-01-21 16:49:51,827 | ===> Batch : 33\n",
      "2022-01-21 16:49:51,827 | FDS enable\n",
      "2022-01-21 16:50:03,419 | Calculate Loss\n",
      "2022-01-21 16:50:03,421 | Update Loss\n",
      "2022-01-21 16:50:03,425 | Backward\n",
      "2022-01-21 16:50:24,268 | Epoch: [1][33/48]\tTime  32.45 ( 32.74)\tData 0.0134 (0.0521)\tLoss (L1) 13.821 (13.368)\n",
      "2022-01-21 16:50:24,281 | ===> Batch : 34\n",
      "2022-01-21 16:50:24,281 | FDS enable\n",
      "2022-01-21 16:50:35,847 | Calculate Loss\n",
      "2022-01-21 16:50:35,849 | Update Loss\n",
      "2022-01-21 16:50:35,853 | Backward\n",
      "2022-01-21 16:50:56,904 | Epoch: [1][34/48]\tTime  32.64 ( 32.73)\tData 0.0134 (0.0510)\tLoss (L1) 14.740 (13.409)\n",
      "2022-01-21 16:50:56,917 | ===> Batch : 35\n",
      "2022-01-21 16:50:56,918 | FDS enable\n",
      "2022-01-21 16:51:08,469 | Calculate Loss\n",
      "2022-01-21 16:51:08,471 | Update Loss\n",
      "2022-01-21 16:51:08,476 | Backward\n",
      "2022-01-21 16:51:29,203 | Epoch: [1][35/48]\tTime  32.30 ( 32.72)\tData 0.0138 (0.0499)\tLoss (L1) 13.452 (13.410)\n",
      "2022-01-21 16:51:29,216 | ===> Batch : 36\n",
      "2022-01-21 16:51:29,217 | FDS enable\n",
      "2022-01-21 16:51:40,826 | Calculate Loss\n",
      "2022-01-21 16:51:40,828 | Update Loss\n",
      "2022-01-21 16:51:40,832 | Backward\n",
      "2022-01-21 16:52:01,892 | Epoch: [1][36/48]\tTime  32.69 ( 32.72)\tData 0.0134 (0.0489)\tLoss (L1) 11.981 (13.370)\n",
      "2022-01-21 16:52:01,906 | ===> Batch : 37\n",
      "2022-01-21 16:52:01,906 | FDS enable\n",
      "2022-01-21 16:52:13,483 | Calculate Loss\n",
      "2022-01-21 16:52:13,484 | Update Loss\n",
      "2022-01-21 16:52:13,489 | Backward\n",
      "2022-01-21 16:52:34,546 | Epoch: [1][37/48]\tTime  32.65 ( 32.72)\tData 0.0138 (0.0480)\tLoss (L1) 13.457 (13.373)\n",
      "2022-01-21 16:52:34,561 | ===> Batch : 38\n",
      "2022-01-21 16:52:34,561 | FDS enable\n",
      "2022-01-21 16:52:46,216 | Calculate Loss\n",
      "2022-01-21 16:52:46,217 | Update Loss\n",
      "2022-01-21 16:52:46,222 | Backward\n",
      "2022-01-21 16:53:07,185 | Epoch: [1][38/48]\tTime  32.64 ( 32.72)\tData 0.0147 (0.0471)\tLoss (L1) 11.545 (13.325)\n",
      "2022-01-21 16:53:07,198 | ===> Batch : 39\n",
      "2022-01-21 16:53:07,199 | FDS enable\n",
      "2022-01-21 16:53:18,796 | Calculate Loss\n",
      "2022-01-21 16:53:18,798 | Update Loss\n",
      "2022-01-21 16:53:18,802 | Backward\n",
      "2022-01-21 16:53:39,402 | Epoch: [1][39/48]\tTime  32.22 ( 32.70)\tData 0.0135 (0.0462)\tLoss (L1) 13.220 (13.322)\n",
      "2022-01-21 16:53:39,416 | ===> Batch : 40\n",
      "2022-01-21 16:53:39,416 | FDS enable\n",
      "2022-01-21 16:53:50,971 | Calculate Loss\n",
      "2022-01-21 16:53:50,972 | Update Loss\n",
      "2022-01-21 16:53:50,977 | Backward\n",
      "2022-01-21 16:54:12,137 | Epoch: [1][40/48]\tTime  32.73 ( 32.71)\tData 0.0135 (0.0454)\tLoss (L1) 13.582 (13.328)\n",
      "2022-01-21 16:54:12,151 | ===> Batch : 41\n",
      "2022-01-21 16:54:12,151 | FDS enable\n",
      "2022-01-21 16:54:23,731 | Calculate Loss\n",
      "2022-01-21 16:54:23,733 | Update Loss\n",
      "2022-01-21 16:54:23,737 | Backward\n",
      "2022-01-21 16:54:44,825 | Epoch: [1][41/48]\tTime  32.69 ( 32.70)\tData 0.0133 (0.0446)\tLoss (L1) 15.167 (13.373)\n",
      "2022-01-21 16:54:44,838 | ===> Batch : 42\n",
      "2022-01-21 16:54:44,839 | FDS enable\n",
      "2022-01-21 16:54:56,378 | Calculate Loss\n",
      "2022-01-21 16:54:56,380 | Update Loss\n",
      "2022-01-21 16:54:56,385 | Backward\n",
      "2022-01-21 16:55:17,223 | Epoch: [1][42/48]\tTime  32.40 ( 32.70)\tData 0.0135 (0.0439)\tLoss (L1) 12.906 (13.362)\n",
      "2022-01-21 16:55:17,237 | ===> Batch : 43\n",
      "2022-01-21 16:55:17,237 | FDS enable\n",
      "2022-01-21 16:55:28,791 | Calculate Loss\n",
      "2022-01-21 16:55:28,792 | Update Loss\n",
      "2022-01-21 16:55:28,797 | Backward\n",
      "2022-01-21 16:55:49,366 | Epoch: [1][43/48]\tTime  32.14 ( 32.68)\tData 0.0136 (0.0432)\tLoss (L1) 13.347 (13.362)\n",
      "2022-01-21 16:55:49,380 | ===> Batch : 44\n",
      "2022-01-21 16:55:49,380 | FDS enable\n",
      "2022-01-21 16:56:00,960 | Calculate Loss\n",
      "2022-01-21 16:56:00,961 | Update Loss\n",
      "2022-01-21 16:56:00,966 | Backward\n",
      "2022-01-21 16:56:21,702 | Epoch: [1][44/48]\tTime  32.34 ( 32.68)\tData 0.0139 (0.0425)\tLoss (L1) 14.841 (13.395)\n",
      "2022-01-21 16:56:21,716 | ===> Batch : 45\n",
      "2022-01-21 16:56:21,716 | FDS enable\n",
      "2022-01-21 16:56:33,269 | Calculate Loss\n",
      "2022-01-21 16:56:33,270 | Update Loss\n",
      "2022-01-21 16:56:33,275 | Backward\n",
      "2022-01-21 16:56:54,138 | Epoch: [1][45/48]\tTime  32.44 ( 32.67)\tData 0.0139 (0.0419)\tLoss (L1) 11.696 (13.358)\n",
      "2022-01-21 16:56:54,151 | ===> Batch : 46\n",
      "2022-01-21 16:56:54,152 | FDS enable\n",
      "2022-01-21 16:57:05,699 | Calculate Loss\n",
      "2022-01-21 16:57:05,701 | Update Loss\n",
      "2022-01-21 16:57:05,705 | Backward\n",
      "2022-01-21 16:57:26,706 | Epoch: [1][46/48]\tTime  32.57 ( 32.67)\tData 0.0136 (0.0413)\tLoss (L1) 11.013 (13.307)\n",
      "2022-01-21 16:57:26,720 | ===> Batch : 47\n",
      "2022-01-21 16:57:26,721 | FDS enable\n",
      "2022-01-21 16:57:38,306 | Calculate Loss\n",
      "2022-01-21 16:57:38,308 | Update Loss\n",
      "2022-01-21 16:57:38,313 | Backward\n",
      "2022-01-21 16:57:59,326 | Epoch: [1][47/48]\tTime  32.62 ( 32.67)\tData 0.0139 (0.0407)\tLoss (L1) 16.281 (13.370)\n",
      "2022-01-21 16:57:59,341 | ===> Batch : 48\n",
      "2022-01-21 16:57:59,341 | FDS enable\n",
      "2022-01-21 16:58:07,807 | Calculate Loss\n",
      "2022-01-21 16:58:07,809 | Update Loss\n",
      "2022-01-21 16:58:07,813 | Backward\n",
      "2022-01-21 16:58:21,920 | Epoch: [1][48/48]\tTime  22.59 ( 32.46)\tData 0.0148 (0.0402)\tLoss (L1) 14.013 (13.379)\n",
      "2022-01-21 16:58:22,066 | Create Epoch [1] features of all training data...\n",
      "100%|██████████| 48/48 [08:56<00:00, 11.18s/it]\n",
      "2022-01-21 17:07:18,604 | Updated smoothed statistics on Epoch [1]!\n",
      "2022-01-21 17:07:18,694 | Updated running statistics with Epoch [1] features!\n",
      "2022-01-21 17:07:30,618 | Val: [0/9]\tTime 11.910 (11.910)\tLoss (MSE) 293.504 (293.504)\tLoss (L1) 14.097 (14.097)\n",
      "2022-01-21 17:07:39,724 | Val: [1/9]\tTime  9.106 (10.508)\tLoss (MSE) 278.668 (286.086)\tLoss (L1) 13.328 (13.713)\n",
      "2022-01-21 17:07:48,856 | Val: [2/9]\tTime  9.133 (10.050)\tLoss (MSE) 339.048 (303.740)\tLoss (L1) 15.256 (14.227)\n",
      "2022-01-21 17:07:57,995 | Val: [3/9]\tTime  9.139 ( 9.822)\tLoss (MSE) 288.836 (300.014)\tLoss (L1) 13.936 (14.154)\n",
      "2022-01-21 17:08:07,147 | Val: [4/9]\tTime  9.151 ( 9.688)\tLoss (MSE) 330.510 (306.113)\tLoss (L1) 15.383 (14.400)\n",
      "2022-01-21 17:08:16,319 | Val: [5/9]\tTime  9.172 ( 9.602)\tLoss (MSE) 341.324 (311.982)\tLoss (L1) 14.905 (14.484)\n",
      "2022-01-21 17:08:25,475 | Val: [6/9]\tTime  9.156 ( 9.538)\tLoss (MSE) 297.248 (309.877)\tLoss (L1) 14.299 (14.458)\n",
      "2022-01-21 17:08:34,643 | Val: [7/9]\tTime  9.168 ( 9.492)\tLoss (MSE) 310.419 (309.945)\tLoss (L1) 14.272 (14.435)\n",
      "2022-01-21 17:08:37,732 | Val: [8/9]\tTime  3.089 ( 8.781)\tLoss (MSE) 321.761 (310.453)\tLoss (L1) 14.220 (14.425)\n",
      "2022-01-21 17:08:37,860 |  * Overall: MSE 310.453\tL1 14.425\tG-Mean 10.022\n",
      "2022-01-21 17:08:37,861 |  * Many: MSE 177.490\tL1 10.865\tG-Mean 7.505\n",
      "2022-01-21 17:08:37,861 |  * Median: MSE 580.182\tL1 22.001\tG-Mean 19.192\n",
      "2022-01-21 17:08:37,862 |  * Low: MSE 820.291\tL1 27.099\tG-Mean 25.522\n",
      "2022-01-21 17:08:37,867 | Best L1 Loss: 14.256\n",
      "2022-01-21 17:08:39,098 | Epoch #1: Train loss [13.3791]; Val loss: MSE [310.4526], L1 [14.4254], G-Mean [10.0222]\n",
      "2022-01-21 17:08:39,104 | Training...\n",
      "2022-01-21 17:08:39,106 | Load train loader\n",
      "2022-01-21 17:08:40,367 | ===> Batch : 1\n",
      "2022-01-21 17:08:40,368 | FDS enable\n",
      "2022-01-21 17:08:54,600 | Calculate Loss\n",
      "2022-01-21 17:08:54,601 | Update Loss\n",
      "2022-01-21 17:08:54,708 | Backward\n",
      "2022-01-21 17:09:16,152 | Epoch: [2][ 1/48]\tTime  37.05 ( 37.05)\tData 1.2610 (1.2610)\tLoss (L1) 15.039 (15.039)\n",
      "2022-01-21 17:09:16,168 | ===> Batch : 2\n",
      "2022-01-21 17:09:16,168 | FDS enable\n",
      "2022-01-21 17:09:27,878 | Calculate Loss\n",
      "2022-01-21 17:09:27,879 | Update Loss\n",
      "2022-01-21 17:09:27,883 | Backward\n",
      "2022-01-21 17:09:48,775 | Epoch: [2][ 2/48]\tTime  32.62 ( 34.83)\tData 0.0155 (0.6383)\tLoss (L1) 12.700 (13.870)\n",
      "2022-01-21 17:09:48,788 | ===> Batch : 3\n",
      "2022-01-21 17:09:48,789 | FDS enable\n",
      "2022-01-21 17:10:00,459 | Calculate Loss\n",
      "2022-01-21 17:10:00,461 | Update Loss\n",
      "2022-01-21 17:10:00,466 | Backward\n",
      "2022-01-21 17:10:21,520 | Epoch: [2][ 3/48]\tTime  32.75 ( 34.14)\tData 0.0132 (0.4299)\tLoss (L1) 11.134 (12.958)\n",
      "2022-01-21 17:10:21,537 | ===> Batch : 4\n",
      "2022-01-21 17:10:21,537 | FDS enable\n",
      "2022-01-21 17:10:33,281 | Calculate Loss\n",
      "2022-01-21 17:10:33,284 | Update Loss\n",
      "2022-01-21 17:10:33,289 | Backward\n",
      "2022-01-21 17:10:54,073 | Epoch: [2][ 4/48]\tTime  32.55 ( 33.74)\tData 0.0167 (0.3266)\tLoss (L1) 13.784 (13.164)\n",
      "2022-01-21 17:10:54,087 | ===> Batch : 5\n",
      "2022-01-21 17:10:54,087 | FDS enable\n",
      "2022-01-21 17:11:05,815 | Calculate Loss\n",
      "2022-01-21 17:11:05,816 | Update Loss\n",
      "2022-01-21 17:11:05,820 | Backward\n",
      "2022-01-21 17:11:26,571 | Epoch: [2][ 5/48]\tTime  32.50 ( 33.49)\tData 0.0137 (0.2641)\tLoss (L1) 15.477 (13.627)\n",
      "2022-01-21 17:11:26,585 | ===> Batch : 6\n",
      "2022-01-21 17:11:26,585 | FDS enable\n",
      "2022-01-21 17:11:38,302 | Calculate Loss\n",
      "2022-01-21 17:11:38,303 | Update Loss\n",
      "2022-01-21 17:11:38,307 | Backward\n",
      "2022-01-21 17:11:59,287 | Epoch: [2][ 6/48]\tTime  32.72 ( 33.36)\tData 0.0132 (0.2222)\tLoss (L1) 13.741 (13.646)\n",
      "2022-01-21 17:11:59,300 | ===> Batch : 7\n",
      "2022-01-21 17:11:59,301 | FDS enable\n",
      "2022-01-21 17:12:10,989 | Calculate Loss\n",
      "2022-01-21 17:12:10,991 | Update Loss\n",
      "2022-01-21 17:12:10,995 | Backward\n",
      "2022-01-21 17:12:31,864 | Epoch: [2][ 7/48]\tTime  32.58 ( 33.25)\tData 0.0131 (0.1924)\tLoss (L1) 12.533 (13.487)\n",
      "2022-01-21 17:12:31,880 | ===> Batch : 8\n",
      "2022-01-21 17:12:31,881 | FDS enable\n",
      "2022-01-21 17:12:43,645 | Calculate Loss\n",
      "2022-01-21 17:12:43,646 | Update Loss\n",
      "2022-01-21 17:12:43,651 | Backward\n",
      "2022-01-21 17:13:04,595 | Epoch: [2][ 8/48]\tTime  32.73 ( 33.19)\tData 0.0161 (0.1703)\tLoss (L1) 13.037 (13.431)\n",
      "2022-01-21 17:13:04,615 | ===> Batch : 9\n",
      "2022-01-21 17:13:04,615 | FDS enable\n",
      "2022-01-21 17:13:16,352 | Calculate Loss\n",
      "2022-01-21 17:13:16,354 | Update Loss\n",
      "2022-01-21 17:13:16,358 | Backward\n",
      "2022-01-21 17:13:37,380 | Epoch: [2][ 9/48]\tTime  32.78 ( 33.14)\tData 0.0200 (0.1536)\tLoss (L1) 11.006 (13.161)\n",
      "2022-01-21 17:13:37,392 | ===> Batch : 10\n",
      "2022-01-21 17:13:37,393 | FDS enable\n",
      "2022-01-21 17:13:49,138 | Calculate Loss\n",
      "2022-01-21 17:13:49,139 | Update Loss\n",
      "2022-01-21 17:13:49,144 | Backward\n",
      "2022-01-21 17:14:09,856 | Epoch: [2][10/48]\tTime  32.48 ( 33.08)\tData 0.0125 (0.1395)\tLoss (L1) 12.064 (13.051)\n",
      "2022-01-21 17:14:09,870 | ===> Batch : 11\n",
      "2022-01-21 17:14:09,871 | FDS enable\n",
      "2022-01-21 17:14:21,612 | Calculate Loss\n",
      "2022-01-21 17:14:21,613 | Update Loss\n",
      "2022-01-21 17:14:21,618 | Backward\n",
      "2022-01-21 17:14:42,465 | Epoch: [2][11/48]\tTime  32.61 ( 33.03)\tData 0.0142 (0.1281)\tLoss (L1) 13.028 (13.049)\n",
      "2022-01-21 17:14:42,478 | ===> Batch : 12\n",
      "2022-01-21 17:14:42,479 | FDS enable\n",
      "2022-01-21 17:14:54,214 | Calculate Loss\n",
      "2022-01-21 17:14:54,215 | Update Loss\n",
      "2022-01-21 17:14:54,220 | Backward\n",
      "2022-01-21 17:15:14,761 | Epoch: [2][12/48]\tTime  32.30 ( 32.97)\tData 0.0126 (0.1185)\tLoss (L1) 13.904 (13.121)\n",
      "2022-01-21 17:15:14,774 | ===> Batch : 13\n",
      "2022-01-21 17:15:14,774 | FDS enable\n",
      "2022-01-21 17:15:26,515 | Calculate Loss\n",
      "2022-01-21 17:15:26,517 | Update Loss\n",
      "2022-01-21 17:15:26,521 | Backward\n",
      "2022-01-21 17:15:47,145 | Epoch: [2][13/48]\tTime  32.38 ( 32.93)\tData 0.0127 (0.1104)\tLoss (L1) 13.373 (13.140)\n",
      "2022-01-21 17:15:47,158 | ===> Batch : 14\n",
      "2022-01-21 17:15:47,159 | FDS enable\n",
      "2022-01-21 17:15:58,902 | Calculate Loss\n",
      "2022-01-21 17:15:58,904 | Update Loss\n",
      "2022-01-21 17:15:58,908 | Backward\n",
      "2022-01-21 17:16:20,023 | Epoch: [2][14/48]\tTime  32.88 ( 32.92)\tData 0.0128 (0.1034)\tLoss (L1) 13.005 (13.130)\n",
      "2022-01-21 17:16:20,036 | ===> Batch : 15\n",
      "2022-01-21 17:16:20,036 | FDS enable\n",
      "2022-01-21 17:16:31,794 | Calculate Loss\n",
      "2022-01-21 17:16:31,796 | Update Loss\n",
      "2022-01-21 17:16:31,800 | Backward\n",
      "2022-01-21 17:16:52,744 | Epoch: [2][15/48]\tTime  32.72 ( 32.91)\tData 0.0125 (0.0973)\tLoss (L1) 10.869 (12.980)\n",
      "2022-01-21 17:16:52,757 | ===> Batch : 16\n",
      "2022-01-21 17:16:52,757 | FDS enable\n",
      "2022-01-21 17:17:04,601 | Calculate Loss\n",
      "2022-01-21 17:17:04,603 | Update Loss\n",
      "2022-01-21 17:17:04,607 | Backward\n",
      "2022-01-21 17:17:25,550 | Epoch: [2][16/48]\tTime  32.81 ( 32.90)\tData 0.0127 (0.0920)\tLoss (L1) 12.249 (12.934)\n",
      "2022-01-21 17:17:25,564 | ===> Batch : 17\n",
      "2022-01-21 17:17:25,565 | FDS enable\n",
      "2022-01-21 17:17:37,467 | Calculate Loss\n",
      "2022-01-21 17:17:37,468 | Update Loss\n",
      "2022-01-21 17:17:37,473 | Backward\n",
      "2022-01-21 17:17:58,554 | Epoch: [2][17/48]\tTime  33.00 ( 32.91)\tData 0.0141 (0.0875)\tLoss (L1) 12.441 (12.905)\n",
      "2022-01-21 17:17:58,567 | ===> Batch : 18\n",
      "2022-01-21 17:17:58,568 | FDS enable\n",
      "2022-01-21 17:18:10,342 | Calculate Loss\n",
      "2022-01-21 17:18:10,343 | Update Loss\n",
      "2022-01-21 17:18:10,347 | Backward\n",
      "2022-01-21 17:18:31,006 | Epoch: [2][18/48]\tTime  32.45 ( 32.88)\tData 0.0135 (0.0834)\tLoss (L1) 11.231 (12.812)\n",
      "2022-01-21 17:18:31,020 | ===> Batch : 19\n",
      "2022-01-21 17:18:31,020 | FDS enable\n",
      "2022-01-21 17:18:42,763 | Calculate Loss\n",
      "2022-01-21 17:18:42,765 | Update Loss\n",
      "2022-01-21 17:18:42,769 | Backward\n",
      "2022-01-21 17:19:03,702 | Epoch: [2][19/48]\tTime  32.70 ( 32.87)\tData 0.0135 (0.0797)\tLoss (L1) 12.921 (12.818)\n",
      "2022-01-21 17:19:03,715 | ===> Batch : 20\n",
      "2022-01-21 17:19:03,716 | FDS enable\n",
      "2022-01-21 17:19:15,482 | Calculate Loss\n",
      "2022-01-21 17:19:15,484 | Update Loss\n",
      "2022-01-21 17:19:15,488 | Backward\n",
      "2022-01-21 17:19:36,360 | Epoch: [2][20/48]\tTime  32.66 ( 32.86)\tData 0.0138 (0.0764)\tLoss (L1) 11.201 (12.737)\n",
      "2022-01-21 17:19:36,374 | ===> Batch : 21\n",
      "2022-01-21 17:19:36,375 | FDS enable\n",
      "2022-01-21 17:19:48,098 | Calculate Loss\n",
      "2022-01-21 17:19:48,099 | Update Loss\n",
      "2022-01-21 17:19:48,103 | Backward\n",
      "2022-01-21 17:20:08,789 | Epoch: [2][21/48]\tTime  32.43 ( 32.84)\tData 0.0137 (0.0734)\tLoss (L1) 11.227 (12.665)\n",
      "2022-01-21 17:20:08,803 | ===> Batch : 22\n",
      "2022-01-21 17:20:08,803 | FDS enable\n",
      "2022-01-21 17:20:20,553 | Calculate Loss\n",
      "2022-01-21 17:20:20,555 | Update Loss\n",
      "2022-01-21 17:20:20,559 | Backward\n",
      "2022-01-21 17:20:41,375 | Epoch: [2][22/48]\tTime  32.59 ( 32.83)\tData 0.0134 (0.0707)\tLoss (L1) 13.486 (12.702)\n",
      "2022-01-21 17:20:41,389 | ===> Batch : 23\n",
      "2022-01-21 17:20:41,390 | FDS enable\n",
      "2022-01-21 17:20:53,128 | Calculate Loss\n",
      "2022-01-21 17:20:53,129 | Update Loss\n",
      "2022-01-21 17:20:53,134 | Backward\n",
      "2022-01-21 17:21:14,434 | Epoch: [2][23/48]\tTime  33.06 ( 32.84)\tData 0.0141 (0.0682)\tLoss (L1) 12.036 (12.673)\n",
      "2022-01-21 17:21:14,449 | ===> Batch : 24\n",
      "2022-01-21 17:21:14,450 | FDS enable\n",
      "2022-01-21 17:21:26,250 | Calculate Loss\n",
      "2022-01-21 17:21:26,251 | Update Loss\n",
      "2022-01-21 17:21:26,256 | Backward\n",
      "2022-01-21 17:21:46,706 | Epoch: [2][24/48]\tTime  32.27 ( 32.82)\tData 0.0148 (0.0660)\tLoss (L1) 13.884 (12.724)\n",
      "2022-01-21 17:21:46,721 | ===> Batch : 25\n",
      "2022-01-21 17:21:46,721 | FDS enable\n",
      "2022-01-21 17:21:58,430 | Calculate Loss\n",
      "2022-01-21 17:21:58,431 | Update Loss\n",
      "2022-01-21 17:21:58,436 | Backward\n",
      "2022-01-21 17:22:19,405 | Epoch: [2][25/48]\tTime  32.70 ( 32.81)\tData 0.0158 (0.0640)\tLoss (L1) 13.088 (12.738)\n",
      "2022-01-21 17:22:19,419 | ===> Batch : 26\n",
      "2022-01-21 17:22:19,419 | FDS enable\n",
      "2022-01-21 17:22:31,149 | Calculate Loss\n",
      "2022-01-21 17:22:31,150 | Update Loss\n",
      "2022-01-21 17:22:31,155 | Backward\n",
      "2022-01-21 17:22:52,001 | Epoch: [2][26/48]\tTime  32.60 ( 32.80)\tData 0.0139 (0.0621)\tLoss (L1) 11.617 (12.695)\n",
      "2022-01-21 17:22:52,015 | ===> Batch : 27\n",
      "2022-01-21 17:22:52,015 | FDS enable\n",
      "2022-01-21 17:23:03,811 | Calculate Loss\n",
      "2022-01-21 17:23:03,812 | Update Loss\n",
      "2022-01-21 17:23:03,816 | Backward\n",
      "2022-01-21 17:23:24,505 | Epoch: [2][27/48]\tTime  32.50 ( 32.79)\tData 0.0139 (0.0603)\tLoss (L1) 11.063 (12.635)\n",
      "2022-01-21 17:23:24,519 | ===> Batch : 28\n",
      "2022-01-21 17:23:24,519 | FDS enable\n",
      "2022-01-21 17:23:36,267 | Calculate Loss\n",
      "2022-01-21 17:23:36,268 | Update Loss\n",
      "2022-01-21 17:23:36,273 | Backward\n",
      "2022-01-21 17:23:57,362 | Epoch: [2][28/48]\tTime  32.86 ( 32.79)\tData 0.0136 (0.0586)\tLoss (L1) 13.760 (12.675)\n",
      "2022-01-21 17:23:57,375 | ===> Batch : 29\n",
      "2022-01-21 17:23:57,376 | FDS enable\n",
      "2022-01-21 17:24:09,127 | Calculate Loss\n",
      "2022-01-21 17:24:09,128 | Update Loss\n",
      "2022-01-21 17:24:09,132 | Backward\n",
      "2022-01-21 17:24:29,924 | Epoch: [2][29/48]\tTime  32.56 ( 32.79)\tData 0.0138 (0.0571)\tLoss (L1) 12.914 (12.683)\n",
      "2022-01-21 17:24:29,937 | ===> Batch : 30\n",
      "2022-01-21 17:24:29,938 | FDS enable\n",
      "2022-01-21 17:24:41,675 | Calculate Loss\n",
      "2022-01-21 17:24:41,676 | Update Loss\n",
      "2022-01-21 17:24:41,681 | Backward\n",
      "2022-01-21 17:25:02,458 | Epoch: [2][30/48]\tTime  32.53 ( 32.78)\tData 0.0136 (0.0556)\tLoss (L1) 14.505 (12.744)\n",
      "2022-01-21 17:25:02,478 | ===> Batch : 31\n",
      "2022-01-21 17:25:02,479 | FDS enable\n",
      "2022-01-21 17:25:14,215 | Calculate Loss\n",
      "2022-01-21 17:25:14,217 | Update Loss\n",
      "2022-01-21 17:25:14,221 | Backward\n",
      "2022-01-21 17:25:35,427 | Epoch: [2][31/48]\tTime  32.97 ( 32.78)\tData 0.0200 (0.0545)\tLoss (L1) 11.510 (12.704)\n",
      "2022-01-21 17:25:35,441 | ===> Batch : 32\n",
      "2022-01-21 17:25:35,442 | FDS enable\n",
      "2022-01-21 17:25:47,122 | Calculate Loss\n",
      "2022-01-21 17:25:47,124 | Update Loss\n",
      "2022-01-21 17:25:47,128 | Backward\n",
      "2022-01-21 17:26:07,713 | Epoch: [2][32/48]\tTime  32.29 ( 32.77)\tData 0.0140 (0.0532)\tLoss (L1) 14.592 (12.763)\n",
      "2022-01-21 17:26:07,728 | ===> Batch : 33\n",
      "2022-01-21 17:26:07,728 | FDS enable\n",
      "2022-01-21 17:26:19,363 | Calculate Loss\n",
      "2022-01-21 17:26:19,364 | Update Loss\n",
      "2022-01-21 17:26:19,369 | Backward\n",
      "2022-01-21 17:26:40,512 | Epoch: [2][33/48]\tTime  32.80 ( 32.77)\tData 0.0145 (0.0520)\tLoss (L1) 14.379 (12.812)\n",
      "2022-01-21 17:26:40,526 | ===> Batch : 34\n",
      "2022-01-21 17:26:40,527 | FDS enable\n",
      "2022-01-21 17:26:52,103 | Calculate Loss\n",
      "2022-01-21 17:26:52,105 | Update Loss\n",
      "2022-01-21 17:26:52,109 | Backward\n",
      "2022-01-21 17:27:12,591 | Epoch: [2][34/48]\tTime  32.08 ( 32.75)\tData 0.0141 (0.0509)\tLoss (L1) 13.891 (12.844)\n",
      "2022-01-21 17:27:12,606 | ===> Batch : 35\n",
      "2022-01-21 17:27:12,606 | FDS enable\n",
      "2022-01-21 17:27:24,185 | Calculate Loss\n",
      "2022-01-21 17:27:24,187 | Update Loss\n",
      "2022-01-21 17:27:24,192 | Backward\n",
      "2022-01-21 17:27:45,006 | Epoch: [2][35/48]\tTime  32.41 ( 32.74)\tData 0.0146 (0.0499)\tLoss (L1) 14.327 (12.886)\n",
      "2022-01-21 17:27:45,020 | ===> Batch : 36\n",
      "2022-01-21 17:27:45,020 | FDS enable\n",
      "2022-01-21 17:27:56,619 | Calculate Loss\n",
      "2022-01-21 17:27:56,621 | Update Loss\n",
      "2022-01-21 17:27:56,625 | Backward\n",
      "2022-01-21 17:28:17,495 | Epoch: [2][36/48]\tTime  32.49 ( 32.73)\tData 0.0134 (0.0489)\tLoss (L1) 13.560 (12.905)\n",
      "2022-01-21 17:28:17,509 | ===> Batch : 37\n",
      "2022-01-21 17:28:17,509 | FDS enable\n",
      "2022-01-21 17:28:29,092 | Calculate Loss\n",
      "2022-01-21 17:28:29,094 | Update Loss\n",
      "2022-01-21 17:28:29,098 | Backward\n",
      "2022-01-21 17:28:49,348 | Epoch: [2][37/48]\tTime  31.85 ( 32.71)\tData 0.0135 (0.0479)\tLoss (L1) 10.934 (12.852)\n",
      "2022-01-21 17:28:49,362 | ===> Batch : 38\n",
      "2022-01-21 17:28:49,362 | FDS enable\n",
      "2022-01-21 17:29:00,924 | Calculate Loss\n",
      "2022-01-21 17:29:00,928 | Update Loss\n",
      "2022-01-21 17:29:00,933 | Backward\n",
      "2022-01-21 17:29:22,135 | Epoch: [2][38/48]\tTime  32.79 ( 32.71)\tData 0.0135 (0.0470)\tLoss (L1) 12.594 (12.845)\n",
      "2022-01-21 17:29:22,149 | ===> Batch : 39\n",
      "2022-01-21 17:29:22,149 | FDS enable\n",
      "2022-01-21 17:29:33,697 | Calculate Loss\n",
      "2022-01-21 17:29:33,698 | Update Loss\n",
      "2022-01-21 17:29:33,703 | Backward\n",
      "2022-01-21 17:29:54,142 | Epoch: [2][39/48]\tTime  32.01 ( 32.69)\tData 0.0135 (0.0461)\tLoss (L1) 13.734 (12.868)\n",
      "2022-01-21 17:29:54,156 | ===> Batch : 40\n",
      "2022-01-21 17:29:54,157 | FDS enable\n",
      "2022-01-21 17:30:05,743 | Calculate Loss\n",
      "2022-01-21 17:30:05,745 | Update Loss\n",
      "2022-01-21 17:30:05,749 | Backward\n",
      "2022-01-21 17:30:26,825 | Epoch: [2][40/48]\tTime  32.68 ( 32.69)\tData 0.0143 (0.0453)\tLoss (L1) 13.560 (12.885)\n",
      "2022-01-21 17:30:26,840 | ===> Batch : 41\n",
      "2022-01-21 17:30:26,840 | FDS enable\n",
      "2022-01-21 17:30:38,437 | Calculate Loss\n",
      "2022-01-21 17:30:38,438 | Update Loss\n",
      "2022-01-21 17:30:38,443 | Backward\n",
      "2022-01-21 17:30:58,952 | Epoch: [2][41/48]\tTime  32.13 ( 32.68)\tData 0.0146 (0.0446)\tLoss (L1) 12.975 (12.887)\n",
      "2022-01-21 17:30:58,967 | ===> Batch : 42\n",
      "2022-01-21 17:30:58,968 | FDS enable\n",
      "2022-01-21 17:31:10,561 | Calculate Loss\n",
      "2022-01-21 17:31:10,563 | Update Loss\n",
      "2022-01-21 17:31:10,568 | Backward\n",
      "2022-01-21 17:31:31,387 | Epoch: [2][42/48]\tTime  32.43 ( 32.67)\tData 0.0148 (0.0439)\tLoss (L1) 11.629 (12.857)\n",
      "2022-01-21 17:31:31,401 | ===> Batch : 43\n",
      "2022-01-21 17:31:31,402 | FDS enable\n",
      "2022-01-21 17:31:42,985 | Calculate Loss\n",
      "2022-01-21 17:31:42,987 | Update Loss\n",
      "2022-01-21 17:31:42,991 | Backward\n",
      "2022-01-21 17:32:04,101 | Epoch: [2][43/48]\tTime  32.71 ( 32.67)\tData 0.0148 (0.0432)\tLoss (L1) 13.361 (12.869)\n",
      "2022-01-21 17:32:04,115 | ===> Batch : 44\n",
      "2022-01-21 17:32:04,115 | FDS enable\n",
      "2022-01-21 17:32:15,685 | Calculate Loss\n",
      "2022-01-21 17:32:15,687 | Update Loss\n",
      "2022-01-21 17:32:15,692 | Backward\n",
      "2022-01-21 17:32:36,430 | Epoch: [2][44/48]\tTime  32.33 ( 32.67)\tData 0.0137 (0.0425)\tLoss (L1) 12.608 (12.863)\n",
      "2022-01-21 17:32:36,444 | ===> Batch : 45\n",
      "2022-01-21 17:32:36,444 | FDS enable\n",
      "2022-01-21 17:32:48,104 | Calculate Loss\n",
      "2022-01-21 17:32:48,105 | Update Loss\n",
      "2022-01-21 17:32:48,110 | Backward\n",
      "2022-01-21 17:33:08,843 | Epoch: [2][45/48]\tTime  32.41 ( 32.66)\tData 0.0136 (0.0419)\tLoss (L1) 9.790 (12.795)\n",
      "2022-01-21 17:33:08,857 | ===> Batch : 46\n",
      "2022-01-21 17:33:08,857 | FDS enable\n",
      "2022-01-21 17:33:20,451 | Calculate Loss\n",
      "2022-01-21 17:33:20,453 | Update Loss\n",
      "2022-01-21 17:33:20,457 | Backward\n",
      "2022-01-21 17:33:41,128 | Epoch: [2][46/48]\tTime  32.28 ( 32.65)\tData 0.0137 (0.0413)\tLoss (L1) 12.539 (12.789)\n",
      "2022-01-21 17:33:41,142 | ===> Batch : 47\n",
      "2022-01-21 17:33:41,142 | FDS enable\n",
      "2022-01-21 17:33:52,705 | Calculate Loss\n",
      "2022-01-21 17:33:52,706 | Update Loss\n",
      "2022-01-21 17:33:52,711 | Backward\n",
      "2022-01-21 17:34:13,416 | Epoch: [2][47/48]\tTime  32.29 ( 32.64)\tData 0.0142 (0.0407)\tLoss (L1) 12.107 (12.775)\n",
      "2022-01-21 17:34:13,430 | ===> Batch : 48\n",
      "2022-01-21 17:34:13,430 | FDS enable\n",
      "2022-01-21 17:34:21,899 | Calculate Loss\n",
      "2022-01-21 17:34:21,900 | Update Loss\n",
      "2022-01-21 17:34:21,905 | Backward\n",
      "2022-01-21 17:34:36,055 | Epoch: [2][48/48]\tTime  22.64 ( 32.44)\tData 0.0136 (0.0401)\tLoss (L1) 13.068 (12.779)\n",
      "2022-01-21 17:34:36,199 | Create Epoch [2] features of all training data...\n",
      "100%|██████████| 48/48 [09:06<00:00, 11.40s/it]\n",
      "2022-01-21 17:43:43,233 | Updated smoothed statistics on Epoch [2]!\n",
      "2022-01-21 17:43:43,319 | Updated running statistics with Epoch [2] features!\n",
      "2022-01-21 17:43:55,169 | Val: [0/9]\tTime 11.838 (11.838)\tLoss (MSE) 274.815 (274.815)\tLoss (L1) 13.355 (13.355)\n",
      "2022-01-21 17:44:04,324 | Val: [1/9]\tTime  9.156 (10.497)\tLoss (MSE) 233.343 (254.079)\tLoss (L1) 12.228 (12.791)\n",
      "2022-01-21 17:44:13,449 | Val: [2/9]\tTime  9.125 (10.039)\tLoss (MSE) 291.340 (266.499)\tLoss (L1) 13.930 (13.171)\n",
      "2022-01-21 17:44:22,592 | Val: [3/9]\tTime  9.143 ( 9.815)\tLoss (MSE) 250.689 (262.547)\tLoss (L1) 13.026 (13.135)\n",
      "2022-01-21 17:44:31,739 | Val: [4/9]\tTime  9.147 ( 9.682)\tLoss (MSE) 300.845 (270.206)\tLoss (L1) 14.517 (13.411)\n",
      "2022-01-21 17:44:40,919 | Val: [5/9]\tTime  9.181 ( 9.598)\tLoss (MSE) 320.976 (278.668)\tLoss (L1) 14.415 (13.578)\n",
      "2022-01-21 17:44:50,082 | Val: [6/9]\tTime  9.163 ( 9.536)\tLoss (MSE) 271.675 (277.669)\tLoss (L1) 13.434 (13.558)\n",
      "2022-01-21 17:44:59,244 | Val: [7/9]\tTime  9.162 ( 9.489)\tLoss (MSE) 280.799 (278.060)\tLoss (L1) 13.341 (13.531)\n",
      "2022-01-21 17:45:02,349 | Val: [8/9]\tTime  3.105 ( 8.780)\tLoss (MSE) 280.211 (278.153)\tLoss (L1) 12.520 (13.487)\n",
      "2022-01-21 17:45:02,481 |  * Overall: MSE 278.153\tL1 13.487\tG-Mean 9.176\n",
      "2022-01-21 17:45:02,481 |  * Many: MSE 191.102\tL1 11.204\tG-Mean 7.620\n",
      "2022-01-21 17:45:02,482 |  * Median: MSE 450.670\tL1 18.053\tG-Mean 13.382\n",
      "2022-01-21 17:45:02,482 |  * Low: MSE 623.211\tL1 22.423\tG-Mean 18.686\n",
      "2022-01-21 17:45:02,487 | Best L1 Loss: 13.487\n",
      "2022-01-21 17:45:03,730 | ===> Saving current best checkpoint...\n",
      "2022-01-21 17:45:04,871 | Epoch #2: Train loss [12.7788]; Val loss: MSE [278.1527], L1 [13.4873], G-Mean [9.1759]\n",
      "2022-01-21 17:45:04,872 | Training...\n",
      "2022-01-21 17:45:04,873 | Load train loader\n",
      "2022-01-21 17:45:06,215 | ===> Batch : 1\n",
      "2022-01-21 17:45:06,217 | FDS enable\n",
      "2022-01-21 17:45:20,394 | Calculate Loss\n",
      "2022-01-21 17:45:20,395 | Update Loss\n",
      "2022-01-21 17:45:20,502 | Backward\n",
      "2022-01-21 17:45:42,004 | Epoch: [3][ 1/48]\tTime  37.13 ( 37.13)\tData 1.3419 (1.3419)\tLoss (L1) 10.964 (10.964)\n",
      "2022-01-21 17:45:42,017 | ===> Batch : 2\n",
      "2022-01-21 17:45:42,017 | FDS enable\n",
      "2022-01-21 17:45:53,712 | Calculate Loss\n",
      "2022-01-21 17:45:53,713 | Update Loss\n",
      "2022-01-21 17:45:53,718 | Backward\n",
      "2022-01-21 17:46:14,722 | Epoch: [3][ 2/48]\tTime  32.72 ( 34.92)\tData 0.0128 (0.6774)\tLoss (L1) 15.213 (13.089)\n",
      "2022-01-21 17:46:14,735 | ===> Batch : 3\n",
      "2022-01-21 17:46:14,736 | FDS enable\n",
      "2022-01-21 17:46:26,406 | Calculate Loss\n",
      "2022-01-21 17:46:26,407 | Update Loss\n",
      "2022-01-21 17:46:26,412 | Backward\n",
      "2022-01-21 17:46:47,172 | Epoch: [3][ 3/48]\tTime  32.45 ( 34.10)\tData 0.0133 (0.4560)\tLoss (L1) 13.206 (13.128)\n",
      "2022-01-21 17:46:47,185 | ===> Batch : 4\n",
      "2022-01-21 17:46:47,186 | FDS enable\n",
      "2022-01-21 17:46:58,910 | Calculate Loss\n",
      "2022-01-21 17:46:58,911 | Update Loss\n",
      "2022-01-21 17:46:58,916 | Backward\n",
      "2022-01-21 17:47:19,993 | Epoch: [3][ 4/48]\tTime  32.82 ( 33.78)\tData 0.0132 (0.3453)\tLoss (L1) 13.888 (13.318)\n",
      "2022-01-21 17:47:20,006 | ===> Batch : 5\n",
      "2022-01-21 17:47:20,006 | FDS enable\n",
      "2022-01-21 17:47:31,734 | Calculate Loss\n",
      "2022-01-21 17:47:31,736 | Update Loss\n",
      "2022-01-21 17:47:31,741 | Backward\n",
      "2022-01-21 17:47:52,722 | Epoch: [3][ 5/48]\tTime  32.73 ( 33.57)\tData 0.0129 (0.2788)\tLoss (L1) 13.435 (13.341)\n",
      "2022-01-21 17:47:52,736 | ===> Batch : 6\n",
      "2022-01-21 17:47:52,736 | FDS enable\n",
      "2022-01-21 17:48:04,446 | Calculate Loss\n",
      "2022-01-21 17:48:04,447 | Update Loss\n",
      "2022-01-21 17:48:04,452 | Backward\n",
      "2022-01-21 17:48:25,233 | Epoch: [3][ 6/48]\tTime  32.51 ( 33.39)\tData 0.0134 (0.2346)\tLoss (L1) 12.237 (13.157)\n",
      "2022-01-21 17:48:25,246 | ===> Batch : 7\n",
      "2022-01-21 17:48:25,247 | FDS enable\n",
      "2022-01-21 17:48:36,992 | Calculate Loss\n",
      "2022-01-21 17:48:36,994 | Update Loss\n",
      "2022-01-21 17:48:36,998 | Backward\n",
      "2022-01-21 17:48:57,786 | Epoch: [3][ 7/48]\tTime  32.55 ( 33.27)\tData 0.0131 (0.2029)\tLoss (L1) 11.049 (12.856)\n",
      "2022-01-21 17:48:57,800 | ===> Batch : 8\n",
      "2022-01-21 17:48:57,801 | FDS enable\n",
      "2022-01-21 17:49:09,515 | Calculate Loss\n",
      "2022-01-21 17:49:09,517 | Update Loss\n",
      "2022-01-21 17:49:09,521 | Backward\n",
      "2022-01-21 17:49:30,350 | Epoch: [3][ 8/48]\tTime  32.56 ( 33.18)\tData 0.0146 (0.1794)\tLoss (L1) 11.228 (12.653)\n",
      "2022-01-21 17:49:30,371 | ===> Batch : 9\n",
      "2022-01-21 17:49:30,372 | FDS enable\n",
      "2022-01-21 17:49:42,143 | Calculate Loss\n",
      "2022-01-21 17:49:42,145 | Update Loss\n",
      "2022-01-21 17:49:42,149 | Backward\n",
      "2022-01-21 17:50:03,159 | Epoch: [3][ 9/48]\tTime  32.81 ( 33.14)\tData 0.0213 (0.1618)\tLoss (L1) 13.309 (12.726)\n",
      "2022-01-21 17:50:03,171 | ===> Batch : 10\n",
      "2022-01-21 17:50:03,172 | FDS enable\n",
      "2022-01-21 17:50:14,910 | Calculate Loss\n",
      "2022-01-21 17:50:14,912 | Update Loss\n",
      "2022-01-21 17:50:14,916 | Backward\n",
      "2022-01-21 17:50:36,014 | Epoch: [3][10/48]\tTime  32.86 ( 33.11)\tData 0.0126 (0.1469)\tLoss (L1) 10.431 (12.496)\n",
      "2022-01-21 17:50:36,028 | ===> Batch : 11\n",
      "2022-01-21 17:50:36,028 | FDS enable\n",
      "2022-01-21 17:50:47,745 | Calculate Loss\n",
      "2022-01-21 17:50:47,747 | Update Loss\n",
      "2022-01-21 17:50:47,751 | Backward\n",
      "2022-01-21 17:51:08,100 | Epoch: [3][11/48]\tTime  32.09 ( 33.02)\tData 0.0141 (0.1348)\tLoss (L1) 14.091 (12.641)\n",
      "2022-01-21 17:51:08,113 | ===> Batch : 12\n",
      "2022-01-21 17:51:08,114 | FDS enable\n",
      "2022-01-21 17:51:19,884 | Calculate Loss\n",
      "2022-01-21 17:51:19,885 | Update Loss\n",
      "2022-01-21 17:51:19,890 | Backward\n",
      "2022-01-21 17:51:40,496 | Epoch: [3][12/48]\tTime  32.40 ( 32.97)\tData 0.0130 (0.1247)\tLoss (L1) 12.510 (12.630)\n",
      "2022-01-21 17:51:40,508 | ===> Batch : 13\n",
      "2022-01-21 17:51:40,509 | FDS enable\n",
      "2022-01-21 17:51:52,213 | Calculate Loss\n",
      "2022-01-21 17:51:52,214 | Update Loss\n",
      "2022-01-21 17:51:52,218 | Backward\n",
      "2022-01-21 17:52:13,193 | Epoch: [3][13/48]\tTime  32.70 ( 32.95)\tData 0.0127 (0.1161)\tLoss (L1) 11.963 (12.579)\n",
      "2022-01-21 17:52:13,205 | ===> Batch : 14\n",
      "2022-01-21 17:52:13,206 | FDS enable\n",
      "2022-01-21 17:52:24,935 | Calculate Loss\n",
      "2022-01-21 17:52:24,937 | Update Loss\n",
      "2022-01-21 17:52:24,941 | Backward\n",
      "2022-01-21 17:52:45,571 | Epoch: [3][14/48]\tTime  32.38 ( 32.91)\tData 0.0125 (0.1087)\tLoss (L1) 12.221 (12.553)\n",
      "2022-01-21 17:52:45,583 | ===> Batch : 15\n",
      "2022-01-21 17:52:45,584 | FDS enable\n",
      "2022-01-21 17:52:57,324 | Calculate Loss\n",
      "2022-01-21 17:52:57,325 | Update Loss\n",
      "2022-01-21 17:52:57,330 | Backward\n",
      "2022-01-21 17:53:17,960 | Epoch: [3][15/48]\tTime  32.39 ( 32.87)\tData 0.0125 (0.1023)\tLoss (L1) 11.155 (12.460)\n",
      "2022-01-21 17:53:17,972 | ===> Batch : 16\n",
      "2022-01-21 17:53:17,973 | FDS enable\n",
      "2022-01-21 17:53:29,672 | Calculate Loss\n",
      "2022-01-21 17:53:29,673 | Update Loss\n",
      "2022-01-21 17:53:29,678 | Backward\n",
      "2022-01-21 17:53:50,520 | Epoch: [3][16/48]\tTime  32.56 ( 32.85)\tData 0.0128 (0.0967)\tLoss (L1) 11.679 (12.411)\n",
      "2022-01-21 17:53:50,534 | ===> Batch : 17\n",
      "2022-01-21 17:53:50,535 | FDS enable\n",
      "2022-01-21 17:54:02,298 | Calculate Loss\n",
      "2022-01-21 17:54:02,299 | Update Loss\n",
      "2022-01-21 17:54:02,304 | Backward\n",
      "2022-01-21 17:54:23,356 | Epoch: [3][17/48]\tTime  32.84 ( 32.85)\tData 0.0139 (0.0918)\tLoss (L1) 12.040 (12.389)\n",
      "2022-01-21 17:54:23,369 | ===> Batch : 18\n",
      "2022-01-21 17:54:23,370 | FDS enable\n",
      "2022-01-21 17:54:35,072 | Calculate Loss\n",
      "2022-01-21 17:54:35,074 | Update Loss\n",
      "2022-01-21 17:54:35,078 | Backward\n",
      "2022-01-21 17:54:56,037 | Epoch: [3][18/48]\tTime  32.68 ( 32.84)\tData 0.0136 (0.0875)\tLoss (L1) 11.392 (12.334)\n",
      "2022-01-21 17:54:56,052 | ===> Batch : 19\n",
      "2022-01-21 17:54:56,053 | FDS enable\n",
      "2022-01-21 17:55:07,803 | Calculate Loss\n",
      "2022-01-21 17:55:07,805 | Update Loss\n",
      "2022-01-21 17:55:07,809 | Backward\n",
      "2022-01-21 17:55:28,819 | Epoch: [3][19/48]\tTime  32.78 ( 32.84)\tData 0.0151 (0.0836)\tLoss (L1) 12.097 (12.322)\n",
      "2022-01-21 17:55:28,833 | ===> Batch : 20\n",
      "2022-01-21 17:55:28,834 | FDS enable\n",
      "2022-01-21 17:55:40,641 | Calculate Loss\n",
      "2022-01-21 17:55:40,642 | Update Loss\n",
      "2022-01-21 17:55:40,647 | Backward\n",
      "2022-01-21 17:56:00,980 | Epoch: [3][20/48]\tTime  32.16 ( 32.81)\tData 0.0144 (0.0802)\tLoss (L1) 12.367 (12.324)\n",
      "2022-01-21 17:56:00,995 | ===> Batch : 21\n",
      "2022-01-21 17:56:00,995 | FDS enable\n",
      "2022-01-21 17:56:12,751 | Calculate Loss\n",
      "2022-01-21 17:56:12,752 | Update Loss\n",
      "2022-01-21 17:56:12,757 | Backward\n",
      "2022-01-21 17:56:33,508 | Epoch: [3][21/48]\tTime  32.53 ( 32.79)\tData 0.0148 (0.0771)\tLoss (L1) 11.955 (12.306)\n",
      "2022-01-21 17:56:33,522 | ===> Batch : 22\n",
      "2022-01-21 17:56:33,523 | FDS enable\n",
      "2022-01-21 17:56:45,274 | Calculate Loss\n",
      "2022-01-21 17:56:45,276 | Update Loss\n",
      "2022-01-21 17:56:45,280 | Backward\n",
      "2022-01-21 17:57:06,621 | Epoch: [3][22/48]\tTime  33.11 ( 32.81)\tData 0.0141 (0.0742)\tLoss (L1) 12.930 (12.335)\n",
      "2022-01-21 17:57:06,635 | ===> Batch : 23\n",
      "2022-01-21 17:57:06,636 | FDS enable\n",
      "2022-01-21 17:57:18,351 | Calculate Loss\n",
      "2022-01-21 17:57:18,352 | Update Loss\n",
      "2022-01-21 17:57:18,357 | Backward\n",
      "2022-01-21 17:57:39,166 | Epoch: [3][23/48]\tTime  32.54 ( 32.80)\tData 0.0140 (0.0716)\tLoss (L1) 12.006 (12.320)\n",
      "2022-01-21 17:57:39,180 | ===> Batch : 24\n",
      "2022-01-21 17:57:39,181 | FDS enable\n",
      "2022-01-21 17:57:50,926 | Calculate Loss\n",
      "2022-01-21 17:57:50,927 | Update Loss\n",
      "2022-01-21 17:57:50,932 | Backward\n",
      "2022-01-21 17:58:11,484 | Epoch: [3][24/48]\tTime  32.32 ( 32.78)\tData 0.0140 (0.0692)\tLoss (L1) 11.585 (12.290)\n",
      "2022-01-21 17:58:11,498 | ===> Batch : 25\n",
      "2022-01-21 17:58:11,498 | FDS enable\n",
      "2022-01-21 17:58:23,250 | Calculate Loss\n",
      "2022-01-21 17:58:23,252 | Update Loss\n",
      "2022-01-21 17:58:23,256 | Backward\n",
      "2022-01-21 17:58:43,731 | Epoch: [3][25/48]\tTime  32.25 ( 32.75)\tData 0.0136 (0.0670)\tLoss (L1) 14.114 (12.363)\n",
      "2022-01-21 17:58:43,745 | ===> Batch : 26\n",
      "2022-01-21 17:58:43,746 | FDS enable\n",
      "2022-01-21 17:58:55,488 | Calculate Loss\n",
      "2022-01-21 17:58:55,490 | Update Loss\n",
      "2022-01-21 17:58:55,494 | Backward\n",
      "2022-01-21 17:59:16,573 | Epoch: [3][26/48]\tTime  32.84 ( 32.76)\tData 0.0141 (0.0649)\tLoss (L1) 11.872 (12.344)\n",
      "2022-01-21 17:59:16,587 | ===> Batch : 27\n",
      "2022-01-21 17:59:16,587 | FDS enable\n",
      "2022-01-21 17:59:28,342 | Calculate Loss\n",
      "2022-01-21 17:59:28,343 | Update Loss\n",
      "2022-01-21 17:59:28,348 | Backward\n",
      "2022-01-21 17:59:49,123 | Epoch: [3][27/48]\tTime  32.55 ( 32.75)\tData 0.0135 (0.0630)\tLoss (L1) 12.087 (12.334)\n",
      "2022-01-21 17:59:49,137 | ===> Batch : 28\n",
      "2022-01-21 17:59:49,138 | FDS enable\n",
      "2022-01-21 18:00:00,861 | Calculate Loss\n",
      "2022-01-21 18:00:00,863 | Update Loss\n",
      "2022-01-21 18:00:00,867 | Backward\n",
      "2022-01-21 18:00:21,817 | Epoch: [3][28/48]\tTime  32.69 ( 32.75)\tData 0.0139 (0.0613)\tLoss (L1) 13.392 (12.372)\n",
      "2022-01-21 18:00:21,831 | ===> Batch : 29\n",
      "2022-01-21 18:00:21,832 | FDS enable\n",
      "2022-01-21 18:00:33,587 | Calculate Loss\n",
      "2022-01-21 18:00:33,588 | Update Loss\n",
      "2022-01-21 18:00:33,593 | Backward\n",
      "2022-01-21 18:00:54,258 | Epoch: [3][29/48]\tTime  32.44 ( 32.74)\tData 0.0142 (0.0597)\tLoss (L1) 15.401 (12.477)\n",
      "2022-01-21 18:00:54,272 | ===> Batch : 30\n",
      "2022-01-21 18:00:54,273 | FDS enable\n",
      "2022-01-21 18:01:06,024 | Calculate Loss\n",
      "2022-01-21 18:01:06,026 | Update Loss\n",
      "2022-01-21 18:01:06,030 | Backward\n",
      "2022-01-21 18:01:26,776 | Epoch: [3][30/48]\tTime  32.52 ( 32.73)\tData 0.0139 (0.0581)\tLoss (L1) 13.364 (12.506)\n",
      "2022-01-21 18:01:26,790 | ===> Batch : 31\n",
      "2022-01-21 18:01:26,790 | FDS enable\n",
      "2022-01-21 18:01:38,523 | Calculate Loss\n",
      "2022-01-21 18:01:38,524 | Update Loss\n",
      "2022-01-21 18:01:38,529 | Backward\n",
      "2022-01-21 18:01:59,262 | Epoch: [3][31/48]\tTime  32.49 ( 32.72)\tData 0.0137 (0.0567)\tLoss (L1) 13.123 (12.526)\n",
      "2022-01-21 18:01:59,277 | ===> Batch : 32\n",
      "2022-01-21 18:01:59,278 | FDS enable\n",
      "2022-01-21 18:02:10,888 | Calculate Loss\n",
      "2022-01-21 18:02:10,890 | Update Loss\n",
      "2022-01-21 18:02:10,894 | Backward\n",
      "2022-01-21 18:02:32,151 | Epoch: [3][32/48]\tTime  32.89 ( 32.73)\tData 0.0153 (0.0554)\tLoss (L1) 13.704 (12.563)\n",
      "2022-01-21 18:02:32,164 | ===> Batch : 33\n",
      "2022-01-21 18:02:32,165 | FDS enable\n",
      "2022-01-21 18:02:43,758 | Calculate Loss\n",
      "2022-01-21 18:02:43,760 | Update Loss\n",
      "2022-01-21 18:02:43,765 | Backward\n",
      "2022-01-21 18:03:04,599 | Epoch: [3][33/48]\tTime  32.45 ( 32.72)\tData 0.0134 (0.0541)\tLoss (L1) 12.771 (12.569)\n",
      "2022-01-21 18:03:04,612 | ===> Batch : 34\n",
      "2022-01-21 18:03:04,613 | FDS enable\n",
      "2022-01-21 18:03:16,199 | Calculate Loss\n",
      "2022-01-21 18:03:16,200 | Update Loss\n",
      "2022-01-21 18:03:16,205 | Backward\n",
      "2022-01-21 18:03:36,993 | Epoch: [3][34/48]\tTime  32.39 ( 32.71)\tData 0.0135 (0.0529)\tLoss (L1) 12.907 (12.579)\n",
      "2022-01-21 18:03:37,007 | ===> Batch : 35\n",
      "2022-01-21 18:03:37,007 | FDS enable\n",
      "2022-01-21 18:03:48,622 | Calculate Loss\n",
      "2022-01-21 18:03:48,624 | Update Loss\n",
      "2022-01-21 18:03:48,628 | Backward\n",
      "2022-01-21 18:04:09,216 | Epoch: [3][35/48]\tTime  32.22 ( 32.70)\tData 0.0135 (0.0518)\tLoss (L1) 10.930 (12.532)\n",
      "2022-01-21 18:04:09,230 | ===> Batch : 36\n",
      "2022-01-21 18:04:09,231 | FDS enable\n",
      "2022-01-21 18:04:20,819 | Calculate Loss\n",
      "2022-01-21 18:04:20,820 | Update Loss\n",
      "2022-01-21 18:04:20,825 | Backward\n",
      "2022-01-21 18:04:41,998 | Epoch: [3][36/48]\tTime  32.78 ( 32.70)\tData 0.0138 (0.0508)\tLoss (L1) 11.544 (12.505)\n",
      "2022-01-21 18:04:42,013 | ===> Batch : 37\n",
      "2022-01-21 18:04:42,013 | FDS enable\n",
      "2022-01-21 18:04:53,603 | Calculate Loss\n",
      "2022-01-21 18:04:53,604 | Update Loss\n",
      "2022-01-21 18:04:53,609 | Backward\n",
      "2022-01-21 18:05:14,377 | Epoch: [3][37/48]\tTime  32.38 ( 32.69)\tData 0.0149 (0.0498)\tLoss (L1) 13.321 (12.527)\n",
      "2022-01-21 18:05:14,390 | ===> Batch : 38\n",
      "2022-01-21 18:05:14,391 | FDS enable\n",
      "2022-01-21 18:05:25,996 | Calculate Loss\n",
      "2022-01-21 18:05:25,998 | Update Loss\n",
      "2022-01-21 18:05:26,003 | Backward\n",
      "2022-01-21 18:05:47,015 | Epoch: [3][38/48]\tTime  32.64 ( 32.69)\tData 0.0138 (0.0488)\tLoss (L1) 10.464 (12.472)\n",
      "2022-01-21 18:05:47,028 | ===> Batch : 39\n",
      "2022-01-21 18:05:47,029 | FDS enable\n",
      "2022-01-21 18:05:58,629 | Calculate Loss\n",
      "2022-01-21 18:05:58,630 | Update Loss\n",
      "2022-01-21 18:05:58,634 | Backward\n",
      "2022-01-21 18:06:19,269 | Epoch: [3][39/48]\tTime  32.25 ( 32.68)\tData 0.0135 (0.0479)\tLoss (L1) 12.219 (12.466)\n",
      "2022-01-21 18:06:19,282 | ===> Batch : 40\n",
      "2022-01-21 18:06:19,283 | FDS enable\n",
      "2022-01-21 18:06:30,867 | Calculate Loss\n",
      "2022-01-21 18:06:30,868 | Update Loss\n",
      "2022-01-21 18:06:30,873 | Backward\n",
      "2022-01-21 18:06:51,807 | Epoch: [3][40/48]\tTime  32.54 ( 32.67)\tData 0.0135 (0.0471)\tLoss (L1) 11.241 (12.435)\n",
      "2022-01-21 18:06:51,822 | ===> Batch : 41\n",
      "2022-01-21 18:06:51,822 | FDS enable\n",
      "2022-01-21 18:07:03,430 | Calculate Loss\n",
      "2022-01-21 18:07:03,431 | Update Loss\n",
      "2022-01-21 18:07:03,436 | Backward\n",
      "2022-01-21 18:07:24,530 | Epoch: [3][41/48]\tTime  32.72 ( 32.67)\tData 0.0146 (0.0463)\tLoss (L1) 11.744 (12.418)\n",
      "2022-01-21 18:07:24,543 | ===> Batch : 42\n",
      "2022-01-21 18:07:24,544 | FDS enable\n",
      "2022-01-21 18:07:36,122 | Calculate Loss\n",
      "2022-01-21 18:07:36,123 | Update Loss\n",
      "2022-01-21 18:07:36,128 | Backward\n",
      "2022-01-21 18:07:56,849 | Epoch: [3][42/48]\tTime  32.32 ( 32.67)\tData 0.0134 (0.0455)\tLoss (L1) 12.057 (12.410)\n",
      "2022-01-21 18:07:56,863 | ===> Batch : 43\n",
      "2022-01-21 18:07:56,864 | FDS enable\n",
      "2022-01-21 18:08:08,416 | Calculate Loss\n",
      "2022-01-21 18:08:08,417 | Update Loss\n",
      "2022-01-21 18:08:08,422 | Backward\n",
      "2022-01-21 18:08:29,500 | Epoch: [3][43/48]\tTime  32.65 ( 32.67)\tData 0.0140 (0.0448)\tLoss (L1) 12.588 (12.414)\n",
      "2022-01-21 18:08:29,514 | ===> Batch : 44\n",
      "2022-01-21 18:08:29,515 | FDS enable\n",
      "2022-01-21 18:08:41,093 | Calculate Loss\n",
      "2022-01-21 18:08:41,095 | Update Loss\n",
      "2022-01-21 18:08:41,099 | Backward\n",
      "2022-01-21 18:09:01,727 | Epoch: [3][44/48]\tTime  32.23 ( 32.66)\tData 0.0140 (0.0441)\tLoss (L1) 14.920 (12.471)\n",
      "2022-01-21 18:09:01,741 | ===> Batch : 45\n",
      "2022-01-21 18:09:01,741 | FDS enable\n",
      "2022-01-21 18:09:13,323 | Calculate Loss\n",
      "2022-01-21 18:09:13,325 | Update Loss\n",
      "2022-01-21 18:09:13,329 | Backward\n",
      "2022-01-21 18:09:34,438 | Epoch: [3][45/48]\tTime  32.71 ( 32.66)\tData 0.0136 (0.0434)\tLoss (L1) 10.644 (12.430)\n",
      "2022-01-21 18:09:34,452 | ===> Batch : 46\n",
      "2022-01-21 18:09:34,452 | FDS enable\n",
      "2022-01-21 18:09:46,045 | Calculate Loss\n",
      "2022-01-21 18:09:46,046 | Update Loss\n",
      "2022-01-21 18:09:46,051 | Backward\n",
      "2022-01-21 18:10:06,909 | Epoch: [3][46/48]\tTime  32.47 ( 32.65)\tData 0.0137 (0.0427)\tLoss (L1) 11.967 (12.420)\n",
      "2022-01-21 18:10:06,924 | ===> Batch : 47\n",
      "2022-01-21 18:10:06,925 | FDS enable\n",
      "2022-01-21 18:10:18,505 | Calculate Loss\n",
      "2022-01-21 18:10:18,507 | Update Loss\n",
      "2022-01-21 18:10:18,511 | Backward\n",
      "2022-01-21 18:10:39,318 | Epoch: [3][47/48]\tTime  32.41 ( 32.65)\tData 0.0148 (0.0421)\tLoss (L1) 14.668 (12.468)\n",
      "2022-01-21 18:10:39,332 | ===> Batch : 48\n",
      "2022-01-21 18:10:39,333 | FDS enable\n",
      "2022-01-21 18:10:47,860 | Calculate Loss\n",
      "2022-01-21 18:10:47,862 | Update Loss\n",
      "2022-01-21 18:10:47,866 | Backward\n",
      "2022-01-21 18:11:02,143 | Epoch: [3][48/48]\tTime  22.82 ( 32.44)\tData 0.0141 (0.0416)\tLoss (L1) 12.212 (12.464)\n",
      "2022-01-21 18:11:02,291 | Create Epoch [3] features of all training data...\n",
      "100%|██████████| 48/48 [09:20<00:00, 11.68s/it]\n",
      "2022-01-21 18:20:23,148 | Updated smoothed statistics on Epoch [3]!\n",
      "2022-01-21 18:20:23,234 | Updated running statistics with Epoch [3] features!\n",
      "2022-01-21 18:20:35,121 | Val: [0/9]\tTime 11.875 (11.875)\tLoss (MSE) 250.110 (250.110)\tLoss (L1) 12.618 (12.618)\n",
      "2022-01-21 18:20:44,256 | Val: [1/9]\tTime  9.135 (10.505)\tLoss (MSE) 227.523 (238.817)\tLoss (L1) 11.889 (12.253)\n",
      "2022-01-21 18:20:53,416 | Val: [2/9]\tTime  9.159 (10.056)\tLoss (MSE) 218.249 (231.961)\tLoss (L1) 11.873 (12.127)\n",
      "2022-01-21 18:21:02,589 | Val: [3/9]\tTime  9.173 ( 9.836)\tLoss (MSE) 244.252 (235.034)\tLoss (L1) 12.673 (12.263)\n",
      "2022-01-21 18:21:11,747 | Val: [4/9]\tTime  9.158 ( 9.700)\tLoss (MSE) 281.389 (244.305)\tLoss (L1) 13.618 (12.534)\n",
      "2022-01-21 18:21:20,922 | Val: [5/9]\tTime  9.176 ( 9.613)\tLoss (MSE) 289.448 (251.829)\tLoss (L1) 13.798 (12.745)\n",
      "2022-01-21 18:21:30,097 | Val: [6/9]\tTime  9.175 ( 9.550)\tLoss (MSE) 237.640 (249.802)\tLoss (L1) 12.151 (12.660)\n",
      "2022-01-21 18:21:39,247 | Val: [7/9]\tTime  9.149 ( 9.500)\tLoss (MSE) 266.263 (251.860)\tLoss (L1) 12.906 (12.691)\n",
      "2022-01-21 18:21:42,337 | Val: [8/9]\tTime  3.091 ( 8.788)\tLoss (MSE) 268.606 (252.579)\tLoss (L1) 12.106 (12.666)\n",
      "2022-01-21 18:21:42,477 |  * Overall: MSE 252.579\tL1 12.666\tG-Mean 8.392\n",
      "2022-01-21 18:21:42,478 |  * Many: MSE 176.777\tL1 10.558\tG-Mean 6.987\n",
      "2022-01-21 18:21:42,478 |  * Median: MSE 404.572\tL1 16.861\tG-Mean 11.978\n",
      "2022-01-21 18:21:42,479 |  * Low: MSE 548.170\tL1 20.970\tG-Mean 17.707\n",
      "2022-01-21 18:21:42,484 | Best L1 Loss: 12.666\n",
      "2022-01-21 18:21:43,727 | ===> Saving current best checkpoint...\n",
      "2022-01-21 18:21:44,871 | Epoch #3: Train loss [12.4643]; Val loss: MSE [252.5795], L1 [12.6656], G-Mean [8.3923]\n",
      "2022-01-21 18:21:44,874 | Training...\n",
      "2022-01-21 18:21:44,875 | Load train loader\n",
      "2022-01-21 18:21:46,180 | ===> Batch : 1\n",
      "2022-01-21 18:21:46,193 | FDS enable\n",
      "2022-01-21 18:22:00,381 | Calculate Loss\n",
      "2022-01-21 18:22:00,382 | Update Loss\n",
      "2022-01-21 18:22:00,490 | Backward\n",
      "2022-01-21 18:22:22,021 | Epoch: [4][ 1/48]\tTime  37.15 ( 37.15)\tData 1.3048 (1.3048)\tLoss (L1) 11.105 (11.105)\n",
      "2022-01-21 18:22:22,034 | ===> Batch : 2\n",
      "2022-01-21 18:22:22,035 | FDS enable\n",
      "2022-01-21 18:22:33,716 | Calculate Loss\n",
      "2022-01-21 18:22:33,717 | Update Loss\n",
      "2022-01-21 18:22:33,722 | Backward\n",
      "2022-01-21 18:22:54,736 | Epoch: [4][ 2/48]\tTime  32.71 ( 34.93)\tData 0.0132 (0.6590)\tLoss (L1) 12.334 (11.719)\n",
      "2022-01-21 18:22:54,749 | ===> Batch : 3\n",
      "2022-01-21 18:22:54,749 | FDS enable\n",
      "2022-01-21 18:23:06,487 | Calculate Loss\n",
      "2022-01-21 18:23:06,488 | Update Loss\n",
      "2022-01-21 18:23:06,492 | Backward\n",
      "2022-01-21 18:23:27,541 | Epoch: [4][ 3/48]\tTime  32.81 ( 34.22)\tData 0.0130 (0.4437)\tLoss (L1) 13.452 (12.297)\n",
      "2022-01-21 18:23:27,556 | ===> Batch : 4\n",
      "2022-01-21 18:23:27,556 | FDS enable\n",
      "2022-01-21 18:23:39,277 | Calculate Loss\n",
      "2022-01-21 18:23:39,280 | Update Loss\n",
      "2022-01-21 18:23:39,285 | Backward\n",
      "2022-01-21 18:24:00,158 | Epoch: [4][ 4/48]\tTime  32.62 ( 33.82)\tData 0.0147 (0.3364)\tLoss (L1) 12.388 (12.320)\n",
      "2022-01-21 18:24:00,171 | ===> Batch : 5\n",
      "2022-01-21 18:24:00,172 | FDS enable\n",
      "2022-01-21 18:24:11,901 | Calculate Loss\n",
      "2022-01-21 18:24:11,903 | Update Loss\n",
      "2022-01-21 18:24:11,907 | Backward\n",
      "2022-01-21 18:24:32,935 | Epoch: [4][ 5/48]\tTime  32.78 ( 33.61)\tData 0.0132 (0.2718)\tLoss (L1) 12.421 (12.340)\n",
      "2022-01-21 18:24:32,950 | ===> Batch : 6\n",
      "2022-01-21 18:24:32,951 | FDS enable\n",
      "2022-01-21 18:24:44,744 | Calculate Loss\n",
      "2022-01-21 18:24:44,745 | Update Loss\n",
      "2022-01-21 18:24:44,750 | Backward\n",
      "2022-01-21 18:25:05,648 | Epoch: [4][ 6/48]\tTime  32.71 ( 33.46)\tData 0.0156 (0.2291)\tLoss (L1) 14.036 (12.623)\n",
      "2022-01-21 18:25:05,661 | ===> Batch : 7\n",
      "2022-01-21 18:25:05,662 | FDS enable\n",
      "2022-01-21 18:25:17,421 | Calculate Loss\n",
      "2022-01-21 18:25:17,423 | Update Loss\n",
      "2022-01-21 18:25:17,427 | Backward\n",
      "2022-01-21 18:25:38,631 | Epoch: [4][ 7/48]\tTime  32.98 ( 33.39)\tData 0.0129 (0.1982)\tLoss (L1) 11.202 (12.420)\n",
      "2022-01-21 18:25:38,644 | ===> Batch : 8\n",
      "2022-01-21 18:25:38,645 | FDS enable\n",
      "2022-01-21 18:25:50,379 | Calculate Loss\n",
      "2022-01-21 18:25:50,381 | Update Loss\n",
      "2022-01-21 18:25:50,386 | Backward\n",
      "2022-01-21 18:26:11,307 | Epoch: [4][ 8/48]\tTime  32.68 ( 33.30)\tData 0.0129 (0.1750)\tLoss (L1) 12.163 (12.388)\n",
      "2022-01-21 18:26:11,325 | ===> Batch : 9\n",
      "2022-01-21 18:26:11,326 | FDS enable\n",
      "2022-01-21 18:26:23,062 | Calculate Loss\n",
      "2022-01-21 18:26:23,063 | Update Loss\n",
      "2022-01-21 18:26:23,068 | Backward\n",
      "2022-01-21 18:26:43,869 | Epoch: [4][ 9/48]\tTime  32.56 ( 33.22)\tData 0.0181 (0.1576)\tLoss (L1) 11.526 (12.292)\n",
      "2022-01-21 18:26:43,882 | ===> Batch : 10\n",
      "2022-01-21 18:26:43,883 | FDS enable\n",
      "2022-01-21 18:26:55,618 | Calculate Loss\n",
      "2022-01-21 18:26:55,620 | Update Loss\n",
      "2022-01-21 18:26:55,625 | Backward\n",
      "2022-01-21 18:27:16,496 | Epoch: [4][10/48]\tTime  32.63 ( 33.16)\tData 0.0127 (0.1431)\tLoss (L1) 11.395 (12.202)\n",
      "2022-01-21 18:27:16,509 | ===> Batch : 11\n",
      "2022-01-21 18:27:16,509 | FDS enable\n",
      "2022-01-21 18:27:28,268 | Calculate Loss\n",
      "2022-01-21 18:27:28,270 | Update Loss\n",
      "2022-01-21 18:27:28,274 | Backward\n",
      "2022-01-21 18:27:48,980 | Epoch: [4][11/48]\tTime  32.48 ( 33.10)\tData 0.0128 (0.1313)\tLoss (L1) 10.771 (12.072)\n",
      "2022-01-21 18:27:49,001 | ===> Batch : 12\n",
      "2022-01-21 18:27:49,002 | FDS enable\n",
      "2022-01-21 18:28:00,739 | Calculate Loss\n",
      "2022-01-21 18:28:00,740 | Update Loss\n",
      "2022-01-21 18:28:00,745 | Backward\n",
      "2022-01-21 18:28:21,812 | Epoch: [4][12/48]\tTime  32.83 ( 33.08)\tData 0.0218 (0.1221)\tLoss (L1) 10.391 (11.932)\n",
      "2022-01-21 18:28:21,824 | ===> Batch : 13\n",
      "2022-01-21 18:28:21,825 | FDS enable\n",
      "2022-01-21 18:28:33,555 | Calculate Loss\n",
      "2022-01-21 18:28:33,557 | Update Loss\n",
      "2022-01-21 18:28:33,561 | Backward\n",
      "2022-01-21 18:28:54,497 | Epoch: [4][13/48]\tTime  32.68 ( 33.05)\tData 0.0124 (0.1137)\tLoss (L1) 12.071 (11.943)\n",
      "2022-01-21 18:28:54,510 | ===> Batch : 14\n",
      "2022-01-21 18:28:54,510 | FDS enable\n",
      "2022-01-21 18:29:06,251 | Calculate Loss\n",
      "2022-01-21 18:29:06,252 | Update Loss\n",
      "2022-01-21 18:29:06,257 | Backward\n",
      "2022-01-21 18:29:26,984 | Epoch: [4][14/48]\tTime  32.49 ( 33.01)\tData 0.0129 (0.1065)\tLoss (L1) 11.640 (11.921)\n",
      "2022-01-21 18:29:26,997 | ===> Batch : 15\n",
      "2022-01-21 18:29:26,998 | FDS enable\n",
      "2022-01-21 18:29:38,762 | Calculate Loss\n",
      "2022-01-21 18:29:38,763 | Update Loss\n",
      "2022-01-21 18:29:38,768 | Backward\n",
      "2022-01-21 18:29:59,447 | Epoch: [4][15/48]\tTime  32.46 ( 32.97)\tData 0.0126 (0.1002)\tLoss (L1) 12.806 (11.980)\n",
      "2022-01-21 18:29:59,460 | ===> Batch : 16\n",
      "2022-01-21 18:29:59,460 | FDS enable\n",
      "2022-01-21 18:30:11,244 | Calculate Loss\n",
      "2022-01-21 18:30:11,246 | Update Loss\n",
      "2022-01-21 18:30:11,250 | Backward\n",
      "2022-01-21 18:30:32,022 | Epoch: [4][16/48]\tTime  32.57 ( 32.95)\tData 0.0127 (0.0948)\tLoss (L1) 13.593 (12.081)\n",
      "2022-01-21 18:30:32,037 | ===> Batch : 17\n",
      "2022-01-21 18:30:32,038 | FDS enable\n",
      "2022-01-21 18:30:43,855 | Calculate Loss\n",
      "2022-01-21 18:30:43,857 | Update Loss\n",
      "2022-01-21 18:30:43,861 | Backward\n",
      "2022-01-21 18:31:04,512 | Epoch: [4][17/48]\tTime  32.49 ( 32.92)\tData 0.0151 (0.0901)\tLoss (L1) 13.205 (12.147)\n",
      "2022-01-21 18:31:04,525 | ===> Batch : 18\n",
      "2022-01-21 18:31:04,526 | FDS enable\n",
      "2022-01-21 18:31:16,287 | Calculate Loss\n",
      "2022-01-21 18:31:16,288 | Update Loss\n",
      "2022-01-21 18:31:16,293 | Backward\n",
      "2022-01-21 18:31:37,241 | Epoch: [4][18/48]\tTime  32.73 ( 32.91)\tData 0.0140 (0.0859)\tLoss (L1) 13.801 (12.239)\n",
      "2022-01-21 18:31:37,256 | ===> Batch : 19\n",
      "2022-01-21 18:31:37,256 | FDS enable\n",
      "2022-01-21 18:31:48,995 | Calculate Loss\n",
      "2022-01-21 18:31:48,996 | Update Loss\n",
      "2022-01-21 18:31:49,001 | Backward\n",
      "2022-01-21 18:32:09,729 | Epoch: [4][19/48]\tTime  32.49 ( 32.89)\tData 0.0151 (0.0821)\tLoss (L1) 12.616 (12.259)\n",
      "2022-01-21 18:32:09,744 | ===> Batch : 20\n",
      "2022-01-21 18:32:09,744 | FDS enable\n",
      "2022-01-21 18:32:21,536 | Calculate Loss\n",
      "2022-01-21 18:32:21,537 | Update Loss\n",
      "2022-01-21 18:32:21,542 | Backward\n",
      "2022-01-21 18:32:42,301 | Epoch: [4][20/48]\tTime  32.57 ( 32.87)\tData 0.0154 (0.0788)\tLoss (L1) 15.417 (12.417)\n",
      "2022-01-21 18:32:42,315 | ===> Batch : 21\n",
      "2022-01-21 18:32:42,315 | FDS enable\n",
      "2022-01-21 18:32:54,064 | Calculate Loss\n",
      "2022-01-21 18:32:54,066 | Update Loss\n",
      "2022-01-21 18:32:54,070 | Backward\n",
      "2022-01-21 18:33:14,907 | Epoch: [4][21/48]\tTime  32.61 ( 32.86)\tData 0.0136 (0.0757)\tLoss (L1) 12.012 (12.397)\n",
      "2022-01-21 18:33:14,921 | ===> Batch : 22\n",
      "2022-01-21 18:33:14,921 | FDS enable\n",
      "2022-01-21 18:33:26,711 | Calculate Loss\n",
      "2022-01-21 18:33:26,713 | Update Loss\n",
      "2022-01-21 18:33:26,717 | Backward\n",
      "2022-01-21 18:33:47,280 | Epoch: [4][22/48]\tTime  32.37 ( 32.84)\tData 0.0138 (0.0729)\tLoss (L1) 14.944 (12.513)\n",
      "2022-01-21 18:33:47,295 | ===> Batch : 23\n",
      "2022-01-21 18:33:47,295 | FDS enable\n",
      "2022-01-21 18:33:59,085 | Calculate Loss\n",
      "2022-01-21 18:33:59,087 | Update Loss\n",
      "2022-01-21 18:33:59,092 | Backward\n",
      "2022-01-21 18:34:20,391 | Epoch: [4][23/48]\tTime  33.11 ( 32.85)\tData 0.0148 (0.0704)\tLoss (L1) 11.921 (12.487)\n",
      "2022-01-21 18:34:20,405 | ===> Batch : 24\n",
      "2022-01-21 18:34:20,405 | FDS enable\n",
      "2022-01-21 18:34:32,191 | Calculate Loss\n",
      "2022-01-21 18:34:32,192 | Update Loss\n",
      "2022-01-21 18:34:32,196 | Backward\n",
      "2022-01-21 18:34:52,927 | Epoch: [4][24/48]\tTime  32.54 ( 32.84)\tData 0.0136 (0.0680)\tLoss (L1) 10.844 (12.419)\n",
      "2022-01-21 18:34:52,944 | ===> Batch : 25\n",
      "2022-01-21 18:34:52,944 | FDS enable\n",
      "2022-01-21 18:35:04,739 | Calculate Loss\n",
      "2022-01-21 18:35:04,740 | Update Loss\n",
      "2022-01-21 18:35:04,745 | Backward\n",
      "2022-01-21 18:35:25,559 | Epoch: [4][25/48]\tTime  32.63 ( 32.83)\tData 0.0164 (0.0659)\tLoss (L1) 10.493 (12.342)\n",
      "2022-01-21 18:35:25,574 | ===> Batch : 26\n",
      "2022-01-21 18:35:25,574 | FDS enable\n",
      "2022-01-21 18:35:37,372 | Calculate Loss\n",
      "2022-01-21 18:35:37,374 | Update Loss\n",
      "2022-01-21 18:35:37,378 | Backward\n",
      "2022-01-21 18:35:58,103 | Epoch: [4][26/48]\tTime  32.54 ( 32.82)\tData 0.0149 (0.0640)\tLoss (L1) 11.573 (12.312)\n",
      "2022-01-21 18:35:58,118 | ===> Batch : 27\n",
      "2022-01-21 18:35:58,119 | FDS enable\n",
      "2022-01-21 18:36:09,929 | Calculate Loss\n",
      "2022-01-21 18:36:09,931 | Update Loss\n",
      "2022-01-21 18:36:09,935 | Backward\n",
      "2022-01-21 18:36:30,742 | Epoch: [4][27/48]\tTime  32.64 ( 32.81)\tData 0.0149 (0.0621)\tLoss (L1) 13.892 (12.371)\n",
      "2022-01-21 18:36:30,756 | ===> Batch : 28\n",
      "2022-01-21 18:36:30,756 | FDS enable\n",
      "2022-01-21 18:36:42,485 | Calculate Loss\n",
      "2022-01-21 18:36:42,487 | Update Loss\n",
      "2022-01-21 18:36:42,491 | Backward\n",
      "2022-01-21 18:37:03,291 | Epoch: [4][28/48]\tTime  32.55 ( 32.80)\tData 0.0138 (0.0604)\tLoss (L1) 11.748 (12.349)\n",
      "2022-01-21 18:37:03,307 | ===> Batch : 29\n",
      "2022-01-21 18:37:03,307 | FDS enable\n",
      "2022-01-21 18:37:15,070 | Calculate Loss\n",
      "2022-01-21 18:37:15,071 | Update Loss\n",
      "2022-01-21 18:37:15,076 | Backward\n",
      "2022-01-21 18:37:35,655 | Epoch: [4][29/48]\tTime  32.36 ( 32.79)\tData 0.0157 (0.0589)\tLoss (L1) 10.103 (12.271)\n",
      "2022-01-21 18:37:35,669 | ===> Batch : 30\n",
      "2022-01-21 18:37:35,669 | FDS enable\n",
      "2022-01-21 18:37:47,391 | Calculate Loss\n",
      "2022-01-21 18:37:47,392 | Update Loss\n",
      "2022-01-21 18:37:47,397 | Backward\n",
      "2022-01-21 18:38:08,552 | Epoch: [4][30/48]\tTime  32.90 ( 32.79)\tData 0.0143 (0.0574)\tLoss (L1) 11.249 (12.237)\n",
      "2022-01-21 18:38:08,573 | ===> Batch : 31\n",
      "2022-01-21 18:38:08,573 | FDS enable\n",
      "2022-01-21 18:38:20,310 | Calculate Loss\n",
      "2022-01-21 18:38:20,311 | Update Loss\n",
      "2022-01-21 18:38:20,315 | Backward\n",
      "2022-01-21 18:38:40,893 | Epoch: [4][31/48]\tTime  32.34 ( 32.77)\tData 0.0208 (0.0562)\tLoss (L1) 12.744 (12.253)\n",
      "2022-01-21 18:38:40,908 | ===> Batch : 32\n",
      "2022-01-21 18:38:40,908 | FDS enable\n",
      "2022-01-21 18:38:52,590 | Calculate Loss\n",
      "2022-01-21 18:38:52,592 | Update Loss\n",
      "2022-01-21 18:38:52,596 | Backward\n",
      "2022-01-21 18:39:13,110 | Epoch: [4][32/48]\tTime  32.22 ( 32.76)\tData 0.0142 (0.0549)\tLoss (L1) 12.714 (12.268)\n",
      "2022-01-21 18:39:13,124 | ===> Batch : 33\n",
      "2022-01-21 18:39:13,125 | FDS enable\n",
      "2022-01-21 18:39:24,734 | Calculate Loss\n",
      "2022-01-21 18:39:24,735 | Update Loss\n",
      "2022-01-21 18:39:24,740 | Backward\n",
      "2022-01-21 18:39:45,610 | Epoch: [4][33/48]\tTime  32.50 ( 32.75)\tData 0.0146 (0.0537)\tLoss (L1) 11.532 (12.246)\n",
      "2022-01-21 18:39:45,623 | ===> Batch : 34\n",
      "2022-01-21 18:39:45,624 | FDS enable\n",
      "2022-01-21 18:39:57,206 | Calculate Loss\n",
      "2022-01-21 18:39:57,208 | Update Loss\n",
      "2022-01-21 18:39:57,212 | Backward\n",
      "2022-01-21 18:40:18,018 | Epoch: [4][34/48]\tTime  32.41 ( 32.74)\tData 0.0135 (0.0525)\tLoss (L1) 11.192 (12.215)\n",
      "2022-01-21 18:40:18,032 | ===> Batch : 35\n",
      "2022-01-21 18:40:18,032 | FDS enable\n",
      "2022-01-21 18:40:29,656 | Calculate Loss\n",
      "2022-01-21 18:40:29,657 | Update Loss\n",
      "2022-01-21 18:40:29,662 | Backward\n",
      "2022-01-21 18:40:50,687 | Epoch: [4][35/48]\tTime  32.67 ( 32.74)\tData 0.0134 (0.0514)\tLoss (L1) 10.764 (12.173)\n",
      "2022-01-21 18:40:50,701 | ===> Batch : 36\n",
      "2022-01-21 18:40:50,701 | FDS enable\n",
      "2022-01-21 18:41:02,287 | Calculate Loss\n",
      "2022-01-21 18:41:02,289 | Update Loss\n",
      "2022-01-21 18:41:02,293 | Backward\n",
      "2022-01-21 18:41:23,090 | Epoch: [4][36/48]\tTime  32.40 ( 32.73)\tData 0.0135 (0.0503)\tLoss (L1) 10.183 (12.118)\n",
      "2022-01-21 18:41:23,104 | ===> Batch : 37\n",
      "2022-01-21 18:41:23,105 | FDS enable\n",
      "2022-01-21 18:41:34,692 | Calculate Loss\n",
      "2022-01-21 18:41:34,694 | Update Loss\n",
      "2022-01-21 18:41:34,698 | Backward\n",
      "2022-01-21 18:41:55,326 | Epoch: [4][37/48]\tTime  32.24 ( 32.71)\tData 0.0147 (0.0494)\tLoss (L1) 11.616 (12.104)\n",
      "2022-01-21 18:41:55,340 | ===> Batch : 38\n",
      "2022-01-21 18:41:55,341 | FDS enable\n",
      "2022-01-21 18:42:06,916 | Calculate Loss\n",
      "2022-01-21 18:42:06,918 | Update Loss\n",
      "2022-01-21 18:42:06,922 | Backward\n",
      "2022-01-21 18:42:27,592 | Epoch: [4][38/48]\tTime  32.27 ( 32.70)\tData 0.0138 (0.0484)\tLoss (L1) 10.609 (12.065)\n",
      "2022-01-21 18:42:27,605 | ===> Batch : 39\n",
      "2022-01-21 18:42:27,606 | FDS enable\n",
      "2022-01-21 18:42:39,323 | Calculate Loss\n",
      "2022-01-21 18:42:39,324 | Update Loss\n",
      "2022-01-21 18:42:39,329 | Backward\n",
      "2022-01-21 18:43:00,160 | Epoch: [4][39/48]\tTime  32.57 ( 32.70)\tData 0.0135 (0.0475)\tLoss (L1) 10.345 (12.021)\n",
      "2022-01-21 18:43:00,173 | ===> Batch : 40\n",
      "2022-01-21 18:43:00,174 | FDS enable\n",
      "2022-01-21 18:43:11,763 | Calculate Loss\n",
      "2022-01-21 18:43:11,764 | Update Loss\n",
      "2022-01-21 18:43:11,768 | Backward\n",
      "2022-01-21 18:43:32,520 | Epoch: [4][40/48]\tTime  32.36 ( 32.69)\tData 0.0135 (0.0467)\tLoss (L1) 10.760 (11.989)\n",
      "2022-01-21 18:43:32,535 | ===> Batch : 41\n",
      "2022-01-21 18:43:32,536 | FDS enable\n",
      "2022-01-21 18:43:44,223 | Calculate Loss\n",
      "2022-01-21 18:43:44,225 | Update Loss\n",
      "2022-01-21 18:43:44,229 | Backward\n",
      "2022-01-21 18:44:05,420 | Epoch: [4][41/48]\tTime  32.90 ( 32.70)\tData 0.0149 (0.0459)\tLoss (L1) 13.516 (12.027)\n",
      "2022-01-21 18:44:05,434 | ===> Batch : 42\n",
      "2022-01-21 18:44:05,434 | FDS enable\n",
      "2022-01-21 18:44:17,078 | Calculate Loss\n",
      "2022-01-21 18:44:17,079 | Update Loss\n",
      "2022-01-21 18:44:17,084 | Backward\n",
      "2022-01-21 18:44:38,180 | Epoch: [4][42/48]\tTime  32.76 ( 32.70)\tData 0.0136 (0.0451)\tLoss (L1) 10.615 (11.993)\n",
      "2022-01-21 18:44:38,193 | ===> Batch : 43\n",
      "2022-01-21 18:44:38,194 | FDS enable\n",
      "2022-01-21 18:44:49,785 | Calculate Loss\n",
      "2022-01-21 18:44:49,787 | Update Loss\n",
      "2022-01-21 18:44:49,791 | Backward\n",
      "2022-01-21 18:45:10,967 | Epoch: [4][43/48]\tTime  32.79 ( 32.70)\tData 0.0137 (0.0444)\tLoss (L1) 10.533 (11.959)\n",
      "2022-01-21 18:45:10,981 | ===> Batch : 44\n",
      "2022-01-21 18:45:10,981 | FDS enable\n",
      "2022-01-21 18:45:22,566 | Calculate Loss\n",
      "2022-01-21 18:45:22,567 | Update Loss\n",
      "2022-01-21 18:45:22,572 | Backward\n",
      "2022-01-21 18:45:43,523 | Epoch: [4][44/48]\tTime  32.56 ( 32.70)\tData 0.0136 (0.0437)\tLoss (L1) 10.219 (11.919)\n",
      "2022-01-21 18:45:43,537 | ===> Batch : 45\n",
      "2022-01-21 18:45:43,537 | FDS enable\n",
      "2022-01-21 18:45:55,139 | Calculate Loss\n",
      "2022-01-21 18:45:55,141 | Update Loss\n",
      "2022-01-21 18:45:55,145 | Backward\n",
      "2022-01-21 18:46:15,779 | Epoch: [4][45/48]\tTime  32.26 ( 32.69)\tData 0.0136 (0.0430)\tLoss (L1) 12.463 (11.931)\n",
      "2022-01-21 18:46:15,792 | ===> Batch : 46\n",
      "2022-01-21 18:46:15,793 | FDS enable\n",
      "2022-01-21 18:46:27,362 | Calculate Loss\n",
      "2022-01-21 18:46:27,364 | Update Loss\n",
      "2022-01-21 18:46:27,368 | Backward\n",
      "2022-01-21 18:46:48,412 | Epoch: [4][46/48]\tTime  32.63 ( 32.69)\tData 0.0135 (0.0424)\tLoss (L1) 10.999 (11.911)\n",
      "2022-01-21 18:46:48,425 | ===> Batch : 47\n",
      "2022-01-21 18:46:48,426 | FDS enable\n",
      "2022-01-21 18:47:00,031 | Calculate Loss\n",
      "2022-01-21 18:47:00,033 | Update Loss\n",
      "2022-01-21 18:47:00,037 | Backward\n",
      "2022-01-21 18:47:21,103 | Epoch: [4][47/48]\tTime  32.69 ( 32.69)\tData 0.0137 (0.0418)\tLoss (L1) 12.431 (11.922)\n",
      "2022-01-21 18:47:21,119 | ===> Batch : 48\n",
      "2022-01-21 18:47:21,120 | FDS enable\n",
      "2022-01-21 18:47:29,622 | Calculate Loss\n",
      "2022-01-21 18:47:29,624 | Update Loss\n",
      "2022-01-21 18:47:29,628 | Backward\n",
      "2022-01-21 18:47:43,867 | Epoch: [4][48/48]\tTime  22.76 ( 32.48)\tData 0.0157 (0.0412)\tLoss (L1) 12.544 (11.931)\n",
      "2022-01-21 18:47:44,021 | Create Epoch [4] features of all training data...\n",
      "100%|██████████| 48/48 [09:08<00:00, 11.43s/it]\n",
      "2022-01-21 18:56:52,830 | Updated smoothed statistics on Epoch [4]!\n",
      "2022-01-21 18:56:52,920 | Updated running statistics with Epoch [4] features!\n",
      "2022-01-21 18:57:04,903 | Val: [0/9]\tTime 11.970 (11.970)\tLoss (MSE) 250.561 (250.561)\tLoss (L1) 12.572 (12.572)\n",
      "2022-01-21 18:57:14,071 | Val: [1/9]\tTime  9.169 (10.569)\tLoss (MSE) 215.982 (233.272)\tLoss (L1) 11.544 (12.058)\n",
      "2022-01-21 18:57:23,220 | Val: [2/9]\tTime  9.149 (10.096)\tLoss (MSE) 254.691 (240.411)\tLoss (L1) 12.987 (12.368)\n",
      "2022-01-21 18:57:32,385 | Val: [3/9]\tTime  9.165 ( 9.863)\tLoss (MSE) 258.737 (244.993)\tLoss (L1) 12.575 (12.419)\n",
      "2022-01-21 18:57:41,588 | Val: [4/9]\tTime  9.202 ( 9.731)\tLoss (MSE) 270.269 (250.048)\tLoss (L1) 13.071 (12.550)\n",
      "2022-01-21 18:57:50,794 | Val: [5/9]\tTime  9.206 ( 9.644)\tLoss (MSE) 294.530 (257.462)\tLoss (L1) 13.733 (12.747)\n",
      "2022-01-21 18:57:59,892 | Val: [6/9]\tTime  9.098 ( 9.566)\tLoss (MSE) 248.766 (256.220)\tLoss (L1) 12.605 (12.727)\n",
      "2022-01-21 18:58:09,107 | Val: [7/9]\tTime  9.215 ( 9.522)\tLoss (MSE) 270.349 (257.986)\tLoss (L1) 12.800 (12.736)\n",
      "2022-01-21 18:58:12,210 | Val: [8/9]\tTime  3.103 ( 8.809)\tLoss (MSE) 278.423 (258.864)\tLoss (L1) 13.143 (12.753)\n",
      "2022-01-21 18:58:12,348 |  * Overall: MSE 258.864\tL1 12.753\tG-Mean 8.447\n",
      "2022-01-21 18:58:12,349 |  * Many: MSE 178.093\tL1 10.549\tG-Mean 6.892\n",
      "2022-01-21 18:58:12,349 |  * Median: MSE 425.120\tL1 17.203\tG-Mean 12.623\n",
      "2022-01-21 18:58:12,349 |  * Low: MSE 561.935\tL1 21.267\tG-Mean 18.992\n",
      "2022-01-21 18:58:12,355 | Best L1 Loss: 12.666\n",
      "2022-01-21 18:58:13,595 | Epoch #4: Train loss [11.9313]; Val loss: MSE [258.8643], L1 [12.7534], G-Mean [8.4470]\n",
      "2022-01-21 18:58:13,596 | Training...\n",
      "2022-01-21 18:58:13,597 | Load train loader\n",
      "2022-01-21 18:58:14,988 | ===> Batch : 1\n",
      "2022-01-21 18:58:14,989 | FDS enable\n",
      "2022-01-21 18:58:29,297 | Calculate Loss\n",
      "2022-01-21 18:58:29,298 | Update Loss\n",
      "2022-01-21 18:58:29,420 | Backward\n",
      "2022-01-21 18:58:50,905 | Epoch: [5][ 1/48]\tTime  37.31 ( 37.31)\tData 1.3908 (1.3908)\tLoss (L1) 11.574 (11.574)\n",
      "2022-01-21 18:58:50,922 | ===> Batch : 2\n",
      "2022-01-21 18:58:50,922 | FDS enable\n",
      "2022-01-21 18:59:02,701 | Calculate Loss\n",
      "2022-01-21 18:59:02,703 | Update Loss\n",
      "2022-01-21 18:59:02,708 | Backward\n",
      "2022-01-21 18:59:23,609 | Epoch: [5][ 2/48]\tTime  32.70 ( 35.01)\tData 0.0169 (0.7039)\tLoss (L1) 10.851 (11.212)\n",
      "2022-01-21 18:59:23,622 | ===> Batch : 3\n",
      "2022-01-21 18:59:23,622 | FDS enable\n",
      "2022-01-21 18:59:35,351 | Calculate Loss\n",
      "2022-01-21 18:59:35,353 | Update Loss\n",
      "2022-01-21 18:59:35,357 | Backward\n",
      "2022-01-21 18:59:56,597 | Epoch: [5][ 3/48]\tTime  32.99 ( 34.33)\tData 0.0127 (0.4735)\tLoss (L1) 11.809 (11.411)\n",
      "2022-01-21 18:59:56,610 | ===> Batch : 4\n",
      "2022-01-21 18:59:56,610 | FDS enable\n",
      "2022-01-21 19:00:08,443 | Calculate Loss\n",
      "2022-01-21 19:00:08,445 | Update Loss\n",
      "2022-01-21 19:00:08,449 | Backward\n",
      "2022-01-21 19:00:29,472 | Epoch: [5][ 4/48]\tTime  32.88 ( 33.97)\tData 0.0133 (0.3584)\tLoss (L1) 12.297 (11.633)\n",
      "2022-01-21 19:00:29,486 | ===> Batch : 5\n",
      "2022-01-21 19:00:29,486 | FDS enable\n",
      "2022-01-21 19:00:41,296 | Calculate Loss\n",
      "2022-01-21 19:00:41,297 | Update Loss\n",
      "2022-01-21 19:00:41,302 | Backward\n",
      "2022-01-21 19:01:02,044 | Epoch: [5][ 5/48]\tTime  32.57 ( 33.69)\tData 0.0132 (0.2894)\tLoss (L1) 13.628 (12.032)\n",
      "2022-01-21 19:01:02,058 | ===> Batch : 6\n",
      "2022-01-21 19:01:02,058 | FDS enable\n",
      "2022-01-21 19:01:13,863 | Calculate Loss\n",
      "2022-01-21 19:01:13,864 | Update Loss\n",
      "2022-01-21 19:01:13,869 | Backward\n",
      "2022-01-21 19:01:35,248 | Epoch: [5][ 6/48]\tTime  33.20 ( 33.61)\tData 0.0138 (0.2434)\tLoss (L1) 12.101 (12.043)\n",
      "2022-01-21 19:01:35,261 | ===> Batch : 7\n",
      "2022-01-21 19:01:35,262 | FDS enable\n",
      "2022-01-21 19:01:47,257 | Calculate Loss\n",
      "2022-01-21 19:01:47,259 | Update Loss\n",
      "2022-01-21 19:01:47,262 | Backward\n",
      "2022-01-21 19:02:08,162 | Epoch: [5][ 7/48]\tTime  32.91 ( 33.51)\tData 0.0135 (0.2106)\tLoss (L1) 9.581 (11.692)\n",
      "2022-01-21 19:02:08,176 | ===> Batch : 8\n",
      "2022-01-21 19:02:08,176 | FDS enable\n",
      "2022-01-21 19:02:19,961 | Calculate Loss\n",
      "2022-01-21 19:02:19,963 | Update Loss\n",
      "2022-01-21 19:02:19,967 | Backward\n",
      "2022-01-21 19:02:41,705 | Epoch: [5][ 8/48]\tTime  33.54 ( 33.51)\tData 0.0142 (0.1860)\tLoss (L1) 11.829 (11.709)\n",
      "2022-01-21 19:02:41,727 | ===> Batch : 9\n",
      "2022-01-21 19:02:41,727 | FDS enable\n",
      "2022-01-21 19:02:53,524 | Calculate Loss\n",
      "2022-01-21 19:02:53,526 | Update Loss\n",
      "2022-01-21 19:02:53,530 | Backward\n",
      "2022-01-21 19:03:14,191 | Epoch: [5][ 9/48]\tTime  32.49 ( 33.40)\tData 0.0213 (0.1677)\tLoss (L1) 13.968 (11.960)\n",
      "2022-01-21 19:03:14,203 | ===> Batch : 10\n",
      "2022-01-21 19:03:14,204 | FDS enable\n",
      "2022-01-21 19:03:26,020 | Calculate Loss\n",
      "2022-01-21 19:03:26,021 | Update Loss\n",
      "2022-01-21 19:03:26,026 | Backward\n",
      "2022-01-21 19:03:47,290 | Epoch: [5][10/48]\tTime  33.10 ( 33.37)\tData 0.0127 (0.1522)\tLoss (L1) 11.332 (11.897)\n",
      "2022-01-21 19:03:47,303 | ===> Batch : 11\n",
      "2022-01-21 19:03:47,304 | FDS enable\n",
      "2022-01-21 19:03:59,271 | Calculate Loss\n",
      "2022-01-21 19:03:59,273 | Update Loss\n",
      "2022-01-21 19:03:59,278 | Backward\n",
      "2022-01-21 19:04:20,074 | Epoch: [5][11/48]\tTime  32.78 ( 33.32)\tData 0.0129 (0.1396)\tLoss (L1) 12.402 (11.943)\n",
      "2022-01-21 19:04:20,087 | ===> Batch : 12\n",
      "2022-01-21 19:04:20,088 | FDS enable\n",
      "2022-01-21 19:04:31,832 | Calculate Loss\n",
      "2022-01-21 19:04:31,834 | Update Loss\n",
      "2022-01-21 19:04:31,838 | Backward\n",
      "2022-01-21 19:04:52,917 | Epoch: [5][12/48]\tTime  32.84 ( 33.28)\tData 0.0133 (0.1290)\tLoss (L1) 12.421 (11.983)\n",
      "2022-01-21 19:04:52,930 | ===> Batch : 13\n",
      "2022-01-21 19:04:52,931 | FDS enable\n",
      "2022-01-21 19:05:04,894 | Calculate Loss\n",
      "2022-01-21 19:05:04,895 | Update Loss\n",
      "2022-01-21 19:05:04,900 | Backward\n",
      "2022-01-21 19:05:26,265 | Epoch: [5][13/48]\tTime  33.35 ( 33.28)\tData 0.0135 (0.1202)\tLoss (L1) 12.118 (11.993)\n",
      "2022-01-21 19:05:26,277 | ===> Batch : 14\n",
      "2022-01-21 19:05:26,277 | FDS enable\n",
      "2022-01-21 19:05:38,126 | Calculate Loss\n",
      "2022-01-21 19:05:38,127 | Update Loss\n",
      "2022-01-21 19:05:38,131 | Backward\n",
      "2022-01-21 19:05:58,892 | Epoch: [5][14/48]\tTime  32.63 ( 33.24)\tData 0.0122 (0.1124)\tLoss (L1) 10.308 (11.873)\n",
      "2022-01-21 19:05:58,906 | ===> Batch : 15\n",
      "2022-01-21 19:05:58,907 | FDS enable\n",
      "2022-01-21 19:06:10,693 | Calculate Loss\n",
      "2022-01-21 19:06:10,694 | Update Loss\n",
      "2022-01-21 19:06:10,699 | Backward\n",
      "2022-01-21 19:06:31,606 | Epoch: [5][15/48]\tTime  32.71 ( 33.20)\tData 0.0142 (0.1059)\tLoss (L1) 11.157 (11.825)\n",
      "2022-01-21 19:06:31,619 | ===> Batch : 16\n",
      "2022-01-21 19:06:31,620 | FDS enable\n",
      "2022-01-21 19:06:43,404 | Calculate Loss\n",
      "2022-01-21 19:06:43,406 | Update Loss\n",
      "2022-01-21 19:06:43,410 | Backward\n",
      "2022-01-21 19:07:04,482 | Epoch: [5][16/48]\tTime  32.88 ( 33.18)\tData 0.0127 (0.1001)\tLoss (L1) 13.951 (11.958)\n",
      "2022-01-21 19:07:04,501 | ===> Batch : 17\n",
      "2022-01-21 19:07:04,501 | FDS enable\n",
      "2022-01-21 19:07:16,273 | Calculate Loss\n",
      "2022-01-21 19:07:16,274 | Update Loss\n",
      "2022-01-21 19:07:16,278 | Backward\n",
      "2022-01-21 19:07:36,813 | Epoch: [5][17/48]\tTime  32.33 ( 33.13)\tData 0.0192 (0.0953)\tLoss (L1) 12.904 (12.014)\n",
      "2022-01-21 19:07:36,827 | ===> Batch : 18\n",
      "2022-01-21 19:07:36,827 | FDS enable\n",
      "2022-01-21 19:07:48,533 | Calculate Loss\n",
      "2022-01-21 19:07:48,534 | Update Loss\n",
      "2022-01-21 19:07:48,539 | Backward\n",
      "2022-01-21 19:08:09,502 | Epoch: [5][18/48]\tTime  32.69 ( 33.11)\tData 0.0135 (0.0908)\tLoss (L1) 13.075 (12.073)\n",
      "2022-01-21 19:08:09,516 | ===> Batch : 19\n",
      "2022-01-21 19:08:09,516 | FDS enable\n",
      "2022-01-21 19:08:21,310 | Calculate Loss\n",
      "2022-01-21 19:08:21,312 | Update Loss\n",
      "2022-01-21 19:08:21,316 | Backward\n",
      "2022-01-21 19:08:42,309 | Epoch: [5][19/48]\tTime  32.81 ( 33.09)\tData 0.0136 (0.0867)\tLoss (L1) 10.448 (11.987)\n",
      "2022-01-21 19:08:42,323 | ===> Batch : 20\n",
      "2022-01-21 19:08:42,323 | FDS enable\n",
      "2022-01-21 19:08:54,064 | Calculate Loss\n",
      "2022-01-21 19:08:54,066 | Update Loss\n",
      "2022-01-21 19:08:54,070 | Backward\n",
      "2022-01-21 19:09:15,408 | Epoch: [5][20/48]\tTime  33.10 ( 33.09)\tData 0.0139 (0.0831)\tLoss (L1) 13.181 (12.047)\n",
      "2022-01-21 19:09:15,423 | ===> Batch : 21\n",
      "2022-01-21 19:09:15,423 | FDS enable\n",
      "2022-01-21 19:09:27,185 | Calculate Loss\n",
      "2022-01-21 19:09:27,186 | Update Loss\n",
      "2022-01-21 19:09:27,191 | Backward\n",
      "2022-01-21 19:09:48,137 | Epoch: [5][21/48]\tTime  32.73 ( 33.07)\tData 0.0147 (0.0798)\tLoss (L1) 8.956 (11.900)\n",
      "2022-01-21 19:09:48,151 | ===> Batch : 22\n",
      "2022-01-21 19:09:48,151 | FDS enable\n",
      "2022-01-21 19:09:59,922 | Calculate Loss\n",
      "2022-01-21 19:09:59,923 | Update Loss\n",
      "2022-01-21 19:09:59,928 | Backward\n",
      "2022-01-21 19:10:20,239 | Epoch: [5][22/48]\tTime  32.10 ( 33.03)\tData 0.0137 (0.0768)\tLoss (L1) 11.465 (11.880)\n",
      "2022-01-21 19:10:20,253 | ===> Batch : 23\n",
      "2022-01-21 19:10:20,253 | FDS enable\n",
      "2022-01-21 19:10:32,031 | Calculate Loss\n",
      "2022-01-21 19:10:32,032 | Update Loss\n",
      "2022-01-21 19:10:32,037 | Backward\n",
      "2022-01-21 19:10:52,818 | Epoch: [5][23/48]\tTime  32.58 ( 33.01)\tData 0.0135 (0.0741)\tLoss (L1) 12.010 (11.885)\n",
      "2022-01-21 19:10:52,837 | ===> Batch : 24\n",
      "2022-01-21 19:10:52,838 | FDS enable\n",
      "2022-01-21 19:11:04,675 | Calculate Loss\n",
      "2022-01-21 19:11:04,677 | Update Loss\n",
      "2022-01-21 19:11:04,682 | Backward\n",
      "2022-01-21 19:11:25,501 | Epoch: [5][24/48]\tTime  32.68 ( 33.00)\tData 0.0193 (0.0718)\tLoss (L1) 12.858 (11.926)\n",
      "2022-01-21 19:11:25,516 | ===> Batch : 25\n",
      "2022-01-21 19:11:25,517 | FDS enable\n",
      "2022-01-21 19:11:37,342 | Calculate Loss\n",
      "2022-01-21 19:11:37,344 | Update Loss\n",
      "2022-01-21 19:11:37,349 | Backward\n",
      "2022-01-21 19:11:57,738 | Epoch: [5][25/48]\tTime  32.24 ( 32.97)\tData 0.0160 (0.0695)\tLoss (L1) 11.590 (11.913)\n",
      "2022-01-21 19:11:57,752 | ===> Batch : 26\n",
      "2022-01-21 19:11:57,753 | FDS enable\n",
      "2022-01-21 19:12:09,509 | Calculate Loss\n",
      "2022-01-21 19:12:09,510 | Update Loss\n",
      "2022-01-21 19:12:09,514 | Backward\n",
      "2022-01-21 19:12:30,270 | Epoch: [5][26/48]\tTime  32.53 ( 32.95)\tData 0.0138 (0.0674)\tLoss (L1) 10.357 (11.853)\n",
      "2022-01-21 19:12:30,284 | ===> Batch : 27\n",
      "2022-01-21 19:12:30,285 | FDS enable\n",
      "2022-01-21 19:12:42,036 | Calculate Loss\n",
      "2022-01-21 19:12:42,037 | Update Loss\n",
      "2022-01-21 19:12:42,042 | Backward\n",
      "2022-01-21 19:13:03,157 | Epoch: [5][27/48]\tTime  32.89 ( 32.95)\tData 0.0140 (0.0654)\tLoss (L1) 10.927 (11.818)\n",
      "2022-01-21 19:13:03,172 | ===> Batch : 28\n",
      "2022-01-21 19:13:03,172 | FDS enable\n",
      "2022-01-21 19:13:14,974 | Calculate Loss\n",
      "2022-01-21 19:13:14,975 | Update Loss\n",
      "2022-01-21 19:13:14,979 | Backward\n",
      "2022-01-21 19:13:35,698 | Epoch: [5][28/48]\tTime  32.54 ( 32.93)\tData 0.0150 (0.0636)\tLoss (L1) 10.801 (11.782)\n",
      "2022-01-21 19:13:35,712 | ===> Batch : 29\n",
      "2022-01-21 19:13:35,712 | FDS enable\n",
      "2022-01-21 19:13:47,467 | Calculate Loss\n",
      "2022-01-21 19:13:47,468 | Update Loss\n",
      "2022-01-21 19:13:47,472 | Backward\n",
      "2022-01-21 19:14:08,316 | Epoch: [5][29/48]\tTime  32.62 ( 32.92)\tData 0.0137 (0.0619)\tLoss (L1) 10.496 (11.738)\n",
      "2022-01-21 19:14:08,330 | ===> Batch : 30\n",
      "2022-01-21 19:14:08,330 | FDS enable\n",
      "2022-01-21 19:14:20,109 | Calculate Loss\n",
      "2022-01-21 19:14:20,111 | Update Loss\n",
      "2022-01-21 19:14:20,115 | Backward\n",
      "2022-01-21 19:14:40,907 | Epoch: [5][30/48]\tTime  32.59 ( 32.91)\tData 0.0139 (0.0603)\tLoss (L1) 10.158 (11.685)\n",
      "2022-01-21 19:14:40,921 | ===> Batch : 31\n",
      "2022-01-21 19:14:40,922 | FDS enable\n",
      "2022-01-21 19:14:52,665 | Calculate Loss\n",
      "2022-01-21 19:14:52,668 | Update Loss\n",
      "2022-01-21 19:14:52,673 | Backward\n",
      "2022-01-21 19:15:13,238 | Epoch: [5][31/48]\tTime  32.33 ( 32.89)\tData 0.0149 (0.0588)\tLoss (L1) 12.850 (11.723)\n",
      "2022-01-21 19:15:13,252 | ===> Batch : 32\n",
      "2022-01-21 19:15:13,253 | FDS enable\n",
      "2022-01-21 19:15:24,922 | Calculate Loss\n",
      "2022-01-21 19:15:24,924 | Update Loss\n",
      "2022-01-21 19:15:24,928 | Backward\n",
      "2022-01-21 19:15:45,445 | Epoch: [5][32/48]\tTime  32.21 ( 32.87)\tData 0.0142 (0.0574)\tLoss (L1) 11.627 (11.720)\n",
      "2022-01-21 19:15:45,458 | ===> Batch : 33\n",
      "2022-01-21 19:15:45,459 | FDS enable\n",
      "2022-01-21 19:15:57,018 | Calculate Loss\n",
      "2022-01-21 19:15:57,020 | Update Loss\n",
      "2022-01-21 19:15:57,024 | Backward\n",
      "2022-01-21 19:16:17,772 | Epoch: [5][33/48]\tTime  32.33 ( 32.85)\tData 0.0137 (0.0561)\tLoss (L1) 9.901 (11.665)\n",
      "2022-01-21 19:16:17,786 | ===> Batch : 34\n",
      "2022-01-21 19:16:17,786 | FDS enable\n",
      "2022-01-21 19:16:29,336 | Calculate Loss\n",
      "2022-01-21 19:16:29,338 | Update Loss\n",
      "2022-01-21 19:16:29,342 | Backward\n",
      "2022-01-21 19:16:50,386 | Epoch: [5][34/48]\tTime  32.61 ( 32.85)\tData 0.0136 (0.0549)\tLoss (L1) 10.403 (11.628)\n",
      "2022-01-21 19:16:50,399 | ===> Batch : 35\n",
      "2022-01-21 19:16:50,399 | FDS enable\n",
      "2022-01-21 19:17:01,956 | Calculate Loss\n",
      "2022-01-21 19:17:01,957 | Update Loss\n",
      "2022-01-21 19:17:01,962 | Backward\n",
      "2022-01-21 19:17:22,713 | Epoch: [5][35/48]\tTime  32.33 ( 32.83)\tData 0.0135 (0.0537)\tLoss (L1) 9.857 (11.577)\n",
      "2022-01-21 19:17:22,726 | ===> Batch : 36\n",
      "2022-01-21 19:17:22,727 | FDS enable\n",
      "2022-01-21 19:17:34,301 | Calculate Loss\n",
      "2022-01-21 19:17:34,303 | Update Loss\n",
      "2022-01-21 19:17:34,307 | Backward\n",
      "2022-01-21 19:17:55,013 | Epoch: [5][36/48]\tTime  32.30 ( 32.82)\tData 0.0135 (0.0526)\tLoss (L1) 13.524 (11.631)\n",
      "2022-01-21 19:17:55,026 | ===> Batch : 37\n",
      "2022-01-21 19:17:55,026 | FDS enable\n",
      "2022-01-21 19:18:06,566 | Calculate Loss\n",
      "2022-01-21 19:18:06,568 | Update Loss\n",
      "2022-01-21 19:18:06,572 | Backward\n",
      "2022-01-21 19:18:27,168 | Epoch: [5][37/48]\tTime  32.16 ( 32.80)\tData 0.0135 (0.0515)\tLoss (L1) 11.084 (11.616)\n",
      "2022-01-21 19:18:27,182 | ===> Batch : 38\n",
      "2022-01-21 19:18:27,182 | FDS enable\n",
      "2022-01-21 19:18:38,686 | Calculate Loss\n",
      "2022-01-21 19:18:38,688 | Update Loss\n",
      "2022-01-21 19:18:38,692 | Backward\n",
      "2022-01-21 19:18:59,702 | Epoch: [5][38/48]\tTime  32.53 ( 32.79)\tData 0.0134 (0.0505)\tLoss (L1) 10.282 (11.581)\n",
      "2022-01-21 19:18:59,716 | ===> Batch : 39\n",
      "2022-01-21 19:18:59,716 | FDS enable\n",
      "2022-01-21 19:19:11,255 | Calculate Loss\n",
      "2022-01-21 19:19:11,257 | Update Loss\n",
      "2022-01-21 19:19:11,261 | Backward\n",
      "2022-01-21 19:19:32,074 | Epoch: [5][39/48]\tTime  32.37 ( 32.78)\tData 0.0135 (0.0496)\tLoss (L1) 11.481 (11.579)\n",
      "2022-01-21 19:19:32,087 | ===> Batch : 40\n",
      "2022-01-21 19:19:32,088 | FDS enable\n",
      "2022-01-21 19:19:43,816 | Calculate Loss\n",
      "2022-01-21 19:19:43,818 | Update Loss\n",
      "2022-01-21 19:19:43,822 | Backward\n",
      "2022-01-21 19:20:05,040 | Epoch: [5][40/48]\tTime  32.97 ( 32.79)\tData 0.0136 (0.0487)\tLoss (L1) 10.816 (11.560)\n",
      "2022-01-21 19:20:05,055 | ===> Batch : 41\n",
      "2022-01-21 19:20:05,055 | FDS enable\n",
      "2022-01-21 19:20:16,753 | Calculate Loss\n",
      "2022-01-21 19:20:16,755 | Update Loss\n",
      "2022-01-21 19:20:16,759 | Backward\n",
      "2022-01-21 19:20:37,648 | Epoch: [5][41/48]\tTime  32.61 ( 32.78)\tData 0.0148 (0.0478)\tLoss (L1) 10.955 (11.545)\n",
      "2022-01-21 19:20:37,662 | ===> Batch : 42\n",
      "2022-01-21 19:20:37,663 | FDS enable\n",
      "2022-01-21 19:20:49,447 | Calculate Loss\n",
      "2022-01-21 19:20:49,448 | Update Loss\n",
      "2022-01-21 19:20:49,453 | Backward\n",
      "2022-01-21 19:21:10,252 | Epoch: [5][42/48]\tTime  32.60 ( 32.78)\tData 0.0143 (0.0470)\tLoss (L1) 10.991 (11.532)\n",
      "2022-01-21 19:21:10,266 | ===> Batch : 43\n",
      "2022-01-21 19:21:10,266 | FDS enable\n",
      "2022-01-21 19:21:21,899 | Calculate Loss\n",
      "2022-01-21 19:21:21,900 | Update Loss\n",
      "2022-01-21 19:21:21,905 | Backward\n",
      "2022-01-21 19:21:43,272 | Epoch: [5][43/48]\tTime  33.02 ( 32.78)\tData 0.0138 (0.0463)\tLoss (L1) 10.298 (11.503)\n",
      "2022-01-21 19:21:43,286 | ===> Batch : 44\n",
      "2022-01-21 19:21:43,286 | FDS enable\n",
      "2022-01-21 19:21:54,941 | Calculate Loss\n",
      "2022-01-21 19:21:54,943 | Update Loss\n",
      "2022-01-21 19:21:54,947 | Backward\n",
      "2022-01-21 19:22:15,481 | Epoch: [5][44/48]\tTime  32.21 ( 32.77)\tData 0.0144 (0.0455)\tLoss (L1) 14.689 (11.575)\n",
      "2022-01-21 19:22:15,495 | ===> Batch : 45\n",
      "2022-01-21 19:22:15,496 | FDS enable\n",
      "2022-01-21 19:22:27,309 | Calculate Loss\n",
      "2022-01-21 19:22:27,310 | Update Loss\n",
      "2022-01-21 19:22:27,314 | Backward\n",
      "2022-01-21 19:22:48,642 | Epoch: [5][45/48]\tTime  33.16 ( 32.78)\tData 0.0141 (0.0448)\tLoss (L1) 11.396 (11.571)\n",
      "2022-01-21 19:22:48,657 | ===> Batch : 46\n",
      "2022-01-21 19:22:48,657 | FDS enable\n",
      "2022-01-21 19:23:00,237 | Calculate Loss\n",
      "2022-01-21 19:23:00,239 | Update Loss\n",
      "2022-01-21 19:23:00,242 | Backward\n",
      "2022-01-21 19:23:20,750 | Epoch: [5][46/48]\tTime  32.11 ( 32.76)\tData 0.0149 (0.0442)\tLoss (L1) 10.837 (11.555)\n",
      "2022-01-21 19:23:20,764 | ===> Batch : 47\n",
      "2022-01-21 19:23:20,764 | FDS enable\n",
      "2022-01-21 19:23:32,344 | Calculate Loss\n",
      "2022-01-21 19:23:32,345 | Update Loss\n",
      "2022-01-21 19:23:32,349 | Backward\n",
      "2022-01-21 19:23:53,508 | Epoch: [5][47/48]\tTime  32.76 ( 32.76)\tData 0.0137 (0.0435)\tLoss (L1) 12.600 (11.578)\n",
      "2022-01-21 19:23:53,522 | ===> Batch : 48\n",
      "2022-01-21 19:23:53,523 | FDS enable\n",
      "2022-01-21 19:24:01,982 | Calculate Loss\n",
      "2022-01-21 19:24:01,984 | Update Loss\n",
      "2022-01-21 19:24:01,988 | Backward\n",
      "2022-01-21 19:24:16,173 | Epoch: [5][48/48]\tTime  22.66 ( 32.55)\tData 0.0145 (0.0429)\tLoss (L1) 11.734 (11.580)\n",
      "2022-01-21 19:24:16,375 | Create Epoch [5] features of all training data...\n",
      "100%|██████████| 48/48 [09:00<00:00, 11.26s/it]\n",
      "2022-01-21 19:33:16,854 | Updated smoothed statistics on Epoch [5]!\n",
      "2022-01-21 19:33:16,931 | Updated running statistics with Epoch [5] features!\n",
      "2022-01-21 19:33:28,622 | Val: [0/9]\tTime 11.679 (11.679)\tLoss (MSE) 257.023 (257.023)\tLoss (L1) 12.854 (12.854)\n",
      "2022-01-21 19:33:37,668 | Val: [1/9]\tTime  9.046 (10.362)\tLoss (MSE) 231.974 (244.498)\tLoss (L1) 11.582 (12.218)\n",
      "2022-01-21 19:33:46,740 | Val: [2/9]\tTime  9.072 ( 9.932)\tLoss (MSE) 257.449 (248.815)\tLoss (L1) 12.561 (12.332)\n",
      "2022-01-21 19:33:55,795 | Val: [3/9]\tTime  9.054 ( 9.713)\tLoss (MSE) 251.698 (249.536)\tLoss (L1) 12.426 (12.356)\n",
      "2022-01-21 19:34:04,960 | Val: [4/9]\tTime  9.166 ( 9.603)\tLoss (MSE) 270.813 (253.791)\tLoss (L1) 13.066 (12.498)\n",
      "2022-01-21 19:34:13,969 | Val: [5/9]\tTime  9.009 ( 9.504)\tLoss (MSE) 302.588 (261.924)\tLoss (L1) 13.793 (12.714)\n",
      "2022-01-21 19:34:23,173 | Val: [6/9]\tTime  9.204 ( 9.461)\tLoss (MSE) 245.907 (259.636)\tLoss (L1) 12.314 (12.657)\n",
      "2022-01-21 19:34:32,266 | Val: [7/9]\tTime  9.093 ( 9.415)\tLoss (MSE) 284.853 (262.788)\tLoss (L1) 13.133 (12.716)\n",
      "2022-01-21 19:34:35,132 | Val: [8/9]\tTime  2.866 ( 8.688)\tLoss (MSE) 277.533 (263.422)\tLoss (L1) 12.201 (12.694)\n",
      "2022-01-21 19:34:35,260 |  * Overall: MSE 263.422\tL1 12.694\tG-Mean 8.138\n",
      "2022-01-21 19:34:35,261 |  * Many: MSE 199.067\tL1 11.020\tG-Mean 6.992\n",
      "2022-01-21 19:34:35,261 |  * Median: MSE 414.063\tL1 16.541\tG-Mean 11.632\n",
      "2022-01-21 19:34:35,262 |  * Low: MSE 454.635\tL1 17.866\tG-Mean 12.718\n",
      "2022-01-21 19:34:35,267 | Best L1 Loss: 12.666\n",
      "2022-01-21 19:34:36,498 | Epoch #5: Train loss [11.5799]; Val loss: MSE [263.4220], L1 [12.6940], G-Mean [8.1384]\n",
      "2022-01-21 19:34:36,500 | Training...\n",
      "2022-01-21 19:34:36,501 | Load train loader\n",
      "2022-01-21 19:34:37,732 | ===> Batch : 1\n",
      "2022-01-21 19:34:37,734 | FDS enable\n",
      "2022-01-21 19:34:52,056 | Calculate Loss\n",
      "2022-01-21 19:34:52,057 | Update Loss\n",
      "2022-01-21 19:34:52,164 | Backward\n",
      "2022-01-21 19:35:13,615 | Epoch: [6][ 1/48]\tTime  37.11 ( 37.11)\tData 1.2309 (1.2309)\tLoss (L1) 10.733 (10.733)\n",
      "2022-01-21 19:35:13,633 | ===> Batch : 2\n",
      "2022-01-21 19:35:13,634 | FDS enable\n",
      "2022-01-21 19:35:25,426 | Calculate Loss\n",
      "2022-01-21 19:35:25,427 | Update Loss\n",
      "2022-01-21 19:35:25,432 | Backward\n",
      "2022-01-21 19:35:46,174 | Epoch: [6][ 2/48]\tTime  32.56 ( 34.84)\tData 0.0179 (0.6244)\tLoss (L1) 11.340 (11.036)\n",
      "2022-01-21 19:35:46,186 | ===> Batch : 3\n",
      "2022-01-21 19:35:46,187 | FDS enable\n",
      "2022-01-21 19:35:57,916 | Calculate Loss\n",
      "2022-01-21 19:35:57,917 | Update Loss\n",
      "2022-01-21 19:35:57,921 | Backward\n",
      "2022-01-21 19:36:19,313 | Epoch: [6][ 3/48]\tTime  33.14 ( 34.27)\tData 0.0128 (0.4205)\tLoss (L1) 10.912 (10.995)\n",
      "2022-01-21 19:36:19,329 | ===> Batch : 4\n",
      "2022-01-21 19:36:19,330 | FDS enable\n",
      "2022-01-21 19:36:31,050 | Calculate Loss\n",
      "2022-01-21 19:36:31,051 | Update Loss\n",
      "2022-01-21 19:36:31,056 | Backward\n",
      "2022-01-21 19:36:52,487 | Epoch: [6][ 4/48]\tTime  33.17 ( 34.00)\tData 0.0157 (0.3193)\tLoss (L1) 13.044 (11.507)\n",
      "2022-01-21 19:36:52,500 | ===> Batch : 5\n",
      "2022-01-21 19:36:52,501 | FDS enable\n",
      "2022-01-21 19:37:04,379 | Calculate Loss\n",
      "2022-01-21 19:37:04,380 | Update Loss\n",
      "2022-01-21 19:37:04,385 | Backward\n",
      "2022-01-21 19:37:25,342 | Epoch: [6][ 5/48]\tTime  32.86 ( 33.77)\tData 0.0133 (0.2581)\tLoss (L1) 10.886 (11.383)\n",
      "2022-01-21 19:37:25,363 | ===> Batch : 6\n",
      "2022-01-21 19:37:25,364 | FDS enable\n",
      "2022-01-21 19:37:37,119 | Calculate Loss\n",
      "2022-01-21 19:37:37,121 | Update Loss\n",
      "2022-01-21 19:37:37,125 | Backward\n",
      "2022-01-21 19:37:58,151 | Epoch: [6][ 6/48]\tTime  32.81 ( 33.61)\tData 0.0206 (0.2185)\tLoss (L1) 13.274 (11.698)\n",
      "2022-01-21 19:37:58,164 | ===> Batch : 7\n",
      "2022-01-21 19:37:58,165 | FDS enable\n",
      "2022-01-21 19:38:09,903 | Calculate Loss\n",
      "2022-01-21 19:38:09,904 | Update Loss\n",
      "2022-01-21 19:38:09,909 | Backward\n",
      "2022-01-21 19:38:30,700 | Epoch: [6][ 7/48]\tTime  32.55 ( 33.46)\tData 0.0137 (0.1893)\tLoss (L1) 11.893 (11.726)\n",
      "2022-01-21 19:38:30,713 | ===> Batch : 8\n",
      "2022-01-21 19:38:30,714 | FDS enable\n",
      "2022-01-21 19:38:42,634 | Calculate Loss\n",
      "2022-01-21 19:38:42,636 | Update Loss\n",
      "2022-01-21 19:38:42,640 | Backward\n",
      "2022-01-21 19:39:03,478 | Epoch: [6][ 8/48]\tTime  32.78 ( 33.37)\tData 0.0128 (0.1672)\tLoss (L1) 12.132 (11.777)\n",
      "2022-01-21 19:39:03,495 | ===> Batch : 9\n",
      "2022-01-21 19:39:03,495 | FDS enable\n",
      "2022-01-21 19:39:15,228 | Calculate Loss\n",
      "2022-01-21 19:39:15,231 | Update Loss\n",
      "2022-01-21 19:39:15,236 | Backward\n",
      "2022-01-21 19:39:36,379 | Epoch: [6][ 9/48]\tTime  32.90 ( 33.32)\tData 0.0171 (0.1505)\tLoss (L1) 11.045 (11.696)\n",
      "2022-01-21 19:39:36,393 | ===> Batch : 10\n",
      "2022-01-21 19:39:36,393 | FDS enable\n",
      "2022-01-21 19:39:48,104 | Calculate Loss\n",
      "2022-01-21 19:39:48,106 | Update Loss\n",
      "2022-01-21 19:39:48,110 | Backward\n",
      "2022-01-21 19:40:09,150 | Epoch: [6][10/48]\tTime  32.77 ( 33.26)\tData 0.0142 (0.1369)\tLoss (L1) 11.637 (11.690)\n",
      "2022-01-21 19:40:09,163 | ===> Batch : 11\n",
      "2022-01-21 19:40:09,163 | FDS enable\n",
      "2022-01-21 19:40:20,944 | Calculate Loss\n",
      "2022-01-21 19:40:20,946 | Update Loss\n",
      "2022-01-21 19:40:20,950 | Backward\n",
      "2022-01-21 19:40:41,326 | Epoch: [6][11/48]\tTime  32.18 ( 33.17)\tData 0.0127 (0.1256)\tLoss (L1) 13.654 (11.868)\n",
      "2022-01-21 19:40:41,340 | ===> Batch : 12\n",
      "2022-01-21 19:40:41,341 | FDS enable\n",
      "2022-01-21 19:40:53,051 | Calculate Loss\n",
      "2022-01-21 19:40:53,053 | Update Loss\n",
      "2022-01-21 19:40:53,058 | Backward\n",
      "2022-01-21 19:41:14,168 | Epoch: [6][12/48]\tTime  32.84 ( 33.14)\tData 0.0139 (0.1163)\tLoss (L1) 10.029 (11.715)\n",
      "2022-01-21 19:41:14,181 | ===> Batch : 13\n",
      "2022-01-21 19:41:14,181 | FDS enable\n",
      "2022-01-21 19:41:25,917 | Calculate Loss\n",
      "2022-01-21 19:41:25,918 | Update Loss\n",
      "2022-01-21 19:41:25,923 | Backward\n",
      "2022-01-21 19:41:46,157 | Epoch: [6][13/48]\tTime  31.99 ( 33.05)\tData 0.0128 (0.1083)\tLoss (L1) 10.834 (11.647)\n",
      "2022-01-21 19:41:46,170 | ===> Batch : 14\n",
      "2022-01-21 19:41:46,170 | FDS enable\n",
      "2022-01-21 19:41:57,851 | Calculate Loss\n",
      "2022-01-21 19:41:57,852 | Update Loss\n",
      "2022-01-21 19:41:57,857 | Backward\n",
      "2022-01-21 19:42:18,954 | Epoch: [6][14/48]\tTime  32.80 ( 33.03)\tData 0.0131 (0.1015)\tLoss (L1) 13.309 (11.766)\n",
      "2022-01-21 19:42:18,967 | ===> Batch : 15\n",
      "2022-01-21 19:42:18,968 | FDS enable\n",
      "2022-01-21 19:42:30,690 | Calculate Loss\n",
      "2022-01-21 19:42:30,692 | Update Loss\n",
      "2022-01-21 19:42:30,696 | Backward\n",
      "2022-01-21 19:42:51,522 | Epoch: [6][15/48]\tTime  32.57 ( 33.00)\tData 0.0138 (0.0957)\tLoss (L1) 11.925 (11.777)\n",
      "2022-01-21 19:42:51,536 | ===> Batch : 16\n",
      "2022-01-21 19:42:51,536 | FDS enable\n",
      "2022-01-21 19:43:03,254 | Calculate Loss\n",
      "2022-01-21 19:43:03,255 | Update Loss\n",
      "2022-01-21 19:43:03,259 | Backward\n",
      "2022-01-21 19:43:23,705 | Epoch: [6][16/48]\tTime  32.18 ( 32.95)\tData 0.0141 (0.0906)\tLoss (L1) 12.076 (11.795)\n",
      "2022-01-21 19:43:23,719 | ===> Batch : 17\n",
      "2022-01-21 19:43:23,719 | FDS enable\n",
      "2022-01-21 19:43:35,525 | Calculate Loss\n",
      "2022-01-21 19:43:35,527 | Update Loss\n",
      "2022-01-21 19:43:35,531 | Backward\n",
      "2022-01-21 19:43:56,095 | Epoch: [6][17/48]\tTime  32.39 ( 32.92)\tData 0.0138 (0.0861)\tLoss (L1) 11.239 (11.763)\n",
      "2022-01-21 19:43:56,108 | ===> Batch : 18\n",
      "2022-01-21 19:43:56,109 | FDS enable\n",
      "2022-01-21 19:44:07,813 | Calculate Loss\n",
      "2022-01-21 19:44:07,815 | Update Loss\n",
      "2022-01-21 19:44:07,820 | Backward\n",
      "2022-01-21 19:44:28,919 | Epoch: [6][18/48]\tTime  32.82 ( 32.91)\tData 0.0137 (0.0821)\tLoss (L1) 12.627 (11.811)\n",
      "2022-01-21 19:44:28,932 | ===> Batch : 19\n",
      "2022-01-21 19:44:28,933 | FDS enable\n",
      "2022-01-21 19:44:40,632 | Calculate Loss\n",
      "2022-01-21 19:44:40,634 | Update Loss\n",
      "2022-01-21 19:44:40,638 | Backward\n",
      "2022-01-21 19:45:01,355 | Epoch: [6][19/48]\tTime  32.44 ( 32.89)\tData 0.0136 (0.0784)\tLoss (L1) 9.425 (11.685)\n",
      "2022-01-21 19:45:01,370 | ===> Batch : 20\n",
      "2022-01-21 19:45:01,370 | FDS enable\n",
      "2022-01-21 19:45:13,090 | Calculate Loss\n",
      "2022-01-21 19:45:13,091 | Update Loss\n",
      "2022-01-21 19:45:13,096 | Backward\n",
      "2022-01-21 19:45:34,162 | Epoch: [6][20/48]\tTime  32.81 ( 32.88)\tData 0.0148 (0.0753)\tLoss (L1) 11.615 (11.682)\n",
      "2022-01-21 19:45:34,176 | ===> Batch : 21\n",
      "2022-01-21 19:45:34,176 | FDS enable\n",
      "2022-01-21 19:45:45,903 | Calculate Loss\n",
      "2022-01-21 19:45:45,904 | Update Loss\n",
      "2022-01-21 19:45:45,909 | Backward\n",
      "2022-01-21 19:46:06,610 | Epoch: [6][21/48]\tTime  32.45 ( 32.86)\tData 0.0137 (0.0723)\tLoss (L1) 12.130 (11.703)\n",
      "2022-01-21 19:46:06,624 | ===> Batch : 22\n",
      "2022-01-21 19:46:06,624 | FDS enable\n",
      "2022-01-21 19:46:18,306 | Calculate Loss\n",
      "2022-01-21 19:46:18,308 | Update Loss\n",
      "2022-01-21 19:46:18,313 | Backward\n",
      "2022-01-21 19:46:38,879 | Epoch: [6][22/48]\tTime  32.27 ( 32.84)\tData 0.0135 (0.0697)\tLoss (L1) 9.090 (11.584)\n",
      "2022-01-21 19:46:38,892 | ===> Batch : 23\n",
      "2022-01-21 19:46:38,892 | FDS enable\n",
      "2022-01-21 19:46:50,574 | Calculate Loss\n",
      "2022-01-21 19:46:50,576 | Update Loss\n",
      "2022-01-21 19:46:50,580 | Backward\n",
      "2022-01-21 19:47:11,759 | Epoch: [6][23/48]\tTime  32.88 ( 32.84)\tData 0.0134 (0.0672)\tLoss (L1) 11.681 (11.588)\n",
      "2022-01-21 19:47:11,774 | ===> Batch : 24\n",
      "2022-01-21 19:47:11,774 | FDS enable\n",
      "2022-01-21 19:47:23,490 | Calculate Loss\n",
      "2022-01-21 19:47:23,491 | Update Loss\n",
      "2022-01-21 19:47:23,495 | Backward\n",
      "2022-01-21 19:47:44,901 | Epoch: [6][24/48]\tTime  33.14 ( 32.85)\tData 0.0149 (0.0650)\tLoss (L1) 15.465 (11.750)\n",
      "2022-01-21 19:47:44,916 | ===> Batch : 25\n",
      "2022-01-21 19:47:44,916 | FDS enable\n",
      "2022-01-21 19:47:56,692 | Calculate Loss\n",
      "2022-01-21 19:47:56,693 | Update Loss\n",
      "2022-01-21 19:47:56,698 | Backward\n",
      "2022-01-21 19:48:17,094 | Epoch: [6][25/48]\tTime  32.19 ( 32.82)\tData 0.0152 (0.0630)\tLoss (L1) 11.108 (11.724)\n",
      "2022-01-21 19:48:17,107 | ===> Batch : 26\n",
      "2022-01-21 19:48:17,108 | FDS enable\n",
      "2022-01-21 19:48:28,823 | Calculate Loss\n",
      "2022-01-21 19:48:28,825 | Update Loss\n",
      "2022-01-21 19:48:28,829 | Backward\n",
      "2022-01-21 19:48:49,438 | Epoch: [6][26/48]\tTime  32.34 ( 32.81)\tData 0.0137 (0.0611)\tLoss (L1) 11.469 (11.714)\n",
      "2022-01-21 19:48:49,451 | ===> Batch : 27\n",
      "2022-01-21 19:48:49,452 | FDS enable\n",
      "2022-01-21 19:49:01,219 | Calculate Loss\n",
      "2022-01-21 19:49:01,221 | Update Loss\n",
      "2022-01-21 19:49:01,225 | Backward\n",
      "2022-01-21 19:49:22,399 | Epoch: [6][27/48]\tTime  32.96 ( 32.81)\tData 0.0140 (0.0594)\tLoss (L1) 10.033 (11.652)\n",
      "2022-01-21 19:49:22,413 | ===> Batch : 28\n",
      "2022-01-21 19:49:22,413 | FDS enable\n",
      "2022-01-21 19:49:34,129 | Calculate Loss\n",
      "2022-01-21 19:49:34,131 | Update Loss\n",
      "2022-01-21 19:49:34,136 | Backward\n",
      "2022-01-21 19:49:54,923 | Epoch: [6][28/48]\tTime  32.52 ( 32.80)\tData 0.0141 (0.0578)\tLoss (L1) 11.012 (11.629)\n",
      "2022-01-21 19:49:54,937 | ===> Batch : 29\n",
      "2022-01-21 19:49:54,937 | FDS enable\n",
      "2022-01-21 19:50:06,677 | Calculate Loss\n",
      "2022-01-21 19:50:06,679 | Update Loss\n",
      "2022-01-21 19:50:06,683 | Backward\n",
      "2022-01-21 19:50:27,415 | Epoch: [6][29/48]\tTime  32.49 ( 32.79)\tData 0.0142 (0.0563)\tLoss (L1) 11.771 (11.634)\n",
      "2022-01-21 19:50:27,428 | ===> Batch : 30\n",
      "2022-01-21 19:50:27,429 | FDS enable\n",
      "2022-01-21 19:50:39,139 | Calculate Loss\n",
      "2022-01-21 19:50:39,141 | Update Loss\n",
      "2022-01-21 19:50:39,146 | Backward\n",
      "2022-01-21 19:51:00,202 | Epoch: [6][30/48]\tTime  32.79 ( 32.79)\tData 0.0138 (0.0549)\tLoss (L1) 12.333 (11.657)\n",
      "2022-01-21 19:51:00,216 | ===> Batch : 31\n",
      "2022-01-21 19:51:00,216 | FDS enable\n",
      "2022-01-21 19:51:11,937 | Calculate Loss\n",
      "2022-01-21 19:51:11,938 | Update Loss\n",
      "2022-01-21 19:51:11,943 | Backward\n",
      "2022-01-21 19:51:32,611 | Epoch: [6][31/48]\tTime  32.41 ( 32.78)\tData 0.0141 (0.0535)\tLoss (L1) 11.451 (11.651)\n",
      "2022-01-21 19:51:32,625 | ===> Batch : 32\n",
      "2022-01-21 19:51:32,625 | FDS enable\n",
      "2022-01-21 19:51:44,291 | Calculate Loss\n",
      "2022-01-21 19:51:44,292 | Update Loss\n",
      "2022-01-21 19:51:44,297 | Backward\n",
      "2022-01-21 19:52:05,088 | Epoch: [6][32/48]\tTime  32.48 ( 32.77)\tData 0.0139 (0.0523)\tLoss (L1) 9.819 (11.594)\n",
      "2022-01-21 19:52:05,101 | ===> Batch : 33\n",
      "2022-01-21 19:52:05,102 | FDS enable\n",
      "2022-01-21 19:52:16,685 | Calculate Loss\n",
      "2022-01-21 19:52:16,686 | Update Loss\n",
      "2022-01-21 19:52:16,690 | Backward\n",
      "2022-01-21 19:52:37,020 | Epoch: [6][33/48]\tTime  31.93 ( 32.74)\tData 0.0137 (0.0511)\tLoss (L1) 10.757 (11.568)\n",
      "2022-01-21 19:52:37,034 | ===> Batch : 34\n",
      "2022-01-21 19:52:37,034 | FDS enable\n",
      "2022-01-21 19:52:48,689 | Calculate Loss\n",
      "2022-01-21 19:52:48,690 | Update Loss\n",
      "2022-01-21 19:52:48,694 | Backward\n",
      "2022-01-21 19:53:09,525 | Epoch: [6][34/48]\tTime  32.50 ( 32.74)\tData 0.0136 (0.0500)\tLoss (L1) 12.495 (11.595)\n",
      "2022-01-21 19:53:09,538 | ===> Batch : 35\n",
      "2022-01-21 19:53:09,539 | FDS enable\n",
      "2022-01-21 19:53:21,144 | Calculate Loss\n",
      "2022-01-21 19:53:21,145 | Update Loss\n",
      "2022-01-21 19:53:21,149 | Backward\n",
      "2022-01-21 19:53:42,288 | Epoch: [6][35/48]\tTime  32.76 ( 32.74)\tData 0.0137 (0.0490)\tLoss (L1) 9.935 (11.548)\n",
      "2022-01-21 19:53:42,301 | ===> Batch : 36\n",
      "2022-01-21 19:53:42,302 | FDS enable\n",
      "2022-01-21 19:53:53,867 | Calculate Loss\n",
      "2022-01-21 19:53:53,868 | Update Loss\n",
      "2022-01-21 19:53:53,873 | Backward\n",
      "2022-01-21 19:54:14,899 | Epoch: [6][36/48]\tTime  32.61 ( 32.73)\tData 0.0138 (0.0480)\tLoss (L1) 13.544 (11.603)\n",
      "2022-01-21 19:54:14,912 | ===> Batch : 37\n",
      "2022-01-21 19:54:14,912 | FDS enable\n",
      "2022-01-21 19:54:26,494 | Calculate Loss\n",
      "2022-01-21 19:54:26,496 | Update Loss\n",
      "2022-01-21 19:54:26,501 | Backward\n",
      "2022-01-21 19:54:47,546 | Epoch: [6][37/48]\tTime  32.65 ( 32.73)\tData 0.0135 (0.0471)\tLoss (L1) 11.233 (11.593)\n",
      "2022-01-21 19:54:47,560 | ===> Batch : 38\n",
      "2022-01-21 19:54:47,560 | FDS enable\n",
      "2022-01-21 19:54:59,145 | Calculate Loss\n",
      "2022-01-21 19:54:59,146 | Update Loss\n",
      "2022-01-21 19:54:59,150 | Backward\n",
      "2022-01-21 19:55:19,969 | Epoch: [6][38/48]\tTime  32.42 ( 32.72)\tData 0.0137 (0.0462)\tLoss (L1) 13.947 (11.655)\n",
      "2022-01-21 19:55:19,983 | ===> Batch : 39\n",
      "2022-01-21 19:55:19,983 | FDS enable\n",
      "2022-01-21 19:55:31,591 | Calculate Loss\n",
      "2022-01-21 19:55:31,592 | Update Loss\n",
      "2022-01-21 19:55:31,596 | Backward\n",
      "2022-01-21 19:55:52,649 | Epoch: [6][39/48]\tTime  32.68 ( 32.72)\tData 0.0138 (0.0454)\tLoss (L1) 11.889 (11.661)\n",
      "2022-01-21 19:55:52,662 | ===> Batch : 40\n",
      "2022-01-21 19:55:52,663 | FDS enable\n",
      "2022-01-21 19:56:04,254 | Calculate Loss\n",
      "2022-01-21 19:56:04,255 | Update Loss\n",
      "2022-01-21 19:56:04,260 | Backward\n",
      "2022-01-21 19:56:25,326 | Epoch: [6][40/48]\tTime  32.68 ( 32.72)\tData 0.0132 (0.0446)\tLoss (L1) 11.854 (11.666)\n",
      "2022-01-21 19:56:25,340 | ===> Batch : 41\n",
      "2022-01-21 19:56:25,341 | FDS enable\n",
      "2022-01-21 19:56:36,922 | Calculate Loss\n",
      "2022-01-21 19:56:36,923 | Update Loss\n",
      "2022-01-21 19:56:36,928 | Backward\n",
      "2022-01-21 19:56:57,452 | Epoch: [6][41/48]\tTime  32.13 ( 32.71)\tData 0.0139 (0.0438)\tLoss (L1) 13.163 (11.703)\n",
      "2022-01-21 19:56:57,466 | ===> Batch : 42\n",
      "2022-01-21 19:56:57,466 | FDS enable\n",
      "2022-01-21 19:57:08,978 | Calculate Loss\n",
      "2022-01-21 19:57:08,979 | Update Loss\n",
      "2022-01-21 19:57:08,983 | Backward\n",
      "2022-01-21 19:57:30,067 | Epoch: [6][42/48]\tTime  32.61 ( 32.70)\tData 0.0135 (0.0431)\tLoss (L1) 12.175 (11.714)\n",
      "2022-01-21 19:57:30,080 | ===> Batch : 43\n",
      "2022-01-21 19:57:30,081 | FDS enable\n",
      "2022-01-21 19:57:41,667 | Calculate Loss\n",
      "2022-01-21 19:57:41,668 | Update Loss\n",
      "2022-01-21 19:57:41,673 | Backward\n",
      "2022-01-21 19:58:02,863 | Epoch: [6][43/48]\tTime  32.80 ( 32.71)\tData 0.0134 (0.0424)\tLoss (L1) 11.774 (11.715)\n",
      "2022-01-21 19:58:02,877 | ===> Batch : 44\n",
      "2022-01-21 19:58:02,878 | FDS enable\n",
      "2022-01-21 19:58:14,482 | Calculate Loss\n",
      "2022-01-21 19:58:14,483 | Update Loss\n",
      "2022-01-21 19:58:14,487 | Backward\n",
      "2022-01-21 19:58:35,480 | Epoch: [6][44/48]\tTime  32.62 ( 32.70)\tData 0.0146 (0.0418)\tLoss (L1) 11.229 (11.704)\n",
      "2022-01-21 19:58:35,493 | ===> Batch : 45\n",
      "2022-01-21 19:58:35,494 | FDS enable\n",
      "2022-01-21 19:58:47,066 | Calculate Loss\n",
      "2022-01-21 19:58:47,068 | Update Loss\n",
      "2022-01-21 19:58:47,072 | Backward\n",
      "2022-01-21 19:59:08,089 | Epoch: [6][45/48]\tTime  32.61 ( 32.70)\tData 0.0137 (0.0412)\tLoss (L1) 10.315 (11.673)\n",
      "2022-01-21 19:59:08,103 | ===> Batch : 46\n",
      "2022-01-21 19:59:08,103 | FDS enable\n",
      "2022-01-21 19:59:19,710 | Calculate Loss\n",
      "2022-01-21 19:59:19,712 | Update Loss\n",
      "2022-01-21 19:59:19,717 | Backward\n",
      "2022-01-21 19:59:40,361 | Epoch: [6][46/48]\tTime  32.27 ( 32.69)\tData 0.0140 (0.0406)\tLoss (L1) 10.677 (11.652)\n",
      "2022-01-21 19:59:40,375 | ===> Batch : 47\n",
      "2022-01-21 19:59:40,376 | FDS enable\n",
      "2022-01-21 19:59:51,970 | Calculate Loss\n",
      "2022-01-21 19:59:51,971 | Update Loss\n",
      "2022-01-21 19:59:51,976 | Backward\n",
      "2022-01-21 20:00:13,045 | Epoch: [6][47/48]\tTime  32.68 ( 32.69)\tData 0.0142 (0.0400)\tLoss (L1) 12.900 (11.678)\n",
      "2022-01-21 20:00:13,058 | ===> Batch : 48\n",
      "2022-01-21 20:00:13,058 | FDS enable\n",
      "2022-01-21 20:00:21,468 | Calculate Loss\n",
      "2022-01-21 20:00:21,470 | Update Loss\n",
      "2022-01-21 20:00:21,475 | Backward\n",
      "2022-01-21 20:00:35,692 | Epoch: [6][48/48]\tTime  22.65 ( 32.48)\tData 0.0134 (0.0394)\tLoss (L1) 11.704 (11.679)\n",
      "2022-01-21 20:00:35,835 | Create Epoch [6] features of all training data...\n",
      "100%|██████████| 48/48 [09:06<00:00, 11.38s/it]\n",
      "2022-01-21 20:09:42,055 | Updated smoothed statistics on Epoch [6]!\n",
      "2022-01-21 20:09:42,142 | Updated running statistics with Epoch [6] features!\n",
      "2022-01-21 20:09:53,911 | Val: [0/9]\tTime 11.755 (11.755)\tLoss (MSE) 233.256 (233.256)\tLoss (L1) 12.271 (12.271)\n",
      "2022-01-21 20:10:03,062 | Val: [1/9]\tTime  9.151 (10.453)\tLoss (MSE) 188.604 (210.930)\tLoss (L1) 10.894 (11.583)\n",
      "2022-01-21 20:10:12,219 | Val: [2/9]\tTime  9.157 (10.021)\tLoss (MSE) 207.930 (209.930)\tLoss (L1) 11.474 (11.546)\n",
      "2022-01-21 20:10:21,445 | Val: [3/9]\tTime  9.226 ( 9.822)\tLoss (MSE) 207.771 (209.390)\tLoss (L1) 11.624 (11.566)\n",
      "2022-01-21 20:10:30,618 | Val: [4/9]\tTime  9.173 ( 9.692)\tLoss (MSE) 266.602 (220.832)\tLoss (L1) 13.034 (11.860)\n",
      "2022-01-21 20:10:39,713 | Val: [5/9]\tTime  9.095 ( 9.593)\tLoss (MSE) 264.620 (228.130)\tLoss (L1) 13.279 (12.096)\n",
      "2022-01-21 20:10:48,883 | Val: [6/9]\tTime  9.170 ( 9.532)\tLoss (MSE) 225.412 (227.742)\tLoss (L1) 11.706 (12.040)\n",
      "2022-01-21 20:10:58,180 | Val: [7/9]\tTime  9.297 ( 9.503)\tLoss (MSE) 252.873 (230.883)\tLoss (L1) 12.384 (12.083)\n",
      "2022-01-21 20:11:01,271 | Val: [8/9]\tTime  3.091 ( 8.791)\tLoss (MSE) 224.293 (230.600)\tLoss (L1) 11.089 (12.041)\n",
      "2022-01-21 20:11:01,409 |  * Overall: MSE 230.600\tL1 12.041\tG-Mean 7.776\n",
      "2022-01-21 20:11:01,410 |  * Many: MSE 155.460\tL1 9.751\tG-Mean 6.245\n",
      "2022-01-21 20:11:01,410 |  * Median: MSE 369.243\tL1 16.391\tG-Mean 11.931\n",
      "2022-01-21 20:11:01,411 |  * Low: MSE 556.850\tL1 21.636\tG-Mean 18.880\n",
      "2022-01-21 20:11:01,416 | Best L1 Loss: 12.041\n",
      "2022-01-21 20:11:02,721 | ===> Saving current best checkpoint...\n",
      "2022-01-21 20:11:03,832 | Epoch #6: Train loss [11.6787]; Val loss: MSE [230.6000], L1 [12.0406], G-Mean [7.7762]\n",
      "2022-01-21 20:11:03,833 | Training...\n",
      "2022-01-21 20:11:03,834 | Load train loader\n",
      "2022-01-21 20:11:05,144 | ===> Batch : 1\n",
      "2022-01-21 20:11:05,148 | FDS enable\n",
      "2022-01-21 20:11:19,268 | Calculate Loss\n",
      "2022-01-21 20:11:19,269 | Update Loss\n",
      "2022-01-21 20:11:19,377 | Backward\n",
      "2022-01-21 20:11:40,904 | Epoch: [7][ 1/48]\tTime  37.07 ( 37.07)\tData 1.3095 (1.3095)\tLoss (L1) 11.129 (11.129)\n",
      "2022-01-21 20:11:40,918 | ===> Batch : 2\n",
      "2022-01-21 20:11:40,919 | FDS enable\n",
      "2022-01-21 20:11:52,682 | Calculate Loss\n",
      "2022-01-21 20:11:52,684 | Update Loss\n",
      "2022-01-21 20:11:52,688 | Backward\n",
      "2022-01-21 20:12:13,526 | Epoch: [7][ 2/48]\tTime  32.62 ( 34.85)\tData 0.0143 (0.6619)\tLoss (L1) 10.729 (10.929)\n",
      "2022-01-21 20:12:13,539 | ===> Batch : 3\n",
      "2022-01-21 20:12:13,539 | FDS enable\n",
      "2022-01-21 20:12:25,243 | Calculate Loss\n",
      "2022-01-21 20:12:25,244 | Update Loss\n",
      "2022-01-21 20:12:25,248 | Backward\n",
      "2022-01-21 20:12:46,103 | Epoch: [7][ 3/48]\tTime  32.58 ( 34.09)\tData 0.0129 (0.4456)\tLoss (L1) 10.485 (10.781)\n",
      "2022-01-21 20:12:46,116 | ===> Batch : 4\n",
      "2022-01-21 20:12:46,117 | FDS enable\n",
      "2022-01-21 20:12:57,815 | Calculate Loss\n",
      "2022-01-21 20:12:57,817 | Update Loss\n",
      "2022-01-21 20:12:57,821 | Backward\n",
      "2022-01-21 20:13:18,509 | Epoch: [7][ 4/48]\tTime  32.41 ( 33.67)\tData 0.0131 (0.3375)\tLoss (L1) 11.850 (11.048)\n",
      "2022-01-21 20:13:18,522 | ===> Batch : 5\n",
      "2022-01-21 20:13:18,523 | FDS enable\n",
      "2022-01-21 20:13:30,229 | Calculate Loss\n",
      "2022-01-21 20:13:30,230 | Update Loss\n",
      "2022-01-21 20:13:30,234 | Backward\n",
      "2022-01-21 20:13:51,405 | Epoch: [7][ 5/48]\tTime  32.90 ( 33.51)\tData 0.0136 (0.2727)\tLoss (L1) 11.780 (11.195)\n",
      "2022-01-21 20:13:51,418 | ===> Batch : 6\n",
      "2022-01-21 20:13:51,419 | FDS enable\n",
      "2022-01-21 20:14:03,144 | Calculate Loss\n",
      "2022-01-21 20:14:03,145 | Update Loss\n",
      "2022-01-21 20:14:03,150 | Backward\n",
      "2022-01-21 20:14:23,965 | Epoch: [7][ 6/48]\tTime  32.56 ( 33.36)\tData 0.0133 (0.2295)\tLoss (L1) 10.026 (11.000)\n",
      "2022-01-21 20:14:23,978 | ===> Batch : 7\n",
      "2022-01-21 20:14:23,979 | FDS enable\n",
      "2022-01-21 20:14:35,733 | Calculate Loss\n",
      "2022-01-21 20:14:35,735 | Update Loss\n",
      "2022-01-21 20:14:35,739 | Backward\n",
      "2022-01-21 20:14:56,429 | Epoch: [7][ 7/48]\tTime  32.46 ( 33.23)\tData 0.0130 (0.1986)\tLoss (L1) 11.094 (11.013)\n",
      "2022-01-21 20:14:56,442 | ===> Batch : 8\n",
      "2022-01-21 20:14:56,443 | FDS enable\n",
      "2022-01-21 20:15:08,144 | Calculate Loss\n",
      "2022-01-21 20:15:08,145 | Update Loss\n",
      "2022-01-21 20:15:08,150 | Backward\n",
      "2022-01-21 20:15:28,863 | Epoch: [7][ 8/48]\tTime  32.43 ( 33.13)\tData 0.0138 (0.1755)\tLoss (L1) 9.295 (10.798)\n",
      "2022-01-21 20:15:28,879 | ===> Batch : 9\n",
      "2022-01-21 20:15:28,880 | FDS enable\n",
      "2022-01-21 20:15:40,629 | Calculate Loss\n",
      "2022-01-21 20:15:40,630 | Update Loss\n",
      "2022-01-21 20:15:40,635 | Backward\n",
      "2022-01-21 20:16:01,519 | Epoch: [7][ 9/48]\tTime  32.66 ( 33.08)\tData 0.0160 (0.1578)\tLoss (L1) 11.968 (10.928)\n",
      "2022-01-21 20:16:01,535 | ===> Batch : 10\n",
      "2022-01-21 20:16:01,535 | FDS enable\n",
      "2022-01-21 20:16:13,229 | Calculate Loss\n",
      "2022-01-21 20:16:13,231 | Update Loss\n",
      "2022-01-21 20:16:13,235 | Backward\n",
      "2022-01-21 20:16:34,134 | Epoch: [7][10/48]\tTime  32.62 ( 33.03)\tData 0.0160 (0.1436)\tLoss (L1) 10.307 (10.866)\n",
      "2022-01-21 20:16:34,150 | ===> Batch : 11\n",
      "2022-01-21 20:16:34,150 | FDS enable\n",
      "2022-01-21 20:16:45,879 | Calculate Loss\n",
      "2022-01-21 20:16:45,880 | Update Loss\n",
      "2022-01-21 20:16:45,885 | Backward\n",
      "2022-01-21 20:17:06,674 | Epoch: [7][11/48]\tTime  32.54 ( 32.99)\tData 0.0157 (0.1319)\tLoss (L1) 11.937 (10.964)\n",
      "2022-01-21 20:17:06,688 | ===> Batch : 12\n",
      "2022-01-21 20:17:06,688 | FDS enable\n",
      "2022-01-21 20:17:18,367 | Calculate Loss\n",
      "2022-01-21 20:17:18,369 | Update Loss\n",
      "2022-01-21 20:17:18,373 | Backward\n",
      "2022-01-21 20:17:39,211 | Epoch: [7][12/48]\tTime  32.54 ( 32.95)\tData 0.0141 (0.1221)\tLoss (L1) 11.599 (11.016)\n",
      "2022-01-21 20:17:39,224 | ===> Batch : 13\n",
      "2022-01-21 20:17:39,224 | FDS enable\n",
      "2022-01-21 20:17:50,965 | Calculate Loss\n",
      "2022-01-21 20:17:50,966 | Update Loss\n",
      "2022-01-21 20:17:50,971 | Backward\n",
      "2022-01-21 20:18:11,825 | Epoch: [7][13/48]\tTime  32.61 ( 32.92)\tData 0.0126 (0.1137)\tLoss (L1) 11.509 (11.054)\n",
      "2022-01-21 20:18:11,838 | ===> Batch : 14\n",
      "2022-01-21 20:18:11,839 | FDS enable\n",
      "2022-01-21 20:18:23,555 | Calculate Loss\n",
      "2022-01-21 20:18:23,557 | Update Loss\n",
      "2022-01-21 20:18:23,561 | Backward\n",
      "2022-01-21 20:18:44,442 | Epoch: [7][14/48]\tTime  32.62 ( 32.90)\tData 0.0129 (0.1065)\tLoss (L1) 13.246 (11.211)\n",
      "2022-01-21 20:18:44,461 | ===> Batch : 15\n",
      "2022-01-21 20:18:44,462 | FDS enable\n",
      "2022-01-21 20:18:56,191 | Calculate Loss\n",
      "2022-01-21 20:18:56,193 | Update Loss\n",
      "2022-01-21 20:18:56,198 | Backward\n",
      "2022-01-21 20:19:17,304 | Epoch: [7][15/48]\tTime  32.86 ( 32.90)\tData 0.0193 (0.1007)\tLoss (L1) 9.894 (11.123)\n",
      "2022-01-21 20:19:17,319 | ===> Batch : 16\n",
      "2022-01-21 20:19:17,320 | FDS enable\n",
      "2022-01-21 20:19:29,087 | Calculate Loss\n",
      "2022-01-21 20:19:29,088 | Update Loss\n",
      "2022-01-21 20:19:29,092 | Backward\n",
      "2022-01-21 20:19:49,760 | Epoch: [7][16/48]\tTime  32.46 ( 32.87)\tData 0.0155 (0.0954)\tLoss (L1) 11.362 (11.138)\n",
      "2022-01-21 20:19:49,774 | ===> Batch : 17\n",
      "2022-01-21 20:19:49,774 | FDS enable\n",
      "2022-01-21 20:20:01,514 | Calculate Loss\n",
      "2022-01-21 20:20:01,515 | Update Loss\n",
      "2022-01-21 20:20:01,520 | Backward\n",
      "2022-01-21 20:20:22,080 | Epoch: [7][17/48]\tTime  32.32 ( 32.84)\tData 0.0140 (0.0906)\tLoss (L1) 10.337 (11.091)\n",
      "2022-01-21 20:20:22,094 | ===> Batch : 18\n",
      "2022-01-21 20:20:22,094 | FDS enable\n",
      "2022-01-21 20:20:33,781 | Calculate Loss\n",
      "2022-01-21 20:20:33,783 | Update Loss\n",
      "2022-01-21 20:20:33,787 | Backward\n",
      "2022-01-21 20:20:54,890 | Epoch: [7][18/48]\tTime  32.81 ( 32.84)\tData 0.0136 (0.0863)\tLoss (L1) 10.743 (11.072)\n",
      "2022-01-21 20:20:54,904 | ===> Batch : 19\n",
      "2022-01-21 20:20:54,905 | FDS enable\n",
      "2022-01-21 20:21:06,711 | Calculate Loss\n",
      "2022-01-21 20:21:06,713 | Update Loss\n",
      "2022-01-21 20:21:06,717 | Backward\n",
      "2022-01-21 20:21:27,537 | Epoch: [7][19/48]\tTime  32.65 ( 32.83)\tData 0.0148 (0.0825)\tLoss (L1) 12.865 (11.166)\n",
      "2022-01-21 20:21:27,551 | ===> Batch : 20\n",
      "2022-01-21 20:21:27,552 | FDS enable\n",
      "2022-01-21 20:21:39,317 | Calculate Loss\n",
      "2022-01-21 20:21:39,319 | Update Loss\n",
      "2022-01-21 20:21:39,323 | Backward\n",
      "2022-01-21 20:22:00,134 | Epoch: [7][20/48]\tTime  32.60 ( 32.81)\tData 0.0137 (0.0791)\tLoss (L1) 12.169 (11.216)\n",
      "2022-01-21 20:22:00,147 | ===> Batch : 21\n",
      "2022-01-21 20:22:00,148 | FDS enable\n",
      "2022-01-21 20:22:11,907 | Calculate Loss\n",
      "2022-01-21 20:22:11,909 | Update Loss\n",
      "2022-01-21 20:22:11,913 | Backward\n",
      "2022-01-21 20:22:32,741 | Epoch: [7][21/48]\tTime  32.61 ( 32.81)\tData 0.0137 (0.0760)\tLoss (L1) 9.711 (11.144)\n",
      "2022-01-21 20:22:32,755 | ===> Batch : 22\n",
      "2022-01-21 20:22:32,755 | FDS enable\n",
      "2022-01-21 20:22:44,479 | Calculate Loss\n",
      "2022-01-21 20:22:44,480 | Update Loss\n",
      "2022-01-21 20:22:44,484 | Backward\n",
      "2022-01-21 20:23:05,357 | Epoch: [7][22/48]\tTime  32.62 ( 32.80)\tData 0.0139 (0.0732)\tLoss (L1) 11.325 (11.153)\n",
      "2022-01-21 20:23:05,370 | ===> Batch : 23\n",
      "2022-01-21 20:23:05,371 | FDS enable\n",
      "2022-01-21 20:23:17,110 | Calculate Loss\n",
      "2022-01-21 20:23:17,111 | Update Loss\n",
      "2022-01-21 20:23:17,115 | Backward\n",
      "2022-01-21 20:23:38,143 | Epoch: [7][23/48]\tTime  32.79 ( 32.80)\tData 0.0136 (0.0706)\tLoss (L1) 10.949 (11.144)\n",
      "2022-01-21 20:23:38,157 | ===> Batch : 24\n",
      "2022-01-21 20:23:38,157 | FDS enable\n",
      "2022-01-21 20:23:49,885 | Calculate Loss\n",
      "2022-01-21 20:23:49,887 | Update Loss\n",
      "2022-01-21 20:23:49,891 | Backward\n",
      "2022-01-21 20:24:10,594 | Epoch: [7][24/48]\tTime  32.45 ( 32.78)\tData 0.0138 (0.0682)\tLoss (L1) 12.018 (11.180)\n",
      "2022-01-21 20:24:10,607 | ===> Batch : 25\n",
      "2022-01-21 20:24:10,607 | FDS enable\n",
      "2022-01-21 20:24:22,363 | Calculate Loss\n",
      "2022-01-21 20:24:22,364 | Update Loss\n",
      "2022-01-21 20:24:22,368 | Backward\n",
      "2022-01-21 20:24:43,241 | Epoch: [7][25/48]\tTime  32.65 ( 32.78)\tData 0.0134 (0.0660)\tLoss (L1) 13.340 (11.267)\n",
      "2022-01-21 20:24:43,254 | ===> Batch : 26\n",
      "2022-01-21 20:24:43,255 | FDS enable\n",
      "2022-01-21 20:24:54,996 | Calculate Loss\n",
      "2022-01-21 20:24:54,997 | Update Loss\n",
      "2022-01-21 20:24:55,001 | Backward\n",
      "2022-01-21 20:25:15,711 | Epoch: [7][26/48]\tTime  32.47 ( 32.76)\tData 0.0138 (0.0640)\tLoss (L1) 12.072 (11.298)\n",
      "2022-01-21 20:25:15,725 | ===> Batch : 27\n",
      "2022-01-21 20:25:15,725 | FDS enable\n",
      "2022-01-21 20:25:27,459 | Calculate Loss\n",
      "2022-01-21 20:25:27,460 | Update Loss\n",
      "2022-01-21 20:25:27,465 | Backward\n",
      "2022-01-21 20:25:48,521 | Epoch: [7][27/48]\tTime  32.81 ( 32.77)\tData 0.0142 (0.0622)\tLoss (L1) 10.813 (11.280)\n",
      "2022-01-21 20:25:48,535 | ===> Batch : 28\n",
      "2022-01-21 20:25:48,535 | FDS enable\n",
      "2022-01-21 20:26:00,296 | Calculate Loss\n",
      "2022-01-21 20:26:00,298 | Update Loss\n",
      "2022-01-21 20:26:00,302 | Backward\n",
      "2022-01-21 20:26:20,954 | Epoch: [7][28/48]\tTime  32.43 ( 32.75)\tData 0.0138 (0.0604)\tLoss (L1) 11.516 (11.288)\n",
      "2022-01-21 20:26:20,968 | ===> Batch : 29\n",
      "2022-01-21 20:26:20,968 | FDS enable\n",
      "2022-01-21 20:26:32,719 | Calculate Loss\n",
      "2022-01-21 20:26:32,720 | Update Loss\n",
      "2022-01-21 20:26:32,724 | Backward\n",
      "2022-01-21 20:26:53,519 | Epoch: [7][29/48]\tTime  32.57 ( 32.75)\tData 0.0136 (0.0588)\tLoss (L1) 11.637 (11.300)\n",
      "2022-01-21 20:26:53,533 | ===> Batch : 30\n",
      "2022-01-21 20:26:53,533 | FDS enable\n",
      "2022-01-21 20:27:05,281 | Calculate Loss\n",
      "2022-01-21 20:27:05,282 | Update Loss\n",
      "2022-01-21 20:27:05,286 | Backward\n",
      "2022-01-21 20:27:26,044 | Epoch: [7][30/48]\tTime  32.53 ( 32.74)\tData 0.0135 (0.0573)\tLoss (L1) 13.623 (11.378)\n",
      "2022-01-21 20:27:26,058 | ===> Batch : 31\n",
      "2022-01-21 20:27:26,059 | FDS enable\n",
      "2022-01-21 20:27:37,797 | Calculate Loss\n",
      "2022-01-21 20:27:37,798 | Update Loss\n",
      "2022-01-21 20:27:37,803 | Backward\n",
      "2022-01-21 20:27:59,029 | Epoch: [7][31/48]\tTime  32.98 ( 32.75)\tData 0.0139 (0.0559)\tLoss (L1) 11.810 (11.392)\n",
      "2022-01-21 20:27:59,043 | ===> Batch : 32\n",
      "2022-01-21 20:27:59,043 | FDS enable\n",
      "2022-01-21 20:28:10,707 | Calculate Loss\n",
      "2022-01-21 20:28:10,708 | Update Loss\n",
      "2022-01-21 20:28:10,713 | Backward\n",
      "2022-01-21 20:28:31,578 | Epoch: [7][32/48]\tTime  32.55 ( 32.74)\tData 0.0136 (0.0546)\tLoss (L1) 12.280 (11.419)\n",
      "2022-01-21 20:28:31,592 | ===> Batch : 33\n",
      "2022-01-21 20:28:31,592 | FDS enable\n",
      "2022-01-21 20:28:43,173 | Calculate Loss\n",
      "2022-01-21 20:28:43,175 | Update Loss\n",
      "2022-01-21 20:28:43,179 | Backward\n",
      "2022-01-21 20:29:04,336 | Epoch: [7][33/48]\tTime  32.76 ( 32.74)\tData 0.0134 (0.0533)\tLoss (L1) 10.284 (11.385)\n",
      "2022-01-21 20:29:04,349 | ===> Batch : 34\n",
      "2022-01-21 20:29:04,350 | FDS enable\n",
      "2022-01-21 20:29:15,958 | Calculate Loss\n",
      "2022-01-21 20:29:15,959 | Update Loss\n",
      "2022-01-21 20:29:15,964 | Backward\n",
      "2022-01-21 20:29:36,571 | Epoch: [7][34/48]\tTime  32.24 ( 32.73)\tData 0.0138 (0.0522)\tLoss (L1) 9.506 (11.330)\n",
      "2022-01-21 20:29:36,585 | ===> Batch : 35\n",
      "2022-01-21 20:29:36,586 | FDS enable\n",
      "2022-01-21 20:29:48,143 | Calculate Loss\n",
      "2022-01-21 20:29:48,145 | Update Loss\n",
      "2022-01-21 20:29:48,149 | Backward\n",
      "2022-01-21 20:30:09,052 | Epoch: [7][35/48]\tTime  32.48 ( 32.72)\tData 0.0144 (0.0511)\tLoss (L1) 10.943 (11.319)\n",
      "2022-01-21 20:30:09,066 | ===> Batch : 36\n",
      "2022-01-21 20:30:09,066 | FDS enable\n",
      "2022-01-21 20:30:20,652 | Calculate Loss\n",
      "2022-01-21 20:30:20,654 | Update Loss\n",
      "2022-01-21 20:30:20,659 | Backward\n",
      "2022-01-21 20:30:41,495 | Epoch: [7][36/48]\tTime  32.44 ( 32.71)\tData 0.0134 (0.0500)\tLoss (L1) 12.127 (11.341)\n",
      "2022-01-21 20:30:41,509 | ===> Batch : 37\n",
      "2022-01-21 20:30:41,510 | FDS enable\n",
      "2022-01-21 20:30:53,069 | Calculate Loss\n",
      "2022-01-21 20:30:53,071 | Update Loss\n",
      "2022-01-21 20:30:53,075 | Backward\n",
      "2022-01-21 20:31:13,922 | Epoch: [7][37/48]\tTime  32.43 ( 32.71)\tData 0.0139 (0.0491)\tLoss (L1) 12.623 (11.376)\n",
      "2022-01-21 20:31:13,935 | ===> Batch : 38\n",
      "2022-01-21 20:31:13,936 | FDS enable\n",
      "2022-01-21 20:31:25,514 | Calculate Loss\n",
      "2022-01-21 20:31:25,516 | Update Loss\n",
      "2022-01-21 20:31:25,520 | Backward\n",
      "2022-01-21 20:31:46,664 | Epoch: [7][38/48]\tTime  32.74 ( 32.71)\tData 0.0136 (0.0481)\tLoss (L1) 10.349 (11.349)\n",
      "2022-01-21 20:31:46,678 | ===> Batch : 39\n",
      "2022-01-21 20:31:46,678 | FDS enable\n",
      "2022-01-21 20:31:58,249 | Calculate Loss\n",
      "2022-01-21 20:31:58,250 | Update Loss\n",
      "2022-01-21 20:31:58,255 | Backward\n",
      "2022-01-21 20:32:19,467 | Epoch: [7][39/48]\tTime  32.80 ( 32.71)\tData 0.0139 (0.0473)\tLoss (L1) 12.137 (11.369)\n",
      "2022-01-21 20:32:19,480 | ===> Batch : 40\n",
      "2022-01-21 20:32:19,481 | FDS enable\n",
      "2022-01-21 20:32:31,046 | Calculate Loss\n",
      "2022-01-21 20:32:31,047 | Update Loss\n",
      "2022-01-21 20:32:31,052 | Backward\n",
      "2022-01-21 20:32:51,368 | Epoch: [7][40/48]\tTime  31.90 ( 32.69)\tData 0.0136 (0.0464)\tLoss (L1) 12.377 (11.394)\n",
      "2022-01-21 20:32:51,381 | ===> Batch : 41\n",
      "2022-01-21 20:32:51,382 | FDS enable\n",
      "2022-01-21 20:33:02,965 | Calculate Loss\n",
      "2022-01-21 20:33:02,967 | Update Loss\n",
      "2022-01-21 20:33:02,971 | Backward\n",
      "2022-01-21 20:33:23,617 | Epoch: [7][41/48]\tTime  32.25 ( 32.68)\tData 0.0135 (0.0456)\tLoss (L1) 10.049 (11.361)\n",
      "2022-01-21 20:33:23,630 | ===> Batch : 42\n",
      "2022-01-21 20:33:23,631 | FDS enable\n",
      "2022-01-21 20:33:35,172 | Calculate Loss\n",
      "2022-01-21 20:33:35,173 | Update Loss\n",
      "2022-01-21 20:33:35,178 | Backward\n",
      "2022-01-21 20:33:56,273 | Epoch: [7][42/48]\tTime  32.66 ( 32.68)\tData 0.0136 (0.0449)\tLoss (L1) 9.999 (11.329)\n",
      "2022-01-21 20:33:56,286 | ===> Batch : 43\n",
      "2022-01-21 20:33:56,287 | FDS enable\n",
      "2022-01-21 20:34:07,883 | Calculate Loss\n",
      "2022-01-21 20:34:07,885 | Update Loss\n",
      "2022-01-21 20:34:07,889 | Backward\n",
      "2022-01-21 20:34:28,575 | Epoch: [7][43/48]\tTime  32.30 ( 32.67)\tData 0.0138 (0.0441)\tLoss (L1) 10.386 (11.307)\n",
      "2022-01-21 20:34:28,589 | ===> Batch : 44\n",
      "2022-01-21 20:34:28,589 | FDS enable\n",
      "2022-01-21 20:34:40,231 | Calculate Loss\n",
      "2022-01-21 20:34:40,233 | Update Loss\n",
      "2022-01-21 20:34:40,237 | Backward\n",
      "2022-01-21 20:35:01,322 | Epoch: [7][44/48]\tTime  32.75 ( 32.67)\tData 0.0136 (0.0434)\tLoss (L1) 11.137 (11.303)\n",
      "2022-01-21 20:35:01,336 | ===> Batch : 45\n",
      "2022-01-21 20:35:01,337 | FDS enable\n",
      "2022-01-21 20:35:12,917 | Calculate Loss\n",
      "2022-01-21 20:35:12,918 | Update Loss\n",
      "2022-01-21 20:35:12,923 | Backward\n",
      "2022-01-21 20:35:33,848 | Epoch: [7][45/48]\tTime  32.53 ( 32.67)\tData 0.0137 (0.0428)\tLoss (L1) 11.107 (11.299)\n",
      "2022-01-21 20:35:33,861 | ===> Batch : 46\n",
      "2022-01-21 20:35:33,862 | FDS enable\n",
      "2022-01-21 20:35:45,481 | Calculate Loss\n",
      "2022-01-21 20:35:45,482 | Update Loss\n",
      "2022-01-21 20:35:45,487 | Backward\n",
      "2022-01-21 20:36:06,471 | Epoch: [7][46/48]\tTime  32.62 ( 32.67)\tData 0.0137 (0.0421)\tLoss (L1) 9.974 (11.270)\n",
      "2022-01-21 20:36:06,484 | ===> Batch : 47\n",
      "2022-01-21 20:36:06,485 | FDS enable\n",
      "2022-01-21 20:36:18,102 | Calculate Loss\n",
      "2022-01-21 20:36:18,103 | Update Loss\n",
      "2022-01-21 20:36:18,108 | Backward\n",
      "2022-01-21 20:36:39,235 | Epoch: [7][47/48]\tTime  32.76 ( 32.67)\tData 0.0135 (0.0415)\tLoss (L1) 12.027 (11.286)\n",
      "2022-01-21 20:36:39,249 | ===> Batch : 48\n",
      "2022-01-21 20:36:39,250 | FDS enable\n",
      "2022-01-21 20:36:47,687 | Calculate Loss\n",
      "2022-01-21 20:36:47,689 | Update Loss\n",
      "2022-01-21 20:36:47,693 | Backward\n",
      "2022-01-21 20:37:01,922 | Epoch: [7][48/48]\tTime  22.69 ( 32.46)\tData 0.0140 (0.0410)\tLoss (L1) 14.800 (11.337)\n",
      "2022-01-21 20:37:02,069 | Create Epoch [7] features of all training data...\n",
      "100%|██████████| 48/48 [09:08<00:00, 11.43s/it]\n",
      "2022-01-21 20:46:10,561 | Updated smoothed statistics on Epoch [7]!\n",
      "2022-01-21 20:46:10,651 | Updated running statistics with Epoch [7] features!\n",
      "2022-01-21 20:46:22,560 | Val: [0/9]\tTime 11.899 (11.899)\tLoss (MSE) 205.169 (205.169)\tLoss (L1) 11.546 (11.546)\n",
      "2022-01-21 20:46:31,697 | Val: [1/9]\tTime  9.137 (10.518)\tLoss (MSE) 189.365 (197.267)\tLoss (L1) 10.655 (11.100)\n",
      "2022-01-21 20:46:40,854 | Val: [2/9]\tTime  9.157 (10.064)\tLoss (MSE) 202.533 (199.022)\tLoss (L1) 11.393 (11.198)\n",
      "2022-01-21 20:46:50,010 | Val: [3/9]\tTime  9.157 ( 9.837)\tLoss (MSE) 210.247 (201.829)\tLoss (L1) 11.674 (11.317)\n",
      "2022-01-21 20:46:59,173 | Val: [4/9]\tTime  9.163 ( 9.702)\tLoss (MSE) 243.516 (210.166)\tLoss (L1) 12.245 (11.502)\n",
      "2022-01-21 20:47:08,334 | Val: [5/9]\tTime  9.161 ( 9.612)\tLoss (MSE) 225.868 (212.783)\tLoss (L1) 12.260 (11.629)\n",
      "2022-01-21 20:47:17,488 | Val: [6/9]\tTime  9.154 ( 9.547)\tLoss (MSE) 180.561 (208.180)\tLoss (L1) 10.692 (11.495)\n",
      "2022-01-21 20:47:26,666 | Val: [7/9]\tTime  9.178 ( 9.501)\tLoss (MSE) 240.943 (212.275)\tLoss (L1) 11.946 (11.551)\n",
      "2022-01-21 20:47:29,760 | Val: [8/9]\tTime  3.094 ( 8.789)\tLoss (MSE) 201.790 (211.825)\tLoss (L1) 10.909 (11.524)\n",
      "2022-01-21 20:47:29,886 |  * Overall: MSE 211.825\tL1 11.524\tG-Mean 7.646\n",
      "2022-01-21 20:47:29,887 |  * Many: MSE 152.311\tL1 9.708\tG-Mean 6.324\n",
      "2022-01-21 20:47:29,887 |  * Median: MSE 320.379\tL1 14.927\tG-Mean 11.113\n",
      "2022-01-21 20:47:29,888 |  * Low: MSE 473.701\tL1 19.260\tG-Mean 16.327\n",
      "2022-01-21 20:47:29,893 | Best L1 Loss: 11.524\n",
      "2022-01-21 20:47:31,130 | ===> Saving current best checkpoint...\n",
      "2022-01-21 20:47:32,284 | Epoch #7: Train loss [11.3366]; Val loss: MSE [211.8246], L1 [11.5237], G-Mean [7.6462]\n",
      "2022-01-21 20:47:32,288 | Training...\n",
      "2022-01-21 20:47:32,289 | Load train loader\n",
      "2022-01-21 20:47:33,463 | ===> Batch : 1\n",
      "2022-01-21 20:47:33,465 | FDS enable\n",
      "2022-01-21 20:47:47,690 | Calculate Loss\n",
      "2022-01-21 20:47:47,691 | Update Loss\n",
      "2022-01-21 20:47:47,798 | Backward\n",
      "2022-01-21 20:48:09,246 | Epoch: [8][ 1/48]\tTime  36.96 ( 36.96)\tData 1.1739 (1.1739)\tLoss (L1) 10.869 (10.869)\n",
      "2022-01-21 20:48:09,260 | ===> Batch : 2\n",
      "2022-01-21 20:48:09,260 | FDS enable\n",
      "2022-01-21 20:48:20,967 | Calculate Loss\n",
      "2022-01-21 20:48:20,968 | Update Loss\n",
      "2022-01-21 20:48:20,972 | Backward\n",
      "2022-01-21 20:48:41,472 | Epoch: [8][ 2/48]\tTime  32.23 ( 34.59)\tData 0.0143 (0.5941)\tLoss (L1) 11.039 (10.954)\n",
      "2022-01-21 20:48:41,488 | ===> Batch : 3\n",
      "2022-01-21 20:48:41,488 | FDS enable\n",
      "2022-01-21 20:48:53,194 | Calculate Loss\n",
      "2022-01-21 20:48:53,197 | Update Loss\n",
      "2022-01-21 20:48:53,201 | Backward\n",
      "2022-01-21 20:49:14,317 | Epoch: [8][ 3/48]\tTime  32.85 ( 34.01)\tData 0.0161 (0.4014)\tLoss (L1) 11.234 (11.047)\n",
      "2022-01-21 20:49:14,332 | ===> Batch : 4\n",
      "2022-01-21 20:49:14,333 | FDS enable\n",
      "2022-01-21 20:49:26,082 | Calculate Loss\n",
      "2022-01-21 20:49:26,084 | Update Loss\n",
      "2022-01-21 20:49:26,088 | Backward\n",
      "2022-01-21 20:49:46,527 | Epoch: [8][ 4/48]\tTime  32.21 ( 33.56)\tData 0.0148 (0.3048)\tLoss (L1) 12.201 (11.336)\n",
      "2022-01-21 20:49:46,551 | ===> Batch : 5\n",
      "2022-01-21 20:49:46,552 | FDS enable\n",
      "2022-01-21 20:49:58,266 | Calculate Loss\n",
      "2022-01-21 20:49:58,267 | Update Loss\n",
      "2022-01-21 20:49:58,272 | Backward\n",
      "2022-01-21 20:50:18,866 | Epoch: [8][ 5/48]\tTime  32.34 ( 33.32)\tData 0.0237 (0.2486)\tLoss (L1) 9.577 (10.984)\n",
      "2022-01-21 20:50:18,879 | ===> Batch : 6\n",
      "2022-01-21 20:50:18,879 | FDS enable\n",
      "2022-01-21 20:50:30,534 | Calculate Loss\n",
      "2022-01-21 20:50:30,536 | Update Loss\n",
      "2022-01-21 20:50:30,540 | Backward\n",
      "2022-01-21 20:50:51,728 | Epoch: [8][ 6/48]\tTime  32.86 ( 33.24)\tData 0.0131 (0.2093)\tLoss (L1) 11.998 (11.153)\n",
      "2022-01-21 20:50:51,741 | ===> Batch : 7\n",
      "2022-01-21 20:50:51,741 | FDS enable\n",
      "2022-01-21 20:51:03,548 | Calculate Loss\n",
      "2022-01-21 20:51:03,549 | Update Loss\n",
      "2022-01-21 20:51:03,554 | Backward\n",
      "2022-01-21 20:51:24,762 | Epoch: [8][ 7/48]\tTime  33.03 ( 33.21)\tData 0.0130 (0.1813)\tLoss (L1) 11.126 (11.149)\n",
      "2022-01-21 20:51:24,777 | ===> Batch : 8\n",
      "2022-01-21 20:51:24,777 | FDS enable\n",
      "2022-01-21 20:51:36,494 | Calculate Loss\n",
      "2022-01-21 20:51:36,496 | Update Loss\n",
      "2022-01-21 20:51:36,500 | Backward\n",
      "2022-01-21 20:51:57,452 | Epoch: [8][ 8/48]\tTime  32.69 ( 33.15)\tData 0.0146 (0.1604)\tLoss (L1) 10.006 (11.006)\n",
      "2022-01-21 20:51:57,469 | ===> Batch : 9\n",
      "2022-01-21 20:51:57,470 | FDS enable\n",
      "2022-01-21 20:52:09,173 | Calculate Loss\n",
      "2022-01-21 20:52:09,174 | Update Loss\n",
      "2022-01-21 20:52:09,178 | Backward\n",
      "2022-01-21 20:52:30,075 | Epoch: [8][ 9/48]\tTime  32.62 ( 33.09)\tData 0.0170 (0.1445)\tLoss (L1) 9.729 (10.864)\n",
      "2022-01-21 20:52:30,089 | ===> Batch : 10\n",
      "2022-01-21 20:52:30,089 | FDS enable\n",
      "2022-01-21 20:52:41,777 | Calculate Loss\n",
      "2022-01-21 20:52:41,779 | Update Loss\n",
      "2022-01-21 20:52:41,783 | Backward\n",
      "2022-01-21 20:53:02,955 | Epoch: [8][10/48]\tTime  32.88 ( 33.07)\tData 0.0140 (0.1314)\tLoss (L1) 13.800 (11.158)\n",
      "2022-01-21 20:53:02,969 | ===> Batch : 11\n",
      "2022-01-21 20:53:02,970 | FDS enable\n",
      "2022-01-21 20:53:14,694 | Calculate Loss\n",
      "2022-01-21 20:53:14,696 | Update Loss\n",
      "2022-01-21 20:53:14,700 | Backward\n",
      "2022-01-21 20:53:35,805 | Epoch: [8][11/48]\tTime  32.85 ( 33.05)\tData 0.0141 (0.1208)\tLoss (L1) 11.752 (11.212)\n",
      "2022-01-21 20:53:35,818 | ===> Batch : 12\n",
      "2022-01-21 20:53:35,818 | FDS enable\n",
      "2022-01-21 20:53:47,569 | Calculate Loss\n",
      "2022-01-21 20:53:47,570 | Update Loss\n",
      "2022-01-21 20:53:47,575 | Backward\n",
      "2022-01-21 20:54:08,377 | Epoch: [8][12/48]\tTime  32.57 ( 33.01)\tData 0.0134 (0.1118)\tLoss (L1) 10.275 (11.134)\n",
      "2022-01-21 20:54:08,400 | ===> Batch : 13\n",
      "2022-01-21 20:54:08,401 | FDS enable\n",
      "2022-01-21 20:54:20,132 | Calculate Loss\n",
      "2022-01-21 20:54:20,134 | Update Loss\n",
      "2022-01-21 20:54:20,138 | Backward\n",
      "2022-01-21 20:54:41,129 | Epoch: [8][13/48]\tTime  32.75 ( 32.99)\tData 0.0232 (0.1050)\tLoss (L1) 12.085 (11.207)\n",
      "2022-01-21 20:54:41,141 | ===> Batch : 14\n",
      "2022-01-21 20:54:41,142 | FDS enable\n",
      "2022-01-21 20:54:52,869 | Calculate Loss\n",
      "2022-01-21 20:54:52,871 | Update Loss\n",
      "2022-01-21 20:54:52,875 | Backward\n",
      "2022-01-21 20:55:13,129 | Epoch: [8][14/48]\tTime  32.00 ( 32.92)\tData 0.0127 (0.0984)\tLoss (L1) 9.737 (11.102)\n",
      "2022-01-21 20:55:13,141 | ===> Batch : 15\n",
      "2022-01-21 20:55:13,142 | FDS enable\n",
      "2022-01-21 20:55:24,847 | Calculate Loss\n",
      "2022-01-21 20:55:24,848 | Update Loss\n",
      "2022-01-21 20:55:24,853 | Backward\n",
      "2022-01-21 20:55:45,840 | Epoch: [8][15/48]\tTime  32.71 ( 32.90)\tData 0.0126 (0.0927)\tLoss (L1) 11.760 (11.146)\n",
      "2022-01-21 20:55:45,853 | ===> Batch : 16\n",
      "2022-01-21 20:55:45,854 | FDS enable\n",
      "2022-01-21 20:55:57,565 | Calculate Loss\n",
      "2022-01-21 20:55:57,568 | Update Loss\n",
      "2022-01-21 20:55:57,573 | Backward\n",
      "2022-01-21 20:56:18,384 | Epoch: [8][16/48]\tTime  32.54 ( 32.88)\tData 0.0139 (0.0878)\tLoss (L1) 9.652 (11.053)\n",
      "2022-01-21 20:56:18,398 | ===> Batch : 17\n",
      "2022-01-21 20:56:18,398 | FDS enable\n",
      "2022-01-21 20:56:30,112 | Calculate Loss\n",
      "2022-01-21 20:56:30,113 | Update Loss\n",
      "2022-01-21 20:56:30,118 | Backward\n",
      "2022-01-21 20:56:51,140 | Epoch: [8][17/48]\tTime  32.76 ( 32.87)\tData 0.0140 (0.0834)\tLoss (L1) 11.314 (11.068)\n",
      "2022-01-21 20:56:51,154 | ===> Batch : 18\n",
      "2022-01-21 20:56:51,154 | FDS enable\n",
      "2022-01-21 20:57:02,881 | Calculate Loss\n",
      "2022-01-21 20:57:02,883 | Update Loss\n",
      "2022-01-21 20:57:02,887 | Backward\n",
      "2022-01-21 20:57:23,417 | Epoch: [8][18/48]\tTime  32.28 ( 32.84)\tData 0.0137 (0.0796)\tLoss (L1) 11.702 (11.103)\n",
      "2022-01-21 20:57:23,431 | ===> Batch : 19\n",
      "2022-01-21 20:57:23,431 | FDS enable\n",
      "2022-01-21 20:57:35,224 | Calculate Loss\n",
      "2022-01-21 20:57:35,226 | Update Loss\n",
      "2022-01-21 20:57:35,230 | Backward\n",
      "2022-01-21 20:57:56,708 | Epoch: [8][19/48]\tTime  33.29 ( 32.86)\tData 0.0140 (0.0761)\tLoss (L1) 13.142 (11.210)\n",
      "2022-01-21 20:57:56,722 | ===> Batch : 20\n",
      "2022-01-21 20:57:56,723 | FDS enable\n",
      "2022-01-21 20:58:08,435 | Calculate Loss\n",
      "2022-01-21 20:58:08,436 | Update Loss\n",
      "2022-01-21 20:58:08,441 | Backward\n",
      "2022-01-21 20:58:29,069 | Epoch: [8][20/48]\tTime  32.36 ( 32.84)\tData 0.0140 (0.0730)\tLoss (L1) 11.569 (11.228)\n",
      "2022-01-21 20:58:29,084 | ===> Batch : 21\n",
      "2022-01-21 20:58:29,085 | FDS enable\n",
      "2022-01-21 20:58:40,964 | Calculate Loss\n",
      "2022-01-21 20:58:40,966 | Update Loss\n",
      "2022-01-21 20:58:40,970 | Backward\n",
      "2022-01-21 20:59:01,684 | Epoch: [8][21/48]\tTime  32.61 ( 32.83)\tData 0.0149 (0.0702)\tLoss (L1) 10.239 (11.181)\n",
      "2022-01-21 20:59:01,697 | ===> Batch : 22\n",
      "2022-01-21 20:59:01,697 | FDS enable\n",
      "2022-01-21 20:59:13,434 | Calculate Loss\n",
      "2022-01-21 20:59:13,436 | Update Loss\n",
      "2022-01-21 20:59:13,440 | Backward\n",
      "2022-01-21 20:59:34,423 | Epoch: [8][22/48]\tTime  32.74 ( 32.82)\tData 0.0134 (0.0676)\tLoss (L1) 10.816 (11.165)\n",
      "2022-01-21 20:59:34,437 | ===> Batch : 23\n",
      "2022-01-21 20:59:34,438 | FDS enable\n",
      "2022-01-21 20:59:46,210 | Calculate Loss\n",
      "2022-01-21 20:59:46,211 | Update Loss\n",
      "2022-01-21 20:59:46,216 | Backward\n",
      "2022-01-21 21:00:07,355 | Epoch: [8][23/48]\tTime  32.93 ( 32.83)\tData 0.0142 (0.0653)\tLoss (L1) 10.889 (11.153)\n",
      "2022-01-21 21:00:07,369 | ===> Batch : 24\n",
      "2022-01-21 21:00:07,370 | FDS enable\n",
      "2022-01-21 21:00:19,113 | Calculate Loss\n",
      "2022-01-21 21:00:19,115 | Update Loss\n",
      "2022-01-21 21:00:19,119 | Backward\n",
      "2022-01-21 21:00:39,759 | Epoch: [8][24/48]\tTime  32.40 ( 32.81)\tData 0.0138 (0.0632)\tLoss (L1) 11.880 (11.183)\n",
      "2022-01-21 21:00:39,779 | ===> Batch : 25\n",
      "2022-01-21 21:00:39,779 | FDS enable\n",
      "2022-01-21 21:00:51,584 | Calculate Loss\n",
      "2022-01-21 21:00:51,586 | Update Loss\n",
      "2022-01-21 21:00:51,591 | Backward\n",
      "2022-01-21 21:01:12,533 | Epoch: [8][25/48]\tTime  32.77 ( 32.81)\tData 0.0201 (0.0615)\tLoss (L1) 9.863 (11.130)\n",
      "2022-01-21 21:01:12,548 | ===> Batch : 26\n",
      "2022-01-21 21:01:12,549 | FDS enable\n",
      "2022-01-21 21:01:24,413 | Calculate Loss\n",
      "2022-01-21 21:01:24,414 | Update Loss\n",
      "2022-01-21 21:01:24,419 | Backward\n",
      "2022-01-21 21:01:44,772 | Epoch: [8][26/48]\tTime  32.24 ( 32.79)\tData 0.0149 (0.0597)\tLoss (L1) 10.992 (11.125)\n",
      "2022-01-21 21:01:44,786 | ===> Batch : 27\n",
      "2022-01-21 21:01:44,786 | FDS enable\n",
      "2022-01-21 21:01:56,537 | Calculate Loss\n",
      "2022-01-21 21:01:56,539 | Update Loss\n",
      "2022-01-21 21:01:56,543 | Backward\n",
      "2022-01-21 21:02:17,765 | Epoch: [8][27/48]\tTime  32.99 ( 32.80)\tData 0.0138 (0.0580)\tLoss (L1) 9.986 (11.083)\n",
      "2022-01-21 21:02:17,779 | ===> Batch : 28\n",
      "2022-01-21 21:02:17,780 | FDS enable\n",
      "2022-01-21 21:02:29,538 | Calculate Loss\n",
      "2022-01-21 21:02:29,539 | Update Loss\n",
      "2022-01-21 21:02:29,544 | Backward\n",
      "2022-01-21 21:02:50,074 | Epoch: [8][28/48]\tTime  32.31 ( 32.78)\tData 0.0140 (0.0564)\tLoss (L1) 8.958 (11.007)\n",
      "2022-01-21 21:02:50,088 | ===> Batch : 29\n",
      "2022-01-21 21:02:50,088 | FDS enable\n",
      "2022-01-21 21:03:01,895 | Calculate Loss\n",
      "2022-01-21 21:03:01,899 | Update Loss\n",
      "2022-01-21 21:03:01,903 | Backward\n",
      "2022-01-21 21:03:22,666 | Epoch: [8][29/48]\tTime  32.59 ( 32.77)\tData 0.0136 (0.0549)\tLoss (L1) 9.113 (10.941)\n",
      "2022-01-21 21:03:22,680 | ===> Batch : 30\n",
      "2022-01-21 21:03:22,681 | FDS enable\n",
      "2022-01-21 21:03:34,473 | Calculate Loss\n",
      "2022-01-21 21:03:34,475 | Update Loss\n",
      "2022-01-21 21:03:34,479 | Backward\n",
      "2022-01-21 21:03:55,436 | Epoch: [8][30/48]\tTime  32.77 ( 32.77)\tData 0.0139 (0.0536)\tLoss (L1) 11.399 (10.957)\n",
      "2022-01-21 21:03:55,451 | ===> Batch : 31\n",
      "2022-01-21 21:03:55,452 | FDS enable\n",
      "2022-01-21 21:04:07,299 | Calculate Loss\n",
      "2022-01-21 21:04:07,300 | Update Loss\n",
      "2022-01-21 21:04:07,305 | Backward\n",
      "2022-01-21 21:04:28,471 | Epoch: [8][31/48]\tTime  33.03 ( 32.78)\tData 0.0148 (0.0523)\tLoss (L1) 11.882 (10.987)\n",
      "2022-01-21 21:04:28,486 | ===> Batch : 32\n",
      "2022-01-21 21:04:28,486 | FDS enable\n",
      "2022-01-21 21:04:40,189 | Calculate Loss\n",
      "2022-01-21 21:04:40,191 | Update Loss\n",
      "2022-01-21 21:04:40,196 | Backward\n",
      "2022-01-21 21:05:01,307 | Epoch: [8][32/48]\tTime  32.84 ( 32.78)\tData 0.0150 (0.0511)\tLoss (L1) 10.837 (10.982)\n",
      "2022-01-21 21:05:01,322 | ===> Batch : 33\n",
      "2022-01-21 21:05:01,322 | FDS enable\n",
      "2022-01-21 21:05:12,919 | Calculate Loss\n",
      "2022-01-21 21:05:12,921 | Update Loss\n",
      "2022-01-21 21:05:12,925 | Backward\n",
      "2022-01-21 21:05:33,218 | Epoch: [8][33/48]\tTime  31.91 ( 32.76)\tData 0.0146 (0.0500)\tLoss (L1) 12.226 (11.020)\n",
      "2022-01-21 21:05:33,232 | ===> Batch : 34\n",
      "2022-01-21 21:05:33,233 | FDS enable\n",
      "2022-01-21 21:05:45,025 | Calculate Loss\n",
      "2022-01-21 21:05:45,027 | Update Loss\n",
      "2022-01-21 21:05:45,031 | Backward\n",
      "2022-01-21 21:06:05,876 | Epoch: [8][34/48]\tTime  32.66 ( 32.75)\tData 0.0145 (0.0490)\tLoss (L1) 11.392 (11.031)\n",
      "2022-01-21 21:06:05,891 | ===> Batch : 35\n",
      "2022-01-21 21:06:05,892 | FDS enable\n",
      "2022-01-21 21:06:17,542 | Calculate Loss\n",
      "2022-01-21 21:06:17,543 | Update Loss\n",
      "2022-01-21 21:06:17,547 | Backward\n",
      "2022-01-21 21:06:38,792 | Epoch: [8][35/48]\tTime  32.92 ( 32.76)\tData 0.0157 (0.0480)\tLoss (L1) 12.099 (11.061)\n",
      "2022-01-21 21:06:38,805 | ===> Batch : 36\n",
      "2022-01-21 21:06:38,806 | FDS enable\n",
      "2022-01-21 21:06:50,403 | Calculate Loss\n",
      "2022-01-21 21:06:50,404 | Update Loss\n",
      "2022-01-21 21:06:50,409 | Backward\n",
      "2022-01-21 21:07:11,676 | Epoch: [8][36/48]\tTime  32.88 ( 32.76)\tData 0.0136 (0.0471)\tLoss (L1) 11.439 (11.072)\n",
      "2022-01-21 21:07:11,689 | ===> Batch : 37\n",
      "2022-01-21 21:07:11,690 | FDS enable\n",
      "2022-01-21 21:07:23,388 | Calculate Loss\n",
      "2022-01-21 21:07:23,389 | Update Loss\n",
      "2022-01-21 21:07:23,394 | Backward\n",
      "2022-01-21 21:07:44,472 | Epoch: [8][37/48]\tTime  32.80 ( 32.76)\tData 0.0138 (0.0462)\tLoss (L1) 10.101 (11.045)\n",
      "2022-01-21 21:07:44,486 | ===> Batch : 38\n",
      "2022-01-21 21:07:44,486 | FDS enable\n",
      "2022-01-21 21:07:56,329 | Calculate Loss\n",
      "2022-01-21 21:07:56,331 | Update Loss\n",
      "2022-01-21 21:07:56,335 | Backward\n",
      "2022-01-21 21:08:17,289 | Epoch: [8][38/48]\tTime  32.82 ( 32.76)\tData 0.0140 (0.0453)\tLoss (L1) 12.710 (11.089)\n",
      "2022-01-21 21:08:17,302 | ===> Batch : 39\n",
      "2022-01-21 21:08:17,303 | FDS enable\n",
      "2022-01-21 21:08:28,878 | Calculate Loss\n",
      "2022-01-21 21:08:28,880 | Update Loss\n",
      "2022-01-21 21:08:28,884 | Backward\n",
      "2022-01-21 21:08:50,045 | Epoch: [8][39/48]\tTime  32.76 ( 32.76)\tData 0.0135 (0.0445)\tLoss (L1) 10.410 (11.072)\n",
      "2022-01-21 21:08:50,059 | ===> Batch : 40\n",
      "2022-01-21 21:08:50,059 | FDS enable\n",
      "2022-01-21 21:09:01,707 | Calculate Loss\n",
      "2022-01-21 21:09:01,708 | Update Loss\n",
      "2022-01-21 21:09:01,712 | Backward\n",
      "2022-01-21 21:09:22,411 | Epoch: [8][40/48]\tTime  32.37 ( 32.75)\tData 0.0139 (0.0437)\tLoss (L1) 12.139 (11.098)\n",
      "2022-01-21 21:09:22,425 | ===> Batch : 41\n",
      "2022-01-21 21:09:22,426 | FDS enable\n",
      "2022-01-21 21:09:34,033 | Calculate Loss\n",
      "2022-01-21 21:09:34,035 | Update Loss\n",
      "2022-01-21 21:09:34,040 | Backward\n",
      "2022-01-21 21:09:54,843 | Epoch: [8][41/48]\tTime  32.43 ( 32.75)\tData 0.0136 (0.0430)\tLoss (L1) 11.958 (11.119)\n",
      "2022-01-21 21:09:54,857 | ===> Batch : 42\n",
      "2022-01-21 21:09:54,857 | FDS enable\n",
      "2022-01-21 21:10:06,451 | Calculate Loss\n",
      "2022-01-21 21:10:06,452 | Update Loss\n",
      "2022-01-21 21:10:06,457 | Backward\n",
      "2022-01-21 21:10:27,327 | Epoch: [8][42/48]\tTime  32.48 ( 32.74)\tData 0.0144 (0.0423)\tLoss (L1) 10.385 (11.102)\n",
      "2022-01-21 21:10:27,341 | ===> Batch : 43\n",
      "2022-01-21 21:10:27,341 | FDS enable\n",
      "2022-01-21 21:10:38,941 | Calculate Loss\n",
      "2022-01-21 21:10:38,942 | Update Loss\n",
      "2022-01-21 21:10:38,947 | Backward\n",
      "2022-01-21 21:10:59,580 | Epoch: [8][43/48]\tTime  32.25 ( 32.73)\tData 0.0136 (0.0417)\tLoss (L1) 11.564 (11.113)\n",
      "2022-01-21 21:10:59,593 | ===> Batch : 44\n",
      "2022-01-21 21:10:59,594 | FDS enable\n",
      "2022-01-21 21:11:11,290 | Calculate Loss\n",
      "2022-01-21 21:11:11,291 | Update Loss\n",
      "2022-01-21 21:11:11,296 | Backward\n",
      "2022-01-21 21:11:32,619 | Epoch: [8][44/48]\tTime  33.04 ( 32.73)\tData 0.0136 (0.0410)\tLoss (L1) 10.477 (11.098)\n",
      "2022-01-21 21:11:32,634 | ===> Batch : 45\n",
      "2022-01-21 21:11:32,634 | FDS enable\n",
      "2022-01-21 21:11:44,323 | Calculate Loss\n",
      "2022-01-21 21:11:44,324 | Update Loss\n",
      "2022-01-21 21:11:44,329 | Backward\n",
      "2022-01-21 21:12:05,351 | Epoch: [8][45/48]\tTime  32.73 ( 32.73)\tData 0.0142 (0.0404)\tLoss (L1) 12.589 (11.131)\n",
      "2022-01-21 21:12:05,365 | ===> Batch : 46\n",
      "2022-01-21 21:12:05,365 | FDS enable\n",
      "2022-01-21 21:12:17,009 | Calculate Loss\n",
      "2022-01-21 21:12:17,011 | Update Loss\n",
      "2022-01-21 21:12:17,014 | Backward\n",
      "2022-01-21 21:12:38,288 | Epoch: [8][46/48]\tTime  32.94 ( 32.74)\tData 0.0137 (0.0398)\tLoss (L1) 10.722 (11.122)\n",
      "2022-01-21 21:12:38,302 | ===> Batch : 47\n",
      "2022-01-21 21:12:38,302 | FDS enable\n",
      "2022-01-21 21:12:49,899 | Calculate Loss\n",
      "2022-01-21 21:12:49,901 | Update Loss\n",
      "2022-01-21 21:12:49,905 | Backward\n",
      "2022-01-21 21:13:10,672 | Epoch: [8][47/48]\tTime  32.38 ( 32.73)\tData 0.0141 (0.0393)\tLoss (L1) 11.323 (11.127)\n",
      "2022-01-21 21:13:10,686 | ===> Batch : 48\n",
      "2022-01-21 21:13:10,686 | FDS enable\n",
      "2022-01-21 21:13:19,202 | Calculate Loss\n",
      "2022-01-21 21:13:19,203 | Update Loss\n",
      "2022-01-21 21:13:19,207 | Backward\n",
      "2022-01-21 21:13:33,462 | Epoch: [8][48/48]\tTime  22.79 ( 32.52)\tData 0.0136 (0.0388)\tLoss (L1) 9.266 (11.100)\n",
      "2022-01-21 21:13:33,631 | Create Epoch [8] features of all training data...\n",
      "100%|██████████| 48/48 [09:08<00:00, 11.43s/it]\n",
      "2022-01-21 21:22:42,381 | Updated smoothed statistics on Epoch [8]!\n",
      "2022-01-21 21:22:42,455 | Updated running statistics with Epoch [8] features!\n",
      "2022-01-21 21:22:54,382 | Val: [0/9]\tTime 11.915 (11.915)\tLoss (MSE) 229.787 (229.787)\tLoss (L1) 11.920 (11.920)\n",
      "2022-01-21 21:23:03,664 | Val: [1/9]\tTime  9.282 (10.599)\tLoss (MSE) 212.849 (221.318)\tLoss (L1) 11.494 (11.707)\n",
      "2022-01-21 21:23:12,818 | Val: [2/9]\tTime  9.154 (10.117)\tLoss (MSE) 224.899 (222.512)\tLoss (L1) 12.012 (11.808)\n",
      "2022-01-21 21:23:21,981 | Val: [3/9]\tTime  9.163 ( 9.879)\tLoss (MSE) 237.687 (226.306)\tLoss (L1) 11.919 (11.836)\n",
      "2022-01-21 21:23:31,167 | Val: [4/9]\tTime  9.186 ( 9.740)\tLoss (MSE) 283.235 (237.692)\tLoss (L1) 12.897 (12.048)\n",
      "2022-01-21 21:23:40,372 | Val: [5/9]\tTime  9.206 ( 9.651)\tLoss (MSE) 271.377 (243.306)\tLoss (L1) 13.147 (12.231)\n",
      "2022-01-21 21:23:49,570 | Val: [6/9]\tTime  9.198 ( 9.586)\tLoss (MSE) 245.003 (243.548)\tLoss (L1) 12.333 (12.246)\n",
      "2022-01-21 21:23:59,038 | Val: [7/9]\tTime  9.468 ( 9.571)\tLoss (MSE) 250.254 (244.387)\tLoss (L1) 12.109 (12.229)\n",
      "2022-01-21 21:24:02,130 | Val: [8/9]\tTime  3.092 ( 8.852)\tLoss (MSE) 258.268 (244.983)\tLoss (L1) 12.250 (12.230)\n",
      "2022-01-21 21:24:02,262 |  * Overall: MSE 244.983\tL1 12.230\tG-Mean 8.008\n",
      "2022-01-21 21:24:02,262 |  * Many: MSE 181.492\tL1 10.549\tG-Mean 6.940\n",
      "2022-01-21 21:24:02,263 |  * Median: MSE 370.852\tL1 15.495\tG-Mean 10.312\n",
      "2022-01-21 21:24:02,263 |  * Low: MSE 496.540\tL1 19.076\tG-Mean 15.378\n",
      "2022-01-21 21:24:02,268 | Best L1 Loss: 11.524\n",
      "2022-01-21 21:24:03,515 | Epoch #8: Train loss [11.0998]; Val loss: MSE [244.9833], L1 [12.2297], G-Mean [8.0080]\n",
      "2022-01-21 21:24:03,517 | Training...\n",
      "2022-01-21 21:24:03,518 | Load train loader\n",
      "2022-01-21 21:24:04,827 | ===> Batch : 1\n",
      "2022-01-21 21:24:04,829 | FDS enable\n",
      "2022-01-21 21:24:18,977 | Calculate Loss\n",
      "2022-01-21 21:24:18,978 | Update Loss\n",
      "2022-01-21 21:24:19,086 | Backward\n",
      "2022-01-21 21:24:40,586 | Epoch: [9][ 1/48]\tTime  37.07 ( 37.07)\tData 1.3096 (1.3096)\tLoss (L1) 11.103 (11.103)\n",
      "2022-01-21 21:24:40,600 | ===> Batch : 2\n",
      "2022-01-21 21:24:40,600 | FDS enable\n",
      "2022-01-21 21:24:52,309 | Calculate Loss\n",
      "2022-01-21 21:24:52,311 | Update Loss\n",
      "2022-01-21 21:24:52,315 | Backward\n",
      "2022-01-21 21:25:13,259 | Epoch: [9][ 2/48]\tTime  32.67 ( 34.87)\tData 0.0135 (0.6615)\tLoss (L1) 10.253 (10.678)\n",
      "2022-01-21 21:25:13,277 | ===> Batch : 3\n",
      "2022-01-21 21:25:13,277 | FDS enable\n",
      "2022-01-21 21:25:24,985 | Calculate Loss\n",
      "2022-01-21 21:25:24,987 | Update Loss\n",
      "2022-01-21 21:25:24,991 | Backward\n",
      "2022-01-21 21:25:45,959 | Epoch: [9][ 3/48]\tTime  32.70 ( 34.15)\tData 0.0178 (0.4470)\tLoss (L1) 12.252 (11.203)\n",
      "2022-01-21 21:25:45,972 | ===> Batch : 4\n",
      "2022-01-21 21:25:45,972 | FDS enable\n",
      "2022-01-21 21:25:57,789 | Calculate Loss\n",
      "2022-01-21 21:25:57,790 | Update Loss\n",
      "2022-01-21 21:25:57,794 | Backward\n",
      "2022-01-21 21:26:18,872 | Epoch: [9][ 4/48]\tTime  32.91 ( 33.84)\tData 0.0133 (0.3385)\tLoss (L1) 11.819 (11.357)\n",
      "2022-01-21 21:26:18,887 | ===> Batch : 5\n",
      "2022-01-21 21:26:18,887 | FDS enable\n",
      "2022-01-21 21:26:30,601 | Calculate Loss\n",
      "2022-01-21 21:26:30,602 | Update Loss\n",
      "2022-01-21 21:26:30,607 | Backward\n",
      "2022-01-21 21:26:51,732 | Epoch: [9][ 5/48]\tTime  32.86 ( 33.64)\tData 0.0146 (0.2738)\tLoss (L1) 10.098 (11.105)\n",
      "2022-01-21 21:26:51,746 | ===> Batch : 6\n",
      "2022-01-21 21:26:51,746 | FDS enable\n",
      "2022-01-21 21:27:03,461 | Calculate Loss\n",
      "2022-01-21 21:27:03,463 | Update Loss\n",
      "2022-01-21 21:27:03,467 | Backward\n",
      "2022-01-21 21:27:23,690 | Epoch: [9][ 6/48]\tTime  31.96 ( 33.36)\tData 0.0133 (0.2303)\tLoss (L1) 10.425 (10.992)\n",
      "2022-01-21 21:27:23,704 | ===> Batch : 7\n",
      "2022-01-21 21:27:23,705 | FDS enable\n",
      "2022-01-21 21:27:35,557 | Calculate Loss\n",
      "2022-01-21 21:27:35,559 | Update Loss\n",
      "2022-01-21 21:27:35,563 | Backward\n",
      "2022-01-21 21:27:56,287 | Epoch: [9][ 7/48]\tTime  32.60 ( 33.25)\tData 0.0148 (0.1996)\tLoss (L1) 10.963 (10.988)\n",
      "2022-01-21 21:27:56,300 | ===> Batch : 8\n",
      "2022-01-21 21:27:56,301 | FDS enable\n",
      "2022-01-21 21:28:07,996 | Calculate Loss\n",
      "2022-01-21 21:28:07,998 | Update Loss\n",
      "2022-01-21 21:28:08,002 | Backward\n",
      "2022-01-21 21:28:29,142 | Epoch: [9][ 8/48]\tTime  32.85 ( 33.20)\tData 0.0129 (0.1762)\tLoss (L1) 11.326 (11.030)\n",
      "2022-01-21 21:28:29,159 | ===> Batch : 9\n",
      "2022-01-21 21:28:29,160 | FDS enable\n",
      "2022-01-21 21:28:40,861 | Calculate Loss\n",
      "2022-01-21 21:28:40,863 | Update Loss\n",
      "2022-01-21 21:28:40,867 | Backward\n",
      "2022-01-21 21:29:01,878 | Epoch: [9][ 9/48]\tTime  32.74 ( 33.15)\tData 0.0173 (0.1586)\tLoss (L1) 11.968 (11.134)\n",
      "2022-01-21 21:29:01,890 | ===> Batch : 10\n",
      "2022-01-21 21:29:01,891 | FDS enable\n",
      "2022-01-21 21:29:13,576 | Calculate Loss\n",
      "2022-01-21 21:29:13,578 | Update Loss\n",
      "2022-01-21 21:29:13,582 | Backward\n",
      "2022-01-21 21:29:34,691 | Epoch: [9][10/48]\tTime  32.81 ( 33.12)\tData 0.0125 (0.1440)\tLoss (L1) 9.697 (10.990)\n",
      "2022-01-21 21:29:34,707 | ===> Batch : 11\n",
      "2022-01-21 21:29:34,708 | FDS enable\n",
      "2022-01-21 21:29:46,390 | Calculate Loss\n",
      "2022-01-21 21:29:46,392 | Update Loss\n",
      "2022-01-21 21:29:46,396 | Backward\n",
      "2022-01-21 21:30:07,094 | Epoch: [9][11/48]\tTime  32.40 ( 33.05)\tData 0.0158 (0.1323)\tLoss (L1) 12.025 (11.084)\n",
      "2022-01-21 21:30:07,106 | ===> Batch : 12\n",
      "2022-01-21 21:30:07,106 | FDS enable\n",
      "2022-01-21 21:30:18,833 | Calculate Loss\n",
      "2022-01-21 21:30:18,834 | Update Loss\n",
      "2022-01-21 21:30:18,839 | Backward\n",
      "2022-01-21 21:30:39,825 | Epoch: [9][12/48]\tTime  32.73 ( 33.03)\tData 0.0122 (0.1223)\tLoss (L1) 13.118 (11.254)\n",
      "2022-01-21 21:30:39,838 | ===> Batch : 13\n",
      "2022-01-21 21:30:39,838 | FDS enable\n",
      "2022-01-21 21:30:51,482 | Calculate Loss\n",
      "2022-01-21 21:30:51,483 | Update Loss\n",
      "2022-01-21 21:30:51,487 | Backward\n",
      "2022-01-21 21:31:12,727 | Epoch: [9][13/48]\tTime  32.90 ( 33.02)\tData 0.0126 (0.1139)\tLoss (L1) 11.626 (11.283)\n",
      "2022-01-21 21:31:12,739 | ===> Batch : 14\n",
      "2022-01-21 21:31:12,739 | FDS enable\n",
      "2022-01-21 21:31:24,431 | Calculate Loss\n",
      "2022-01-21 21:31:24,432 | Update Loss\n",
      "2022-01-21 21:31:24,437 | Backward\n",
      "2022-01-21 21:31:44,989 | Epoch: [9][14/48]\tTime  32.26 ( 32.96)\tData 0.0122 (0.1066)\tLoss (L1) 10.154 (11.202)\n",
      "2022-01-21 21:31:45,003 | ===> Batch : 15\n",
      "2022-01-21 21:31:45,004 | FDS enable\n",
      "2022-01-21 21:31:56,752 | Calculate Loss\n",
      "2022-01-21 21:31:56,754 | Update Loss\n",
      "2022-01-21 21:31:56,758 | Backward\n",
      "2022-01-21 21:32:17,607 | Epoch: [9][15/48]\tTime  32.62 ( 32.94)\tData 0.0144 (0.1005)\tLoss (L1) 10.081 (11.127)\n",
      "2022-01-21 21:32:17,619 | ===> Batch : 16\n",
      "2022-01-21 21:32:17,620 | FDS enable\n",
      "2022-01-21 21:32:29,419 | Calculate Loss\n",
      "2022-01-21 21:32:29,421 | Update Loss\n",
      "2022-01-21 21:32:29,425 | Backward\n",
      "2022-01-21 21:32:50,471 | Epoch: [9][16/48]\tTime  32.86 ( 32.93)\tData 0.0128 (0.0950)\tLoss (L1) 9.416 (11.020)\n",
      "2022-01-21 21:32:50,486 | ===> Batch : 17\n",
      "2022-01-21 21:32:50,486 | FDS enable\n",
      "2022-01-21 21:33:02,301 | Calculate Loss\n",
      "2022-01-21 21:33:02,302 | Update Loss\n",
      "2022-01-21 21:33:02,307 | Backward\n",
      "2022-01-21 21:33:22,758 | Epoch: [9][17/48]\tTime  32.29 ( 32.90)\tData 0.0143 (0.0902)\tLoss (L1) 12.122 (11.085)\n",
      "2022-01-21 21:33:22,772 | ===> Batch : 18\n",
      "2022-01-21 21:33:22,772 | FDS enable\n",
      "2022-01-21 21:33:34,504 | Calculate Loss\n",
      "2022-01-21 21:33:34,506 | Update Loss\n",
      "2022-01-21 21:33:34,510 | Backward\n",
      "2022-01-21 21:33:55,353 | Epoch: [9][18/48]\tTime  32.59 ( 32.88)\tData 0.0137 (0.0860)\tLoss (L1) 10.388 (11.046)\n",
      "2022-01-21 21:33:55,368 | ===> Batch : 19\n",
      "2022-01-21 21:33:55,369 | FDS enable\n",
      "2022-01-21 21:34:07,061 | Calculate Loss\n",
      "2022-01-21 21:34:07,063 | Update Loss\n",
      "2022-01-21 21:34:07,067 | Backward\n",
      "2022-01-21 21:34:27,971 | Epoch: [9][19/48]\tTime  32.62 ( 32.87)\tData 0.0153 (0.0823)\tLoss (L1) 11.031 (11.046)\n",
      "2022-01-21 21:34:27,987 | ===> Batch : 20\n",
      "2022-01-21 21:34:27,987 | FDS enable\n",
      "2022-01-21 21:34:39,786 | Calculate Loss\n",
      "2022-01-21 21:34:39,788 | Update Loss\n",
      "2022-01-21 21:34:39,792 | Backward\n",
      "2022-01-21 21:35:00,692 | Epoch: [9][20/48]\tTime  32.72 ( 32.86)\tData 0.0154 (0.0789)\tLoss (L1) 10.889 (11.038)\n",
      "2022-01-21 21:35:00,707 | ===> Batch : 21\n",
      "2022-01-21 21:35:00,707 | FDS enable\n",
      "2022-01-21 21:35:12,406 | Calculate Loss\n",
      "2022-01-21 21:35:12,407 | Update Loss\n",
      "2022-01-21 21:35:12,411 | Backward\n",
      "2022-01-21 21:35:33,062 | Epoch: [9][21/48]\tTime  32.37 ( 32.84)\tData 0.0149 (0.0759)\tLoss (L1) 10.210 (10.998)\n",
      "2022-01-21 21:35:33,083 | ===> Batch : 22\n",
      "2022-01-21 21:35:33,084 | FDS enable\n",
      "2022-01-21 21:35:44,813 | Calculate Loss\n",
      "2022-01-21 21:35:44,814 | Update Loss\n",
      "2022-01-21 21:35:44,819 | Backward\n",
      "2022-01-21 21:36:05,627 | Epoch: [9][22/48]\tTime  32.57 ( 32.82)\tData 0.0211 (0.0734)\tLoss (L1) 9.293 (10.921)\n",
      "2022-01-21 21:36:05,643 | ===> Batch : 23\n",
      "2022-01-21 21:36:05,644 | FDS enable\n",
      "2022-01-21 21:36:17,425 | Calculate Loss\n",
      "2022-01-21 21:36:17,427 | Update Loss\n",
      "2022-01-21 21:36:17,431 | Backward\n",
      "2022-01-21 21:36:38,253 | Epoch: [9][23/48]\tTime  32.63 ( 32.81)\tData 0.0160 (0.0709)\tLoss (L1) 13.294 (11.024)\n",
      "2022-01-21 21:36:38,268 | ===> Batch : 24\n",
      "2022-01-21 21:36:38,268 | FDS enable\n",
      "2022-01-21 21:36:49,983 | Calculate Loss\n",
      "2022-01-21 21:36:49,985 | Update Loss\n",
      "2022-01-21 21:36:49,990 | Backward\n",
      "2022-01-21 21:37:10,752 | Epoch: [9][24/48]\tTime  32.50 ( 32.80)\tData 0.0147 (0.0685)\tLoss (L1) 8.915 (10.936)\n",
      "2022-01-21 21:37:10,767 | ===> Batch : 25\n",
      "2022-01-21 21:37:10,768 | FDS enable\n",
      "2022-01-21 21:37:22,468 | Calculate Loss\n",
      "2022-01-21 21:37:22,470 | Update Loss\n",
      "2022-01-21 21:37:22,474 | Backward\n",
      "2022-01-21 21:37:43,119 | Epoch: [9][25/48]\tTime  32.37 ( 32.78)\tData 0.0148 (0.0664)\tLoss (L1) 11.636 (10.964)\n",
      "2022-01-21 21:37:43,134 | ===> Batch : 26\n",
      "2022-01-21 21:37:43,134 | FDS enable\n",
      "2022-01-21 21:37:54,818 | Calculate Loss\n",
      "2022-01-21 21:37:54,820 | Update Loss\n",
      "2022-01-21 21:37:54,824 | Backward\n",
      "2022-01-21 21:38:15,978 | Epoch: [9][26/48]\tTime  32.86 ( 32.79)\tData 0.0143 (0.0644)\tLoss (L1) 9.906 (10.923)\n",
      "2022-01-21 21:38:15,992 | ===> Batch : 27\n",
      "2022-01-21 21:38:15,993 | FDS enable\n",
      "2022-01-21 21:38:27,708 | Calculate Loss\n",
      "2022-01-21 21:38:27,710 | Update Loss\n",
      "2022-01-21 21:38:27,714 | Backward\n",
      "2022-01-21 21:38:48,397 | Epoch: [9][27/48]\tTime  32.42 ( 32.77)\tData 0.0140 (0.0625)\tLoss (L1) 11.606 (10.949)\n",
      "2022-01-21 21:38:48,411 | ===> Batch : 28\n",
      "2022-01-21 21:38:48,412 | FDS enable\n",
      "2022-01-21 21:39:00,117 | Calculate Loss\n",
      "2022-01-21 21:39:00,118 | Update Loss\n",
      "2022-01-21 21:39:00,123 | Backward\n",
      "2022-01-21 21:39:21,311 | Epoch: [9][28/48]\tTime  32.91 ( 32.78)\tData 0.0141 (0.0608)\tLoss (L1) 13.384 (11.036)\n",
      "2022-01-21 21:39:21,325 | ===> Batch : 29\n",
      "2022-01-21 21:39:21,325 | FDS enable\n",
      "2022-01-21 21:39:33,052 | Calculate Loss\n",
      "2022-01-21 21:39:33,054 | Update Loss\n",
      "2022-01-21 21:39:33,058 | Backward\n",
      "2022-01-21 21:39:53,376 | Epoch: [9][29/48]\tTime  32.07 ( 32.75)\tData 0.0136 (0.0592)\tLoss (L1) 12.391 (11.082)\n",
      "2022-01-21 21:39:53,392 | ===> Batch : 30\n",
      "2022-01-21 21:39:53,393 | FDS enable\n",
      "2022-01-21 21:40:05,158 | Calculate Loss\n",
      "2022-01-21 21:40:05,160 | Update Loss\n",
      "2022-01-21 21:40:05,164 | Backward\n",
      "2022-01-21 21:40:26,378 | Epoch: [9][30/48]\tTime  33.00 ( 32.76)\tData 0.0159 (0.0577)\tLoss (L1) 10.024 (11.047)\n",
      "2022-01-21 21:40:26,393 | ===> Batch : 31\n",
      "2022-01-21 21:40:26,393 | FDS enable\n",
      "2022-01-21 21:40:38,162 | Calculate Loss\n",
      "2022-01-21 21:40:38,163 | Update Loss\n",
      "2022-01-21 21:40:38,168 | Backward\n",
      "2022-01-21 21:40:59,317 | Epoch: [9][31/48]\tTime  32.94 ( 32.77)\tData 0.0152 (0.0564)\tLoss (L1) 11.480 (11.061)\n",
      "2022-01-21 21:40:59,331 | ===> Batch : 32\n",
      "2022-01-21 21:40:59,331 | FDS enable\n",
      "2022-01-21 21:41:10,999 | Calculate Loss\n",
      "2022-01-21 21:41:11,000 | Update Loss\n",
      "2022-01-21 21:41:11,005 | Backward\n",
      "2022-01-21 21:41:31,784 | Epoch: [9][32/48]\tTime  32.47 ( 32.76)\tData 0.0138 (0.0550)\tLoss (L1) 8.906 (10.994)\n",
      "2022-01-21 21:41:31,798 | ===> Batch : 33\n",
      "2022-01-21 21:41:31,798 | FDS enable\n",
      "2022-01-21 21:41:43,386 | Calculate Loss\n",
      "2022-01-21 21:41:43,388 | Update Loss\n",
      "2022-01-21 21:41:43,392 | Backward\n",
      "2022-01-21 21:42:04,452 | Epoch: [9][33/48]\tTime  32.67 ( 32.76)\tData 0.0138 (0.0538)\tLoss (L1) 13.493 (11.070)\n",
      "2022-01-21 21:42:04,466 | ===> Batch : 34\n",
      "2022-01-21 21:42:04,466 | FDS enable\n",
      "2022-01-21 21:42:16,079 | Calculate Loss\n",
      "2022-01-21 21:42:16,081 | Update Loss\n",
      "2022-01-21 21:42:16,085 | Backward\n",
      "2022-01-21 21:42:36,602 | Epoch: [9][34/48]\tTime  32.15 ( 32.74)\tData 0.0139 (0.0526)\tLoss (L1) 11.991 (11.097)\n",
      "2022-01-21 21:42:36,616 | ===> Batch : 35\n",
      "2022-01-21 21:42:36,616 | FDS enable\n",
      "2022-01-21 21:42:48,176 | Calculate Loss\n",
      "2022-01-21 21:42:48,178 | Update Loss\n",
      "2022-01-21 21:42:48,182 | Backward\n",
      "2022-01-21 21:43:09,193 | Epoch: [9][35/48]\tTime  32.59 ( 32.73)\tData 0.0135 (0.0515)\tLoss (L1) 11.097 (11.097)\n",
      "2022-01-21 21:43:09,208 | ===> Batch : 36\n",
      "2022-01-21 21:43:09,208 | FDS enable\n",
      "2022-01-21 21:43:20,821 | Calculate Loss\n",
      "2022-01-21 21:43:20,822 | Update Loss\n",
      "2022-01-21 21:43:20,827 | Backward\n",
      "2022-01-21 21:43:41,860 | Epoch: [9][36/48]\tTime  32.67 ( 32.73)\tData 0.0147 (0.0505)\tLoss (L1) 12.214 (11.128)\n",
      "2022-01-21 21:43:41,873 | ===> Batch : 37\n",
      "2022-01-21 21:43:41,874 | FDS enable\n",
      "2022-01-21 21:43:53,468 | Calculate Loss\n",
      "2022-01-21 21:43:53,470 | Update Loss\n",
      "2022-01-21 21:43:53,474 | Backward\n",
      "2022-01-21 21:44:13,837 | Epoch: [9][37/48]\tTime  31.98 ( 32.71)\tData 0.0139 (0.0495)\tLoss (L1) 9.801 (11.092)\n",
      "2022-01-21 21:44:13,851 | ===> Batch : 38\n",
      "2022-01-21 21:44:13,852 | FDS enable\n",
      "2022-01-21 21:44:25,422 | Calculate Loss\n",
      "2022-01-21 21:44:25,423 | Update Loss\n",
      "2022-01-21 21:44:25,428 | Backward\n",
      "2022-01-21 21:44:46,646 | Epoch: [9][38/48]\tTime  32.81 ( 32.71)\tData 0.0144 (0.0486)\tLoss (L1) 10.526 (11.077)\n",
      "2022-01-21 21:44:46,659 | ===> Batch : 39\n",
      "2022-01-21 21:44:46,660 | FDS enable\n",
      "2022-01-21 21:44:58,268 | Calculate Loss\n",
      "2022-01-21 21:44:58,270 | Update Loss\n",
      "2022-01-21 21:44:58,274 | Backward\n",
      "2022-01-21 21:45:19,010 | Epoch: [9][39/48]\tTime  32.36 ( 32.70)\tData 0.0135 (0.0477)\tLoss (L1) 12.748 (11.120)\n",
      "2022-01-21 21:45:19,024 | ===> Batch : 40\n",
      "2022-01-21 21:45:19,025 | FDS enable\n",
      "2022-01-21 21:45:30,587 | Calculate Loss\n",
      "2022-01-21 21:45:30,589 | Update Loss\n",
      "2022-01-21 21:45:30,593 | Backward\n",
      "2022-01-21 21:45:51,497 | Epoch: [9][40/48]\tTime  32.49 ( 32.70)\tData 0.0140 (0.0468)\tLoss (L1) 10.761 (11.111)\n",
      "2022-01-21 21:45:51,512 | ===> Batch : 41\n",
      "2022-01-21 21:45:51,512 | FDS enable\n",
      "2022-01-21 21:46:03,182 | Calculate Loss\n",
      "2022-01-21 21:46:03,184 | Update Loss\n",
      "2022-01-21 21:46:03,188 | Backward\n",
      "2022-01-21 21:46:24,439 | Epoch: [9][41/48]\tTime  32.94 ( 32.71)\tData 0.0150 (0.0460)\tLoss (L1) 12.309 (11.140)\n",
      "2022-01-21 21:46:24,453 | ===> Batch : 42\n",
      "2022-01-21 21:46:24,453 | FDS enable\n",
      "2022-01-21 21:46:36,101 | Calculate Loss\n",
      "2022-01-21 21:46:36,103 | Update Loss\n",
      "2022-01-21 21:46:36,107 | Backward\n",
      "2022-01-21 21:46:56,943 | Epoch: [9][42/48]\tTime  32.50 ( 32.70)\tData 0.0135 (0.0453)\tLoss (L1) 13.231 (11.190)\n",
      "2022-01-21 21:46:56,956 | ===> Batch : 43\n",
      "2022-01-21 21:46:56,957 | FDS enable\n",
      "2022-01-21 21:47:08,528 | Calculate Loss\n",
      "2022-01-21 21:47:08,530 | Update Loss\n",
      "2022-01-21 21:47:08,534 | Backward\n",
      "2022-01-21 21:47:29,425 | Epoch: [9][43/48]\tTime  32.48 ( 32.70)\tData 0.0136 (0.0445)\tLoss (L1) 9.725 (11.156)\n",
      "2022-01-21 21:47:29,439 | ===> Batch : 44\n",
      "2022-01-21 21:47:29,439 | FDS enable\n",
      "2022-01-21 21:47:41,050 | Calculate Loss\n",
      "2022-01-21 21:47:41,052 | Update Loss\n",
      "2022-01-21 21:47:41,056 | Backward\n",
      "2022-01-21 21:48:01,956 | Epoch: [9][44/48]\tTime  32.53 ( 32.69)\tData 0.0138 (0.0438)\tLoss (L1) 9.659 (11.122)\n",
      "2022-01-21 21:48:01,970 | ===> Batch : 45\n",
      "2022-01-21 21:48:01,970 | FDS enable\n",
      "2022-01-21 21:48:13,572 | Calculate Loss\n",
      "2022-01-21 21:48:13,574 | Update Loss\n",
      "2022-01-21 21:48:13,578 | Backward\n",
      "2022-01-21 21:48:34,132 | Epoch: [9][45/48]\tTime  32.18 ( 32.68)\tData 0.0138 (0.0432)\tLoss (L1) 10.878 (11.116)\n",
      "2022-01-21 21:48:34,146 | ===> Batch : 46\n",
      "2022-01-21 21:48:34,146 | FDS enable\n",
      "2022-01-21 21:48:45,744 | Calculate Loss\n",
      "2022-01-21 21:48:45,745 | Update Loss\n",
      "2022-01-21 21:48:45,750 | Backward\n",
      "2022-01-21 21:49:06,740 | Epoch: [9][46/48]\tTime  32.61 ( 32.68)\tData 0.0136 (0.0425)\tLoss (L1) 10.107 (11.094)\n",
      "2022-01-21 21:49:06,753 | ===> Batch : 47\n",
      "2022-01-21 21:49:06,754 | FDS enable\n",
      "2022-01-21 21:49:18,329 | Calculate Loss\n",
      "2022-01-21 21:49:18,330 | Update Loss\n",
      "2022-01-21 21:49:18,334 | Backward\n",
      "2022-01-21 21:49:39,053 | Epoch: [9][47/48]\tTime  32.31 ( 32.67)\tData 0.0137 (0.0419)\tLoss (L1) 13.074 (11.137)\n",
      "2022-01-21 21:49:39,067 | ===> Batch : 48\n",
      "2022-01-21 21:49:39,068 | FDS enable\n",
      "2022-01-21 21:49:47,504 | Calculate Loss\n",
      "2022-01-21 21:49:47,506 | Update Loss\n",
      "2022-01-21 21:49:47,511 | Backward\n",
      "2022-01-21 21:50:01,859 | Epoch: [9][48/48]\tTime  22.81 ( 32.47)\tData 0.0147 (0.0413)\tLoss (L1) 10.988 (11.134)\n",
      "2022-01-21 21:50:02,008 | Create Epoch [9] features of all training data...\n",
      "100%|██████████| 48/48 [09:11<00:00, 11.48s/it]\n",
      "2022-01-21 21:59:13,171 | Updated smoothed statistics on Epoch [9]!\n",
      "2022-01-21 21:59:13,254 | Updated running statistics with Epoch [9] features!\n",
      "2022-01-21 21:59:25,167 | Val: [0/9]\tTime 11.900 (11.900)\tLoss (MSE) 229.700 (229.700)\tLoss (L1) 12.085 (12.085)\n",
      "2022-01-21 21:59:34,569 | Val: [1/9]\tTime  9.402 (10.651)\tLoss (MSE) 196.788 (213.244)\tLoss (L1) 10.895 (11.490)\n",
      "2022-01-21 21:59:44,081 | Val: [2/9]\tTime  9.512 (10.271)\tLoss (MSE) 215.786 (214.091)\tLoss (L1) 11.645 (11.542)\n",
      "2022-01-21 21:59:53,497 | Val: [3/9]\tTime  9.416 (10.058)\tLoss (MSE) 214.920 (214.299)\tLoss (L1) 11.712 (11.584)\n",
      "2022-01-21 22:00:03,049 | Val: [4/9]\tTime  9.552 ( 9.956)\tLoss (MSE) 229.575 (217.354)\tLoss (L1) 11.941 (11.655)\n",
      "2022-01-21 22:00:12,609 | Val: [5/9]\tTime  9.560 ( 9.890)\tLoss (MSE) 236.926 (220.616)\tLoss (L1) 12.286 (11.760)\n",
      "2022-01-21 22:00:22,029 | Val: [6/9]\tTime  9.419 ( 9.823)\tLoss (MSE) 202.627 (218.046)\tLoss (L1) 11.334 (11.700)\n",
      "2022-01-21 22:00:31,495 | Val: [7/9]\tTime  9.466 ( 9.779)\tLoss (MSE) 250.914 (222.154)\tLoss (L1) 12.166 (11.758)\n",
      "2022-01-21 22:00:34,892 | Val: [8/9]\tTime  3.397 ( 9.069)\tLoss (MSE) 213.622 (221.788)\tLoss (L1) 11.312 (11.739)\n",
      "2022-01-21 22:00:35,030 |  * Overall: MSE 221.788\tL1 11.739\tG-Mean 7.651\n",
      "2022-01-21 22:00:35,031 |  * Many: MSE 162.095\tL1 9.953\tG-Mean 6.371\n",
      "2022-01-21 22:00:35,031 |  * Median: MSE 340.495\tL1 15.301\tG-Mean 11.049\n",
      "2022-01-21 22:00:35,032 |  * Low: MSE 457.274\tL1 18.753\tG-Mean 15.597\n",
      "2022-01-21 22:00:35,037 | Best L1 Loss: 11.524\n",
      "2022-01-21 22:00:36,307 | Epoch #9: Train loss [11.1344]; Val loss: MSE [221.7877], L1 [11.7387], G-Mean [7.6507]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "        tb_logger.log_value('train_loss', train_loss, epoch)\n",
    "        tb_logger.log_value('val_loss_mse', val_loss_mse, epoch)\n",
    "        tb_logger.log_value('val_loss_l1', val_loss_l1, epoch)\n",
    "        tb_logger.log_value('val_loss_gmean', val_loss_gmean, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"checkpoints/agedb_resnet50_fds_model_sqrt_inv_fds_gau_5_2_0_1_0.9_adam_l1_0.001_256/ckpt.best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 23:07:54,457 | ========================================================================================================================\n",
      "2022-01-21 23:07:54,457 | Test best model on testset...\n",
      "2022-01-21 23:07:54,484 | Loaded best model, epoch 8, best val loss 11.5237\n",
      "2022-01-21 23:08:06,563 | Test: [0/9]\tTime 12.078 (12.078)\tLoss (MSE) 234.452 (234.452)\tLoss (L1) 12.221 (12.221)\n",
      "2022-01-21 23:08:16,005 | Test: [1/9]\tTime  9.442 (10.760)\tLoss (MSE) 174.029 (204.240)\tLoss (L1) 10.278 (11.250)\n",
      "2022-01-21 23:08:25,428 | Test: [2/9]\tTime  9.423 (10.314)\tLoss (MSE) 202.547 (203.676)\tLoss (L1) 11.289 (11.263)\n",
      "2022-01-21 23:08:34,861 | Test: [3/9]\tTime  9.432 (10.094)\tLoss (MSE) 192.346 (200.843)\tLoss (L1) 11.133 (11.230)\n",
      "2022-01-21 23:08:44,292 | Test: [4/9]\tTime  9.431 ( 9.961)\tLoss (MSE) 222.040 (205.083)\tLoss (L1) 11.689 (11.322)\n",
      "2022-01-21 23:08:53,730 | Test: [5/9]\tTime  9.438 ( 9.874)\tLoss (MSE) 186.528 (201.990)\tLoss (L1) 10.556 (11.195)\n",
      "2022-01-21 23:09:03,147 | Test: [6/9]\tTime  9.417 ( 9.809)\tLoss (MSE) 190.943 (200.412)\tLoss (L1) 11.179 (11.192)\n",
      "2022-01-21 23:09:12,611 | Test: [7/9]\tTime  9.464 ( 9.766)\tLoss (MSE) 190.346 (199.154)\tLoss (L1) 11.205 (11.194)\n",
      "2022-01-21 23:09:15,990 | Test: [8/9]\tTime  3.379 ( 9.056)\tLoss (MSE) 240.586 (200.935)\tLoss (L1) 11.940 (11.226)\n",
      "2022-01-21 23:09:16,114 |  * Overall: MSE 200.935\tL1 11.226\tG-Mean 7.362\n",
      "2022-01-21 23:09:16,114 |  * Many: MSE 145.972\tL1 9.424\tG-Mean 6.048\n",
      "2022-01-21 23:09:16,115 |  * Median: MSE 289.107\tL1 14.362\tG-Mean 10.768\n",
      "2022-01-21 23:09:16,115 |  * Low: MSE 476.195\tL1 19.569\tG-Mean 16.486\n",
      "2022-01-21 23:09:16,122 | Test loss: MSE [200.9352], L1 [11.2259], G-Mean [7.3622]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# test with best checkpoint\n",
    "print(\"=\" * 120)\n",
    "print(\"Test best model on testset...\")\n",
    "#checkpoint = torch.load(f\"{args.store_root}/{args.store_name}/ckpt.best.pth.tar\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEaCAYAAAD3+OukAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABb/klEQVR4nO2deXxU5b3/P89kklkzM5lMWJIAEUQUXKgLuFRBQKy11dpaWlt7a1uvP4pL6b316o9q/XXnVrliW7e27rbVelutinahCJbigiDKoojIFghJJskkk8yWmXl+fzznnDlnlmTWM2Hm+369eCWcOXPOk5PJ+ZzvzjjnHARBEERVYij3AgiCIIjyQSJAEARRxZAIEARBVDEkAgRBEFUMiQBBEEQVQyJAEARRxZAIEMc0+/fvB2MMGzduVLYxxvDkk0+O+L758+fj2muvLfj8jz76KIxGY8HHIYhyQSJAlIXLL78cc+bMSftaKBSC2+3GbbfdltexOzo6cOWVVxayvBTa29vBGMP69es127/whS/g8OHDRT1XJoolXAShhkSAKAvXXXcdNm/ejHfeeSfltT/+8Y/o7+/P+4Y3YcIEmM3mQpeYFRaLBePHj9flXARRCkgEiLJwySWXYPLkyfj1r3+d8tqvf/1rLF68GG1tbbjnnnswe/Zs2O12TJgwAV/84hfR0dEx4rGT3UEHDhzAJz7xCVgsFkyaNAm/+MUvUt7zu9/9DnPnzoXT6YTH48Gll16KDz74QHl90qRJAIALL7wQjDG0tbUBSO8Oeumll3DGGWfAZDJh3LhxWLZsGYaGhpTXr7nmGixatAi/+tWvMGXKFDgcDlx22WXo7Owc/cKNwOuvv44LLrgAFosFDQ0N+NKXvoSuri7l9fb2dnzuc5+Dx+OB2WzG1KlTceeddyqv//nPf8bHPvYxWK1WuFwuzJkzB2+//XZBayLGPiQCRFkwGAz4xje+gd/+9rcIBoPK9j179mDDhg247rrrlG133XUXtm/fjmeffRYHDx7EF7/4xazPwznHFVdcgZ6eHqxfvx4vvPACnn/+eWzdulWzXzgcxm233YatW7fi73//O2pqanDppZciEokAgLL/H//4R3R0dGDz5s1pz/fuu+/isssuwwUXXIB33nkHjz32GF588UUsXbpUs9/mzZvxyiuvYM2aNfjrX/+K7du34zvf+U7WP1cyR48exeLFi9Ha2oo333wTL7zwAnbs2KFxiy1btgz9/f1Yu3Yt3n//fTz00ENobW1V3v/5z38eV111FXbu3InXXnsNy5cvp3hHNcAJoky0t7fzmpoa/thjjynb/uu//otPnDiRDw8Pp33P1q1bOQDe3t7OOed83759HAD/5z//qewDgD/xxBOcc87//ve/cwB89+7dyutdXV3cbDbzb3zjGxnX1tPTwwHwjRs3cs45P3ToEAfAX3nlFc1+jzzyCK+pqVH+f/XVV/OzzjpLs89zzz3HGWN8//79nHPOv/rVr/KmpiYeCoWUfVauXMknTJiQcT2ccz5v3ryMa77tttt4S0sLD4fDyrZt27ZxAHzDhg2cc85PPfVUfscdd6R9v3xd9+3bN+IaiMqDLAGibLS0tODSSy9VXELDw8N49NFH8fWvf115Al2/fj0uvvhiTJo0CfX19fj4xz8OQLh4smHXrl3weDw44YQTlG1NTU2YMWOGZr9t27bhiiuuwHHHHYf6+npMnjw5p/PI7Ny5ExdccIFm27x588A5x65du5RtJ554Ikwmk/L/5ubmgtxBO3fuxNlnn426ujpl22mnnQan04mdO3cCAJYvX46f/OQnmDt3Lm655Ra8+uqryr6nnnoqLr74Ypx88sm44oorcM899+DQoUN5r4c4diARIMrKddddh40bN+K9997D888/D6/XqwSEDx48iE9+8pNoa2vDU089hbfeegvPP/88AChummIQCASwePFiMMbwyCOP4M0338TmzZvBGCvqedSob9aAiGPwEjf0/drXvoYDBw5g6dKl6OjowCWXXIKrr74aAFBTU4OXX34Z69atw1lnnYU//vGPOOGEE/Diiy+WdE1E+SERIMqKOkD8m9/8RgkIA8JvHgwGsXr1apx33nmYMWNGzk/LM2fOhNfrxZ49e5RtXq8Xu3fvVv7/3nvvobu7Gz/+8Y8xf/58nHTSSejr69PclOWbdiwWG/F8s2bN0jxhA8CGDRvAGMOsWbNyWnsuzJo1C6+//rpGtN555x309/fj5JNPVrZNnDgRX/va1/D444/joYcewm9/+1sMDAwAEEI0Z84crFixAq+++irmzZuHRx55pGRrJsYGJAJEWZEDxA8//DD+9re/aQLC06dPB2MMq1atwr59+/Dcc8/hBz/4QU7HX7hwIU477TRcffXVePPNN7Ft2zZ8+ctfRm1trbLPlClTYDKZ8Itf/AJ79+7FP/7xD3zrW98CY0zZx+PxwG63429/+xuOHj2Kvr6+tOe7+eabsXXrVnz729/G+++/j7/85S+48cYb8eUvf1lxMRVCb28vtm3bpvn30Ucf4YYbbsDAwACuueYa7NixAxs3bsRXvvIVnH/++Tj//PMBADfccANeeukl7N27Fzt37sSf/vQnxc22adMm/PCHP8Qbb7yBgwcP4h//+AfeffddzJw5s+A1E2OcMsckCEIJEKcLCP/yl7/kra2t3Gw28/POO4+//PLLmgDtaIFheZ+LLrqIm0wm3tLSwlevXp0SZH3mmWf48ccfz00mE589ezZfv349r6mp4Y888oiyz2OPPcbb2tp4TU0NnzJlCuc8NTDMOedr1qzhp59+Oq+rq+Mej4cvXbqUDw4OKq9/9atf5QsXLtS854knnuCj/TnOmzePA0j5d/HFF3POOX/ttdf4+eefz81mM3c6nfyqq67inZ2dyvuXLVvGp0+fzs1mM3e73fyTn/wk37FjB+ec8x07dvBLLrmEjx8/ntfV1fHJkyfz73znO5pAM1GZMM5pshhBEES1Qu4ggiCIKoZEgCAIooohESAIgqhiSAQIgiCqGBIBgiCIKuaY7A515MiRrPf1eDzwer0lXM2xBV2PVOiaaKHroaVSrkdzc3Pa7WQJEARBVDEkAgRBEFUMiQBBEEQVQyJAEARRxZAIEARBVDEkAgRBEFUMiQBBEEQVQyIwholvWgceCpR7GQRBVDAkAmMU3n0U/JHV4P9aV+6lEARRwZAIjFUCg+LrkdwGnRMEQeQCicBYJSjcQPwwiQBBEKWDRGCsIscCjhwEDX8jCKJUkAiUGR4YBI9G02yXRCAYAPqO/eZVBEGMTUgEykz8/90EvvbPqS+os4KOHNRvQQRBVBUkAmWEx2PiKb+7M/XFYEIE+GESAYIgSsMxOU+gYoiExdfgUOprwQBQWwdY7QAFhwmCKBFkCZSTsBABLqeDqgkGAIsVaJkMTu4ggiBKBIlAOQmHxNdAGksgFADMVrDmKUDHQfB4XN+1EQRRFZAIlJOIJAJp3EFcZQkgEgG8aeIGBEEQBUIiUE4kd1BaS0ASAdYyRfyfKocJgigBJALlRHYHBdM0iQsOAWYrMHESAMoQIgiiNOiSHXTfffdh69atcDqdWLVqFQDgD3/4A/7xj3/A4XAAAK666iqcfvrpeixn7CC7g4Yj4MMRsNq6xGuhIJjFCma2AI3jKEOIIIiSoIsIzJ8/H5/4xCdw7733arZfeumluOyyy/RYQsnh+/cAvl6w2XOzf4/sDgLEk79aBIJDIiYAAC1TKEOIIIiSoIs7aObMmbDb7Xqcqmzwvz2H+NO/ye1NEZUIqOICnHMgGFREgLVMBo4eTttegiAIohDKWiz217/+Fa+++iqmTp2Kf/u3f8soFGvXrsXatWsBACtXroTH48n6HEajMaf986UvFsVwJJzTuQLGGvil7111taiV3hsPBtDN47A1NsHm8SA442QMvPxHNAwHYZxwXEHr1Ot6HEvQNdFC10NLpV+PsonA4sWLceWVVwIAnn76aTz++ONYtmxZ2n0XLVqERYsWKf/3erNvqObxeHLaP19iA/1AKJDTueK9Pcr3viOHwdzjAQDcJ7YPcY6g1wvucAMAene8A4OlvqB16nU9jiXommih66GlUq5Hc3Nz2u1lyw5yuVwwGAwwGAxYuHAh9u7dW66lFIdQEIhEcivqUrmDuLpWQM4WMksxgQktgMFAaaIEQRSdsolAX1+f8v2bb76JSZMmlWspxSEcFF/Vfv5R3xNKfB9IFQFmsYmvtXXAuGYaMEMQRNHRxR20evVq7Nq1C36/H0uXLsWSJUuwc+dO7N+/H4wxNDU14brrrtNjKWmJv/4KEI/DcO7C/A8SkkQgHALMluzeEwmLfUNBbdWwbAlYVMeZ2Ap0tOe/PoIgiDToIgLLly9P2bZgwQI9Tp0VfN0acUMulghkSzgM1DuB4YjWEpBnCUiWAACwpong27eAx+NgBqrxIwiiONDdBAAGfED30bzHOPJYTNzIgUQBWDbvi4QAk1nc7FWWAJcFQY4JAEDTBCA6DPT3gSAIoliQCACAv19YAgO+/N4vxwMAIJSLJSCJgNWWZAlIx7MkRIA1TRDfdHfkt0aCIIg0VL0I8HAoEcztPprfQUIqEcjBEkAkDNSZAIst8fQPJKwCszmxTRIBnm4KGUEQRJ5UvQjA3698y71FEIFcLYE6k7AENIHhIGC2gBlqEtvcTSJNlCwBgiCKCImASgTSzvrNBpUI8FwsgXAITI4JJFsC6ngAAGY0CiEgS4AgiCJCM4Y1IlAESyCcQ51AJAyYzGB1XFMsxkMBTTxAoWkCOFkCBEEUkaq3BLgsAu4m8KKIQDDzfsnIMYHkwHAwvQiwpgn5CxVBEEQaql4EMCBEgE2dAeQZE+B5WAKc80R2kMUGRMLg0WHxYgYRQNMEYHBAjJ4kCIIoAiQCg/3iabxlCuDrBc+l7YOMukAs22KxaBSIxxOWAJCoFA4FwcwZLAGArAGCIIoGicBAv6jalW+wPV25H0O2BOrqshcBOYAs1wkACZeQeqCMGnmN+WYxEQRBJFH1IsD9PqDeCeYRbZzzesoOBQFmAOzO7EVA3q/OBGaR5igoIhBMLwIeuVaARIAgiOJQ9SIA/4DGEki+wcbX/AF8x5aRjxEWef0wmbNPEZVjB2pLIDgIHo9Jx0vjDrLaAHs9uYMIgigaJAL+frB6pxACk1lzg+VDfvA//xbxv/xp5GOEAuK9JnPO7iBmMiWe+oOBhGvJmsYSAADPBLIECIIoGlUtApxzQHYHMQZ4xoN7VcVY770DcA58tBtcbhCX7jihhCWQuzsoYQnwwFDqQJkk9EgT5bEY+MGP8m6oRxDEsUNViwBCQZGl43CK/yfdYPmubeKb4Qjw0QeZjyPPEMjJEkjjDlKJAFO1kdbQNBHo7RadS0vFO28i/sPl4Bv+UrpzEAQxJqhuEfD7xFe7EAHWNAHwipbSnHMhAjNOARgD3/1u5uNIlgAzmbOvGFZiAibAZBGBZbUlYMkwmKZpPBCLAb3d2Z0nD7jUTZU/9WvwD3eV7DwEQZSf6hYBuVBMbQlEIqJnf1cH0NMFduZ5wKSp4Lt3ZD5OHu4grs4OYkxqIjeYdqCMGtY0UXxTSpeQvAaXG/H7V4L39ZTuXARBlJXqFoFBqWVEvQsAwDyJPHzZFcRmzgY78RTgo/czF5KFgmA5u4NUMQFAaR2RdqCMmiaRylrS4HBQpLwabvweEA4jfv9PR4yJEARx7FLVIsAHZBFwiK/KDbZTiEDjOKBpItgJp4jYwd730x8oFExkB+WTIgokZgqkGSijwdUIGI2ltwQsFrCWyTB8fTmw7wPwvz1XuvMRBFE2qloElA6i9ZI7qHEcwBjQeRjY/S7YrI8JV830mQAzgH+QwSUku4PqTEAslugBNBKyxWAyia8Wq6gUlruJZhABZjCILKaSWgIBxRJhp58DeMYDHYdKdz6CIMoGiYDFClZbBwDiq6sRfPNGIBgAmzlbbLfagCnTwN/fnnIIHo2K2b+yOwjIziUUCQHG2sTgGKtN3HyDQTE8ps6U+b1NE0vaOiKllXW9E9w/ULLzEQRRPkgE7A7ttqbxQNcRYRGceKqymc04WbhFkrN/5Bt+ziIQTuwPSWgCQ8pAGcZYxrcyz3ig+2jp8vhly0bG7khkUhEEUVFUtQhwfz/gcGm2KZ0626aD2eoT22ecCsSiwN73tAeRffgmtQhkkSYaDidcQQBgsQsRyDRQRs24CcJqGPSPfp58SGplzRxO0V6DIIiKo6pFIK0lIGUIsZNma7dPPwkwGMB3J7mEZBEwW0WdAJDdYBl5vrCM1QaEg+CD/lFFgI1vBQDEf7Qc8acfAv/wPfB4fPRzZksooC1WszuBwX6qICaICqTqRYAlWQIY3wIAYLM+ptnMzFagbXoaEZAqfM3mnCwBHgkn0kOBRNVwn3d0S2DWbLBv/IeoX1i/BvH/vgXxn3wH/OBHo543K4JJ7iCHU2RH0TAbgqg4qlYEeDwuLAE5M0iCfexsGL51h8gISoJNnwXs36PN/gmncwdlaQlo3EGSCPR0Z64RkNdhqIHh7PmoueE2GP7nSbB/uwHo8yL+4/9A/H8fTY1b5EqyS0qqqFbqKgiCqBiqVgQQHBKTveq17iBmNIKdfEb6wOy4CaJlw4DqZqi4gyzKk31WN2F5tKR8XrlraHAIbDRLQL1eixWG8xfD8IP7wM5dCP7XPyH+0+/k3VtItLIOaYSIyUJJcQGCqDiqVwQGtNXC2cAcDeKb/j5lG1eLgFm6qWdTMJbsDpIHywCju4PSrc1mh+GrN4J97qvA4QOArzfnYwBQFasluYMAyhAiiAqkekVAuqGxJHfQiDjd4utAQgQ0KaLyTT2UnQgwU1JgWGYUd9BIsImTU9eYC8FEoFtBcgdRrQBBVB7Gci+gbMg3tJxEwAUA4P19UJxFaktAdiFlYwkkuYM0IpCHJaDgSrVWckKqWNa4pGSXmZ9iAgRRaVSNJcBjMXBV6wMuuzZyEQE5k0h9g5XnC9fWAcZa8X22FcN1aQLDQGEiILmseL6WQCh1qA2rk9pdkwgQRMVRPSLw2M8RX3Ub+LCU2SPHBJLrBEaAGWvFjN+BJBEwW8AYE8FkkylFBOLrX0J88z8Ta4nHRctqdUxAbUlkGiiTDbKo+Qp1ByXNM6h3kAgQRAVSNSLA5s4H+vvA39ooNgz2A1Y7mDFHj5jTDe5LFQEFkyVFBPjf/wy+/uXEBmWqWMISYAaDYgGwTANlsoAZjULY8rQEuDLPIMkaof5BBFGR6CIC9913H6699lr853/+Z8prL7zwApYsWYKBgRLfYGbOBiZOAl/7Z1H5OtCfyHrJBYdLc4PlKSJgSi0WG/CJIjAZOWagjgkACQugEEsAAJwN4P2+/N6bacZxvZOygwiiAtFFBObPn48VK1akbPd6vXj33Xfh8XhKvgbGGNiiTwMHPwL27BJ9g3KJB8jHcbq1MYFwkgjUmcFVxWI8EhbWgq830XZBFom6DCJQQHYQAMDZkH92UAZLgNU7qE6AICoQXURg5syZsNvtKdsfe+wxfPnLXx6xY2YxYXMvBGz1iP/j+bTVwlnhdAm3knxDT7YEzOaEuwcQVgAghtUHBsX30uuaFFEgkSFUSGAYUj1D3tlBcgV0kkBR/yCCqEjKliK6efNmuN1utLW1jbrv2rVrsXbtWgDAypUrc7IcjEajZn//J65A4NknAaMRllPPhCNHK2RoYisGo8NotJhgsDvQEx1GTWMTXNJx+uz14IEhuKX/D/d2Qi7bciGOWo9H2eZoGgeT6vw+VwPCABpbWmFwNuS0LjX+Cc0IbPkXGhsbUwQ2+XqkvJdxBC1WNI0bl/RzN2MwGkWj1QKDLVXQj2VGuybVBl0PLZV+PcoiAuFwGM8++yxuu+22rPZftGgRFi1apPzf6/WOsLcWj8ej2Z/PmQ8891sgEkGo1oRIDscCgLhRDKDp2bcXbOIkxAb9iE2sUc4RYwZgaFD5Pz+4X3mv76MPwWxO8E4xEGYgFAZTnT9eUyuOHQiBDee2Ls0aa03AcATeQwfArNobdvL1SHlvbw+4yZKyT5yJj0rPgY/AxjXnvbaxyGjXpNqg66GlUq5Hc3P6v9uyZAd1dnaiq6sLN998M66//nr09PTglltugc/nK/m5mdsDdsZ54j/12aeHKu93JhVjJcUEWFJ2EFelVXJfj/gmkiEmYK8H6kxgtbU5r0uDskZfxl14OIz4w3eDJ7WXSJkqJkH9gwiiMimLJTB58mT85je/Uf5//fXX46c//SkcjtxvyvnALrocfPNGsHETc3+z1DpCqRoOBUVaqExynYAcEwCAPiECPHm+sLyuRZeBzdS2sM4H5nCBAyI4PLE1/U5HDoC/9gpw0mywcy5MbE9uIy1D/YMIoiLRRQRWr16NXbt2we/3Y+nSpViyZAkWLFigx6nTwtqmw3DnIylTxbJCah2B/j7RUjoaHblOYMAnnqxr6wDZEginTxFl7ibA3ZT7mpJxJQlVOuR2F2qRAjJPNlP1D9InjE8QhB7oIgLLly8f8fV7771Xj2VoYPkGXi02cUPv79M2j5MxmYBIGDweFwVgAz7RqdRsSbheMrmDikWabqcpyGmsyamkwQDgakzdn/oHEURFUjUVw8WCMZYoGAulSaeUv5du9HzAJ/ZvaEwUjGVwBxUNqw0wGkcUAS53Ok2xBIJpK5apfxBBVCYkAvngcoP392nmCysoIqC6yTpcYC53wh0UCYs+QbV1JVmeEKpRCsaktfN07qBMxWpl6B/EvZ3FnZ9MEIQGEoF8cLjEU7Z0I2VJFcMAElXBAz4xx7jBAwz6wYcj4rU6U2mL5EZrHRFOtQR4PC5+pkzFajr3D+J730d8xXXA9i26nXMsEP/bc4ityi59miAKhUQgD5jTrXUHaVJEE3OGeXRYVAk7XAk/u683tY10KXA2AP0jTBcLpwkMR0IA5yNYAvr2D4r/7VmA80RqbZXA9+wE9u0p9zKIKoFEIB+cLvFUPyg9FWsCwypLQG5X7XCBNUhTyfp6UgfKlADmaEj196uRLQH/gJgrDCRaRmSwBPTsH8S7jgBvvy7+EwqOvHMpzh8MIL7x7+Vpk9HrFQ8R8fzmRBNELpAI5IM8ZrK7Q3xNKwLBxAhLlSXA+7xiEH2JRUAI1QB4NJr+dfnGyuPAoF/alqGNtEy9S7f+QfzvzwM1NeI/4TKIwBsbwB/7BdDVofu5lQSCMogfUX2QCOSBMnC+M50ISG6ecDjxJC7HBAD93EGOBuHaGcwQyFXPQZYDyFIbaTZSYDgaTbSbLhHcPwC+aa2YAWEyl+dm6O0UXwf1rZDmw8OJ4HuQRIAoPSQC+SDPGu46Iv6vSREVgsDDoUTmjcMlZvaaLCJDSA930CitI9TtrhWxkm/umYbaSAVjGYWlSPD1LwGRCNjizwiBLYcI9HaLr7KVpBfquROh0ootQQAkAvkhu4O6jgAGgzbVU7EEQlpLAAAa3CImEAnrExgGMgeHQ0GllbYiVmnmC6thjtL3D+KRMPgra4BTzgRrniyEsxwxAckS4ENlFIESW1wEAZAI5Ee9U+T5D/qV+cIKch8hWQRM5kTGkKtRZLqEw4ltpUISAZ6pYCwcAuTeSZII8OBoMYHS9w/ir68H/P0wXHyF2GA2J3ot6UlPl/iqswjwXrIECH0hEcgDZY4vkNpsTbYEIiGlUEx5n6tRsgT0iAlI582UIRQOiR5DxtrEPqNYAur+QSXjo91CwE44WVqL/pYAj6jiOWV0B3GKCRA6QCKQL/JN1qQVAWaoETfWUCjRMkKmoVG4Z0LB0scEautE+4hMlkAoKIrcHC5VTCC17kGDDv2DuL8fcDYkrKtyuIPkeAAABMogAvLPTpYAoQMkAvkixwXS3TDN5oQlUO9KbG9oBGIxSQRKbAkAgKNhBHdQUDzxO1yJyuJQQFQyy6mZSejSP2jApxn7ycxJXVn1wNuV+F5nS4D3eoFGaapbcEjXcxPVCYlAnjC5pXQ6EagzKymiLNkdpN6n1GQYOM85T2QoaSyBDG2k1TicpRUBfz+YWjjL4Q7qlUSgwaN/YLjXC0ycJL4ndxChAyQC+SJbAqY0ImAygweGRFDRkXiq1bRoLnVgGFKaaDpLIDosLBKTWYiUHOgNBTPHA2TsjpLFBDjnYi3qa1YOd5C3SxSqtUzRPTAMnxessUkSP3IHEaUnq3kCGzduRFtbG1pbW3HkyBE8+OCDMBgMuPbaa9HS0lLqNY5NJEuApbMETGagt0sUayXHBGRKHRgGpE6ivtTtcqGYSYoJ+PvB43GRHZQpHiBT79SmMRaTcAiIRDTuIJgtUgsFaT6DHvR0Aw0esHoHeMchfc4JMfITg35RWGi2UooooQtZ/VU9/fTTsNvFwPLHH38c06ZNw0knnaQZEVl1jBQTMJmBbjFMXu0OgsMp6goAfWICThcQDoEnP0mHVQFgRwMQj4sn3kxTxVSwemfp6gRkN5PGHaSdz6AHvKdT+OVtDn1jArK4NngAi1XMeyaIEpOVCAwMDMDlciESiWD37t246qqrcOWVV2L//v0lXt7YRWkdkTYmYEo8xaluaMxQo4gH0yMmkGnCmNIC26xNJQ2OMEtAOaYL8PvAJZErKpLVohFO2d2mp0uopwuscRxgsye6weqBJALMLUSAYgKEHmQlAg6HA0ePHsW2bdswbdo01NbWYnhYpz+MsYozswhoXETJc4xdcixBh5iAK4MIqGYcM7UIZJgqpjnmxxcBZivi93w/0UW1WMiWgCPJHQToJgJ8eFhcr8ZxgL1ebBwa1OfcvSpLwGyl7CBCF7ISgc997nO45ZZbcP/99+Oyyy4DAGzfvh1Tpkwp6eLGNA0ewF4PNr459TW1vz9ZBOS4gE4pogBSM4Q07iAXAKmyOAtLgI1rhuH67wI9XYj/8keisKpIKO0rNCmicldWndJE+7pFLMczDrBJIqCXS0jjDipTzySi6sgqMDx//nycc845AACTdPOaPn36qAPkKxlmMsFw1+MJH78a+enVWJviY2cNHnBAvxRRiBu8ZoZZcmAYkCyBLFJEAbDpM2H4xrcR/9WdiD90Nwz/52bh6ioUJSaQlB0E6HdDlGoEWOM4IBYTvyu9MoT6vEC9E6y2FsxsTbTxIIgSknVMgHMOk8mEeDyOV155BW+//TYcDkep1zemYTU16UdEypaAw5X6uo7uINjqRfVpUl6/Eig2mRND6b2dIkA8WkxAgp35cbArvwZs3QT+5j+Ls15/P2CximpnGXk9ermD5J5BjSpLQCcR4L3eRMtxi5VSRAldyEoEVq5ciY4O0Tv/97//PV544QWsWbMGjz/+eEkXd8wi3+CTXUEA2EmnAcefpE0XLRHMYACs9lSftuxakZvfOVyJttijxAQ0x1/4aSEynUeKs+CkamF5jQD0y5Tp6RLWneTuA3TsJNrnBdySCJitQEikxhJEKclKBDo6OtDW1gYA+Oc//4kVK1bgjjvuwKZNm0q5tmOXkURgyvGoueW/RQsGPbDVpz7JyjEB2dVS70pM0MrSEgCEJYR658izjHOA+/tTr5lJ55hATxfgahQ/m02kRevmDur1gqktAUD3lhl859sY3rNL13MS5SUrETAYDIhGozh48CCsVis8Hg+sVitCIZ17uhwrSDculkYEdMden/okGwqKJ/g6ye3icCV84VnEBDS43OC+4ogA/P0ZLQFd3UEeqXePyQLUGHURAR4KiGwgd5II6JghxKPDiP/qZxj8wyO6nZMoP1kFhmfPno27774bfr8f5557LgCgvb0dbre7pIs7ZlHFBMqOrT59iqjJrMQrmMMFziW3Qw6WAABR91AkSwADPrBpJ2m3ydlBegWGe7rAZpwCAOL62Ov1yQ7q6xFfG1TuIEDfWoH33gUCQ+ABfVJiibFBViKwdOlSbNiwATU1NbjgggsAAH6/H5///OdLurhjFWa2iKySMSACzFYPfuSgdmM4pK1vUK8zR0uAudzgB/fmv0AJHo+Jeb4OrSXADDXCYtHBLcKjUaCvN9HFEwCsdnA96gSkGgHZHcQs0mdIx+Awf2uj+EpZSVVFViJQW1uLRYsWIR6Po7+/H06nE7NmzSr12o5dxpQlYE91Z4SC2sZ3cuEbkLMIiE6lPvBYLGML6qwY9Iv8/GR3EKBfE7k+L8DjWhGwp4mplAAuzzBwJ1sC+tyQeXQYfNvr4nsSgaoiKxEIBAJ4+OGHsWnTJsRiMdTU1ODcc8/F17/+dVitOd40qoGWycCk48Dappd7JeImFhKtD5ixFoCUIqpOUVWLVT7uILn7p6uAjKd0fYOUNekkAj2qGgEZW73SB6qkyMNk5Gsoi7FelsB77wABEZPgwQDSJD4TFUpWgeFHHnkEoVAId911F5588kncddddiEQiePjhh0u9vmMS5mhAzffuAWuaUO6liCZoAKD28ya5gzQB7NG6iCahtKYoNDicrm+Qak08XHoR4D3S07hKBJitfsS2ETwcQuzuO8D37yns5H09gKNBjC4FFBFIfirnRw6muveKAH/rX4DFBvaxcxCvsnYVvH0fYv/xlUSadJWRlQhs27YNN954I5qbm1FbW4vm5mYsW7YM77zzTqnXRxSKnOaoDm5msgSMtWC1tbkd3yk9uWaaYJYlPF3fIBm93EE9neJpXHbJAOlTbNXs2QXseht8498LOrUoFFNZUhncQfEn70P8tw8UdK6Uc0eHwd9+HWz2HGE5RiLgsVhRzzGW4ZvWCUv00P5yL6UsZCUCdXV1GBjQNgsbGBiA0ZiVN4koIyxdE7RwKH2Tu1zjAUCiNUWhlkC6lhEyermDvF2A0624zQCIm+JwJGOPJC7l1PMdW8VQnHxRF4oBqqyoJHdQrxfw9eR/nnTs2gYEh8DO/LjuKbnlhnMOvvU18X2xUp2PMbK6iy9YsAA/+tGPcOmll6KpqQnd3d1Ys2YNFi5cWOr1EYWitD5QiXg4qHX7WO0iHz5HVxAAISCMFZ4mOuATlbpWe8pLzGwB95beL897uxM1AjJqS8qdWuDH9+wQ3/R0AUcPAxNbcz8v56JQbOZsZRsz1AgLSJUiyjkX11ndVqMIyK4gzJydsOhCgcTPXskc+FCJBaG/yOJ6jJCVCHz2s59FQ0MD/vWvf6G3txdutxuXX345FixYkNVJ7rvvPmzduhVOpxOrVq0CADz11FN46623wBiD0+nEsmXLqO6gFEgiwAf9iWCfPF9YQm4dkY8lwIxGwO4oPCYgFYqlnR5mMiea3pWSo4c1N2IAYDaHSNUM+LVP6pDaTu/bA5x+ruihtGMLWB4igMCQEGZ3k3a7JWnE5JAfiEaBaBR8OKLtsZQnfHgYfNsbYLPnCAvIYpVSU6vEEtiySYwSNVkK/wwfo2QlAowxLFiwQHPTj8Vi+OUvf4kbbrhh1PfPnz8fn/jEJ3Dvvfcq2y677DJ88YtfBAC89NJL+N///V9cd911ua6fGA2b1h3EORc31OQGds6G/JvaudyiFXUB8HR9g2SkEZOlhA8NiqfslsnaF9LFVGT27wGiwzCcPR/xjkPgO7YCF12e+8m9nQAA5hmv3Z48YlJ9kxroBxqTRCMfDu4VrqDZc6VzVo87iHMOvuVfwImniiK5Aj/Dxyp5D22Nx+P45z+z6x45c+ZMZTyljDq1NBwOp+/GSRSO2SKedGR3UCQscuFNWtePYck3YLjiK/mdw+kujiWQqa7CbAFCocJ87hLc358+C6RDZNywiZO020cYLMP37BTfHD8TbNbpwAc7xJzgXJFEIMUVlTxiUn2TSjc7Oh9kcVMqlaXPRTXUCrTvB7qPgp1xXnE+w8coZY3s/v73v8err74Kq9WKO+64I+N+a9euxdq1awGIjqYejyfjvskYjcac9q9EuuudMMWicHg8YNI0MLvHA6v6unguyPv4/ROaETl8oKDr7B3yo7Z1CpxpjjHkbsQgj8NTb9cGtPNg4A+/QXjbG/D86lllm9FohG2gD34A7pNno0a1hhjj8AKwsbj2egHoO/AhYq1t8Bw3FeHz5sO39s9wHD0A0xnn5rSmoeAgBgE0zpgJg2y5AehzOMGDAbil8wZjEciRHQeLw1SEz3XQaMAAgIbmFhg9HgwPtaAXQH2dEeYK/7sZ/NufMGQwwLPgEgx2HUHow51pP8OVfg8pqwhcddVVuOqqq/Dss8/iL3/5C5YsWZJ2v0WLFmHRokXK/71eb9bn8Hg8Oe1ficQtNgS93Yh4vXANC9/64HAMgSJdl7jJAu7rRXdXZ97DZWK+PsTrLGl/V/GYsAC8R9oTs53zJNZ1FOjuRPeHH4BJsx08Hg8G97wH1JnQixow1Rp4JAIAGDzaoblePB5D/L13wM46H16vF3z8JKCuDv2b1sMw5YSc1hQ/+BFgtaE3GAaCCUsiVlML+AeUaxI/dEB5rb/9IAxtM3K/AMnn7hJWSF8oAub1ggfF52OguwuDFf53E/vnWuCEk9E7HEPcbAUf9KO740hKrKVS7iHNzWmmIGIUEVi3bl3G12JFzCM+//zz8dOf/jSjCBAFosp15+oh88XC5RYupoH+xNCcHODhsPD5p6sRALR+6gJFQOnKeWCvZq38yCFg4qSUwDSrM4neRcnuoPYDwmUyfabYr7YOmHEq+I4tOS+Je7u0rSrkc1ssiQFAgHAH1dUBkUjKoKC8ka+HnBRQJe4gfuQgcLQdbOGnxAanquhxLBR56siIIjCaz3/mzJl5n7ijowMTJ04EAGzevDmjShFFwGZX0uCUm0qBbhU1zOkWGSX9vXmJAPw+8TVDYFhpyKe6IfKOQ4g/+DMY/vNHYJkCyumQbm78wIdgp52V2N5xSOkemoK1Xptii0R9AJue6KHFTj4dfPtb4F1HwMbl8Hn2dgITWlK3m7XTxXh/L9A4HujtLl5MIBgQ6cHy02+VBIb5lk0AY2AfE2NzmUv1GSYRSDCSnz4XVq9ejV27dsHv92Pp0qVYsmQJtm7dio6ODjDG4PF4KDOohDB7PfgB0elTaUNgKp4IKDd+Xx8wJY/3S0+1LF3fICCRtaRKE+UfvgccPgAc2ify27NFFgFV59P40KAo1mqelP499vrUTqJ7dgJuj6bPEDv5dHCIwSzZigDnHOjpBDv59NQXLYnpYsxgEJaAyw1Eh4XVVQykudJKW3FjrRCESheBfR8ALW1gsgWgfIarLzisS0wg3UD6bGsMiCJgc4g8d6jGNBZzxrEy0L4nv8Zj8g1tpOwgQJsm2id8tNzXm9s55Z//wIfKpmj7fgAAa56c5g0Q7jRViijnHPzDXWAzTtXsxsY1A00TwLdvAS68NLv1+H3CvZOcHgoIS4BzIBIS3/t6wabPAg+HwGXrqVACgZT6EFYN8439/YBL5Vp0ChHg/X1V1zwv7xRR4hjCZhf9YCJh7ZD5YiH76X355VkrN7RRYgIa/7g8hCWHSmXOubAEpMIgOS88JveMSU4PlUnuH9TdIZ7Kp6e6Q9nkaeL1bOmWagQa04iAfHMOBBLVwi63EMsiuYN4cChFBAySBVLR+Pu1bkRbvXCLVaElQCJQDahy3RV3UK4to0eAGY2FzRqWg5z2DCJgSvVTc7l/Ti5/tMMRIBZL3Lwll1C0fZ9wgSTn6UuwpJkM6eIBCjb7iF1Hk+FKjcAIIhAKJKqFnQ3i5lXMmIDFptnELDat4FYiSaNMmcEgLFoSAaISYar+QYnAcBEtAQBwFlA1PNAPmCxgptTePAAyuIOECPBcmqlJAshOOBlgDFxyCUUP7gMmtGROb7WLdtJysRrfskm4D9K1iLDageBQ9oVtct+aNALE1J1EpZsTky2BQb+YxlYoaSwBVuGWAA+HRNFk8kOHyy2C71VGQSJQCbmzVYEtyRIwGABjji2jR8NVwFOU35fZFQSkz1iRYgI5nVO2ghoagfHNSrA82r4vczwAENcvFgXCQfCuDmDHFrALFqfvc2S1iSd2qb5gVLydomdSOvecRfVzywLrdIvBOzyek8WRkWBA3PRVVLoIIFPbcrIEcmN4eBjXX399MddClArZHTToRzwkOogWu00HK2Dg/Ih9gwAxtlKVscLDIdF0DchLBJjFCjb5eODAXvBQAPHuzszxACAhooN+8A0vAwYD2AUXp99X7oIazO4Gzb2d6V1BQMJlFwok2hzLlgBQHJdQKI07yFrpIiDSfVmSJcBc+X+Gj2VGzA7atWtXxtei0WjRF0OUCKvUSXRoQFgCxUwPlXG6gX4feDyWe9Wwvz9tsZQGkzkxbF4OCrsagf6+RArlaMgZLxYrMGUa8OYG4APR/2ckS4DZ60UOeV8P+Ma1YLPPBss0StMq3VADQ9mN2/R2Zh5Dqp4uJt/wnQ1gDqdYz4APaMknJ1cgAuXB1MCwudJFwCe+plgCbtFILhIWRYJVwogi8P3vfx8ulwuGbP7AiLGLOjCcPFWsWMhVw/4B7eD6bPD3gx03SqsF9WAZ2RV03HTg7ddF0DSbgjG5OtZsBZsyTeT0v75ebBvJEpBF9JU1QGAQbEHm9E9mtUutp0e3BHg8JobEnHle+h0sSTEBqw2szgQuWQJ8wFdYOmM4KH5nKYHhyk4R5ZIlALtD+4Is2v19VVUwNqIIeDwe3HTTTZgxI7VHSSQSwVe+kmfXSUJXWJ1JuFMG/UIEilgtrJzD2SBufr7enESAx+NSpoZr5B3NiRYKXLIE2HEzwN9+XVgGWYgAlwe0WKyKC4Zve0PER0b6o5dElL+1UTx5p8sKklFbAqPh6xWxhozuoEQLB97fq+SyKz9roa0jArJlpP08MKsNCIeyt7CONTJUqGs+w1UkAiP+hqdNm4a9e/emfc1gMFR0Z72KQ8p1F+6gElkCQO4+1cF+IB4fvd2EyZywBKSMINZ2fG7nVPXJYVYbMK4ZGI7A2DpFxB0yIccEOAeb/8mR4ynSUzXPwhJQ5gikqxGAPF3MLJ7K5WphQJoEV1N4TEAOlKezBICE+63S8A+IHkzJfwfS9a22MZMjisBNN92ExYsXp33NaDRqhsQQYxx7PfiQX1QMl8ASUCoupT8gvvd9xH7wLXA5BTITfarUx5EwW7QxAatd3MSRwx9tSFsjwaZMAwDUtLaN/D55sIzFCnb2/Oz2zcISGLFGQEb2z/t6RfAdUk673Vm4JSCJYtrsIKBym8j5fYDdmSrmyoNMdY2ZHFEEjEZjxmHy8XgcTz/9dEkWRZQA2RIIBdOnIxaK0yW+9veB93Qhfu+PRV+ffR+M/D511stIqGICvM8r0jzlc2YrAsEAUFcnitsAYIqwJIyTjhvxbcxYCzSOA5t3yejzDOSn6qwsgS4xnzl5rKTmeBYhKP1JbjaHU2RVFUIGS8Ag/7/E09zKBfcPpHcf2uoBozHvyvdjlbwdfrFYDH/605+KuRailEj9b3iwNJYAM9aKQFvnYcR/+SNRjAOA945cS8Llpy7nyCLA1IFhXy/Q4BHnrHemiAB/+3XEfvjt1GKqpOpYdpzIyjFOmTrajwfDD+4Fu+LqUfdjRqNwM6SxBPiOLUptAgDhDnK6wWpHqNkwW4U1FY1qhbLeVbAlwNXZUioUS6BSM4SSqoVlGGNSlhu5g4gKRG59ULLsIEBUXL6xATh8EIaltwJ1pkQ6ZyZ8veJpOFPzOBmTas5wnxesoTFxzqQ/Wv7Om6IlxKC2/TOCAW27jOmzYPjW/4PpzI+P+qOxOlP2qa8WW1pLIP7k/Yj/8kfKzZf3jFAjoBzLChxtF9+rhJIVo3+QEiPJEBOoWHdQP1i9I/1rLjfFBIgKxV6vEoESxAQAxV3BvvAN0RrZ7QHv6x75Pb5eUTGbwe2oIFkCPDosbn5yOp+rMUVo+BExLxh+7XB4HtJ2zGSMgZ18+shB4Xyw2cGTLAHOufBF+3rAX3hKbPR2pg6XT0ZVvauJmzicgN9X2NzlYAZLQM5wqkBLgHMukhEyZaM53dpZzlXAiH95O3bsyPgaFYsdY9gconkaUJrAMADD/E+Cn3Qa2AJpWlODR+TBjwD3ZTmIxmwRWURS101IlgBzuTWzAXg8DsgiMJjkLgmmtk0uCRZbqjsoHBKtJMwW8LXPg8+dJ4Lio4gAM1uh3ObVMYF6pzheOJh/M8BAAGCGFMtQ7lnEQ8HKa6ss/x4yWALM5QZ/7x2dF1VeRhSB+++/f8Q3U4roMYSctQKUzB3EZs/V3DRYgwd819sjv8nXI8RiNOQ1dxySji1ZAk43MOADj8XEE723M5FFlM4dlGshWz7Y7GL6lxp5cM6nvgj+8v8i/qu7RKFWhs6lCmrRUsdNlNYR/fmLQHAIsKS2EDGou5dWGnIcJaMl0CAaAIZDpUmgGIOMKAKUAlo5KK0PgOJ3EM2E2yOyhaLRzO4eX+/o1cJAYqaA7B+XhcPlFoNXBnzCOjicGMbO/QPaJ9k0zdJKAbPYwAP7tRslQWITW4HP/hv4E+Jva1R3kGy1WW2aLqus3pVoHTFuYn4LTdNGGqhsd5DyexghJgBABIdzGRF6DEMxgWpBan0AYPQ0x2LR0Chu0Bl8rDwaFU9mo2QGAao1KyKQcAcBUDKE+OH9ItAMpLqDQoGizlHIiM2e6g6Sp6fVO8E+fhEgC182MQEg9RrJlkABGULpBsoAENXlNTWVKQKq30M6mHpUapVAIlAt2BMiULLAcBKsQcp/zxQcHpD+0LKNCQDgHe3CNSQ/wcoBYnmuQPsBcWO12LQjIeNxcVNL8+RbdCw24VJQpajyQZUIGAwwXPsfYJ+5euQaASAhWsnXSLqJFVQrEEptHgdIqZImS4o7iEeHxXU8hlH/HtLiFJ+naporQCJQLdjUIqCjOwiJXj8p+LKsFgYSwnW0HXA1JvzYSaX+/PABoKVNiJ5fFRMIh4RVYtFBAGV3SlD1JJ30BMrGNcNw6ZLRW3pLN2mWHMuQO2AWMms4OJRZFNV1GRBZNfHvLhVN9I5lRrEElLnDVZQmSiJQLWgCw3q5gyS/faYMIfnpPQdLAKGg4goCILI8DAYxM3g4AnQdAWudAtgdiac+IGNOfEmQZwqoawUG+0W1co4CzDK4g5ixVojNQP7uIAQDiellyVis2hGTfp8Idh89nP/5xgKD/UCdKfPvwWoXDQVJBIhKgxlrEzdSvWICFqsQnL70IpAYlJJF333VmplKBJihRgy67+8RmUPxONA8RTzpqbOD5KdyHWICLF0n0Ww6paZDcQelyWoqtGAsOARYM1yPJEsAPdLvMJt2GGOZgfTVwjKMMfFQMlrPqwqCRKCakF1COmUHMcakgrFMlkCvCEAm93VPh/rJLTmlVKry5O0iM4i1TgGzO5JEIH2ztJKQxhLg/v7sfs5kpP5IrClNpkq9Uxw3D8RAmRHqJkxJItArborJRXDHGnxw9N8Dm3Ey+M6t4OGwTqsqLyQC1YQsAnrmP49UMCbNHsiqZ73aekm2HFxucazDB4QpP65Z/KH7BxIVtRn65JSEtJbAwOitMdLAxjXD8L17gFPPTH0xyRLgH+zMHH9JJhIRxYOZ3GOWpJiA/GQcPLZFIJvfAztngahO3/Z6yZcTf309YrcvS+1zpSMkAtWETfg7WbGHzI8Aa2jM7A5SD0oZ7TjGWtHhEVp3EAAx6tHXK9JDJ7aKorF6BzAcSTSyy9AioSRIloBmpoDfJ6yTPGCTjksbQGZSEznOOeIvPIX4nf8X/OVnsjuoarZC2nOmuIOkDK8iWgI8HEb89VcQf+ZhxP7ndsRuvRZ8+1tFO35asvk9TJ8FuJsSU+dU8Lc2Zi+0WcC3bBLJDsmFjTpCIlBFMLtDvxoBGbdHVPRGh1Nfy7ZlhIwc0E7jDsKQHziwF6ylTWyT/9Bld0lQO0ugpCRZAqJvUIb2xYXgcIl+UA/9D/jzvxPbBv0jvkUhlL6NtIJZO2JSsQSKKQLrXwJ/6G7wdWvEcXu6wD8apfV4IeeTfw/Js4WTYAYD2Nx5wM63wVU1Lnzn24g/+DPEH/158dbzoTTHvZAAf4GQCFQTs06H+ex5+p6zwSNSM9NlW/h6sksPlZEFrCGNOwgQT1OtYvA6k2+48hOWLAKZAqHFxGwRPXnkG2Y4JKySUW4+OSPXCryxAezyLwNt00UBWDZI1yNjjMRsAUKhhDtNtgSCRQwMdxwEHC4YfvkH1Nz2P6lxiGITDorfQxZizM65EOBx8DdfBSAKG+NP/0Zkou16G3zv+4WvR20BFDogqABIBKoIw3kL4bj+/+p6TiYXQyXFBXg4LG6SWbqDAIgbU40xJbDH1C2WW4QIKPvIf2ShgKgkrit9PIQxJqwB2R0k/4HbiysCbNJxgNUOdu1/wvCpLwjXTrbtn2WxyGQZmS2it5HkTlN6IUUi4MNprLo84F0dwPjmRBdXs6W0g2yUAfNZiMDEScCU48FfWwdAWC3oOAT29W8DdgfiLz5V8HL4nl2J7wttC14AJAJEaZGe2lMyhOSKzOSn+pEwWwCXOzWQrLYmktxB3K+yBMxW/QanW1WdROXmcUW2BNi0E2FY/VsY5s5LPedojGYZqYfchwJC0GRBL5Y10NUBpu7PY7GUdoZBjr8Hds4C4NA+RLZvAX/+98DM2WBzLgBbfAWwYyv4aFPzRmPPrkSSRhpLIP7SM4jd95PCzpEFJAJEaZGqhlOCw7lUC0uwcRMBaS6wBllIrPaEINQnWQLBgD7VwjJWeyIwnMMTaK6oA8ZMaleRDUqq50gxAUC4Z2RXkDyGswhxAR4KiJ5S41UiYLJoC9SKTY4WGZtzPlBTA9/KW4FwEIYvXCtmUFx4CWCrR/yFwqwBvmcnMHO2SJNOIwL8gx3A+9sLOkc2kAgQJYWZreJGk+wOki0BZ/aWALvmJhiu+6/UF+Qqz5bJiZuixSb8t5II8JFaJJQCa+KGzOXWDpk6VxaLnNxBI2dLKQkE4aBSOMWKKALoOiqOqe6AmpyRVGSUmoosfw+s3gmcfAZ4YAjswkvBmieL7WYr2EWXA9vfAt+/J7+19HqBni6wE2YJUUoXE+jvEz2oIqWtVyARIEpPQ2OqO6gvh5YREsxQk3YKGGMMOOk0sNPmaLfZHYk/rlBQv0ppQIjAUJIlUOzsoGQsNiAcAo9lkXMuZ/5kuiZqd5BkCbDWNrGtKCJwRHzVuIOspZ1hoIhA9r8Hw0WXo3bmbLBPX6XZzhZ8CrDaEX/x6byWwqWsIDZ9lij6SxcTkDOTSjzpbJSZfgRRBNye1FnD/b2iZbG1OE/nNTd9L3VjvRNcdgcFhkp/E1bBrPZEpo7fN3K/mmKhzAYeGr06ORgAzJbMc5PVw+Z7ukSNxoRWAMKqKnTiGO+URSBhCTCzDu6gHH8PbMYpcJ93Ibxe7UMMs1jB5lygBI5zZs8ukQ3Vepw0KlRrCSht1gEhAk0T8jtPFugiAvfddx+2bt0Kp9OJVatWAQCeeOIJbNmyBUajEePHj8eyZctgs+lorhO6wRo84Af2ajdKNQKjdtEsBKlqGIBolpbv8JV8SLYE9BAgpXtpIAsRGMU9JtVk8FBQZAY1eBJNCIeKEBju6gCcbu0NucTuoKL/HpwuYXmNNDQpA3zPTmDaDLCaGrB6J3j3Ue0OA33pvy8BuriD5s+fjxUrVmi2nXrqqVi1ahXuuusuTJw4Ec8++6weSyHKgdsjKltVqYVZzxYuAE3/oKQh8yXHYgOiw+DDEdGvRgcRYPJNPYvgMJcsgYwoXVsD4L3dQOM4wGLP+vijnr+rAxifJMrm4rqD+NF2xF9Zo9Q6cL+vuL8HuQ1Ljk31+NAgcOQg2PSZYkO9K7VYTF2kVmJ3kC4iMHPmTNjtds220047DTWSf/eEE05Ab2/1tG6tOuQKX5/KJeTrFe0eSkm9Q5sdpEe1sIz81BwYGrVzZdFQ3EFZ3EiDgZFdcUnuINbYBNTVCbdQkWICLHl8o9kCRKPpq8vzgK97Efx3D4L/+bdiQ7EtAWueltHe9wDORTwAEO6gcFDbsE491KYaYgLr1q3Dueeem/H1tWvXYu3atQCAlStX5jTg3mg05rR/pVOO6xFumwYfAGdsGHUeDzjn6B7og2VCM+pLuJbBpgkYGvKj0VGPruEIrB4P7GnOV4prEhw/AQMAGupq0RcYRN3xM+As8XUfntiCXgD1xhqYRzlXz3AYBocLDZmuR3MLugwGWOJRBPr7YJ3UBntTE7pt9TDFY3AU8LPEA0PoHvDB1jYNNtVxAo0e+AE0Wq0wFKGmos/biQhj4Gv+AOvkNgwN+fP6PWT6fIQntojPdW0N6nI4pr99HwJGIzxnngNmMiM4sRUDANy1BtRIxwlEh+EXJ4cpHCzpZ6fsIvCnP/0JNTU1OP/88zPus2jRIixatEj5f3KQZiQ8Hk9O+1c65bgevEY0rPPt/xCGCZPAA0PgoSCCJivCJVxLvKYW4Bze3e8BAAJxIJTmfKW4JjwmXBB9hw8h7utFuNZU8uvOwxEAwEBnBwZHOVdsYACsoSntmjweD3p6egCTGYG9HwCcI2C2I+T1Im62ItTXg0gBPws/KOJDAbsLQdVx4tI16zl8CCyich2GAog/tBqGq64Dc2d/M4wd2ieCt0N++B+8C+DI6/eQ6fPBoyILq/9IO5gn+3hTbPsWYPI09PgHAf8gOBMekd4D+8EM4m8lfviQqHAf34JQZweGi/DZaW5O044cZU4RXb9+PbZs2YKbbrqptAFCorwkTxiTTd0SxwSUucpeKeimd0wA0mjN6LBO7iBVYHg0somRmCyiPTcg3EEAYLVpu6PmAe/sEN8kxQQ0tQlqDu0Htr0ugqnJx+rtRvzJ+1JaWfBgQCQftEyB4f/8F9DaJtpg5DPYJxNSTIDn4A7iA33AR7vBTjwtsTHdqND+XvGZafCU3B1UNhHYtm0b/vznP+OWW26ByWQq1zIIHWAms6ig3feByIKQ0kVzah6Xz3mloiAuFyaVIybQKY1j1DUmkIXPfqSBMurjyd1DG8dJ23JoTZEJuUagKTkwrBohqkYWnTTtlvmOLeAb/gJ8lNTQ7Wg7AIBNaAUzW2G48XvAqWeBnXhqYWtXo8R9chCBzRuBeBxs7gWJjXIjQFWaKJdnbTgbSp4dpIs7aPXq1di1axf8fj+WLl2KJUuW4Nlnn0U0GsUPf/hDAMD06dNx3XXX6bEcohxMmQZsewPxbW+IJnBAdmMlC0FuD9AtPXnqaQnIQVcpH57pkR1kNAJ1plEtAR4dFt00R6uglm/KjCmtOZjVlmgrnS9dHSI9ODlfXylQ04qA0uIiXc99KQWYH/wIbMYpifd0CBHARFHbwFxu1Nx4e2HrTkb+HQ9l2b4bouMrJh2nVB8DSDwgqDOE+vvE34ezQbRij8cy13QUiC4isHz58pRtCxYs0OPUxBjBcNP3gMMHRL3Awb3A8DDgGV/ak8pN5LrL4Q6SBsvoaQkA4sY+mjso29kK8k3Z2ZAYRGS1FzxnmHcd0VYKK+cT6+GhoLYYLbkbqxpZGA5+pN1+9JDoyeMpXZEVM9TkZBnxziPAvg/Arvya9jgms2gkp3EH9YFNnipEIB4XcyLymEyXDWUPDBPVATPWAlOOB5tyvH4nlXvEKCKgXzEiq60VKZVyZaxuImAd3Wc/ylQxBVkEZFcQoPRE4pznH8frPKJp8ZFyvuRaAcnnztNaAkIY+CGtCPCOw8C45pyLuHLGasvaEuBvrAcYA5tzQeqL9YmqYR6PibGhkjuIA8IyKJEIUO8gomJhtXUiuKmIgM5T1dRPzSXoIJqWbJrIyQNlRhmwIwdqlZkQgLjpRaPCnZQHPBgQN7t0loBltJhA6s1WaRXecUjbaO3oIWBCS15rzAlbfVaBYc65cAXNOCVlPCoAqX+QZOn4B0QQ2+kWlgBQ0uAwiQBR2djrRXYOoG8XUfX5TGYwvZIfrFm4g0ZrIy2TzhKQ35NvcLhLxGdYcrUwkBgfmiICI8QEBgdEjCkeVzKZeDQKdB8Vg2FKjS1L99i+D8T8BHn2QzIOV8IdJLdZdzYADiECpawaJhEgKhvZDVNjFO2m9UQOHOY5YD4fspopIN9kR3UHSa83JlkCQN5xAa50D00VAVZTIwLbSe4gPkJ2EAb7gWkniv3kuED3USAWUxrelRJmtWflDuJvbACMtWCnpy+KZSp3kJJC7WxQWQKl66hAIkBUNvIN2GLVvxZFbiugY/dSWKyjPqUr3U2ztASYyhJg1gItATlG0pS+cCltEzmVCCgzj5EYHM/ajhfiJItAxyGxVh1EALb6UdtG8GgUfPM/gdPOSly/ZCQR4JwnnvpdjSJobLaIGEGJIBEgKhqmEgHdzy3/wesqAiXIDnKrA8MFNpHr6pBubhncY2lFQDpXLKY9bziUGBw/aaoSHOZH5fRQPWICYpa0WpzUcM7B1zwN+PthmDs/83EcTvHzBYZUloBL+upWXESlgLKDiMqmvnwiIN8w9agRULBYgeEIeHQ4kdaZTJbZQeyUM4Gjh7UBVknYeCC/mQIiPXSEFgtmqwgeqwkMicZ10ahwCclCJLuH7A6wyVPB178sBup0tAuh0aM40FYvbt7hYIqo8nAI/NGfg7+1EezsC4HTzsp8HLmS2e8TQWB7feL353SJSuMSQZYAUdnIloCe1cIy5bAE1DMFMhEMAHV1o6ZPsqYJMHzp/2inuRXgDuLRYeDwAbDmEQK2Zktq24jAYCLf36+KC0jfs3onMHmqsAqOHhaWwEQdXEGAqpOo9nrwnm7Ef3Yr+JZ/gV35NbCvLx+x2IupCsZEtXCimp453ZQdRBB5I/9xldES0N0dBIzsrgkG8s+UshQQGP7wPSAUBJv5scz7JLmDeDQq3D7yQHp1muigPDjeATZpmtj/4F7gaLs+8QAATBm0ow0Ox3/3ANDVAcONt8Nw8RWjx6OU/kH94oYvB4QBkTnU7yvampMhESAqGjkmwPRODwVUloCe2UGS2GV4UufxOPi+D7Q3mVyOX1snxoLmYwlsf0tkaZ10WsZ9mDmpzkESM3kqnLpgTDM4fkKLWNe7m4WI6JEeCmQeLHP0MNjJZwiXWjZI7iDu9wH9vSI9VMbpFvMGSjR1jUSAqGyUwLDOhWKQ0gcBsGJ2rhyNUTqJ8jdfBdr3g130mfzPYbXnFRjm27cAJ8xKdAtNR3JgWM68kYvLBlWtI5SYgFO4rFrbwN99EwDA9CgUA9IOluGcAz6v0m8pK+TP6YBPPPWrmyvKAeISxQVIBIjKRgkMl8ESmHI80DZd+Kv1YoROonw4Av7ck8DkqelbF2RLmnbSmbJjlNe7jwIdh8BOHeXJODkmIJ2HuT0iOJwcE6gxKj8zmzQViEiVzLpZAlKPKLU7KDAo1iG3UM8CVlMjChs72oFYNDUmAAA+EgGCyB35KVx+YtMR1tiEmu+u0pr2pUbO3kljCfBX1gA9XTBc+TUwQwF/+lZt0zQ+0If4TV8Ef++djG/hO7YAANjJo4iAxQJEIiLLB0icx2oXT8vqgjF/P1DvSPjbZbG1WPN2d+WMNU076T4xNyNte4iRqHeBt+8T79W4g1ziK1kCBJE7zGaH4Zu3gp27sNxL0YcMgWE+NAi+5hlg1sfARvDJZ30OdUxg34dAKAh+aF/Gt/B33wLGTRzdTZM0U0CxOGxCBDQxgcEBTTU2k0VgQqt+hYEms7BG1AVj0ryMXCwBACKBQC6mc6rdQeL7UrWOIBEgKh52+rnKgJmKR459JAVu+cvPAMEhGD53TcGnYFZtawr56TVTVSsPh4Hd27MLkib3D5JFIJ0lMJg0OL5lCmAw6BcPAITYJPUP4pIlkFNMAFKaqOxWU1sCtnrRFrtEIkDFYgRRQTBDjbiRqtxBPBQA/8eLYGfPB5t0XOEnsdq07g+pcVtGd8Xud4HhSFYiwCxW0TpZ7h8kP2FbbWD1zkR/IADw92tak7M6E9hXb0pYBHphtWtjAn29YhCPI0eXlFrQ1DEBg0G4NUkECILICotV6w7qPAJEh8Fmn12c41vtQCAxU4C37wcA8EyWwPYtojHcCbNGP3byiMngEFBbB1ZbB26v1w6W8Q+k1GAYzi3DsCqbXWt59XkBR0PuswyUmhZbalsNZ0PJqobJHUQQlYbVpgkMc6l984jtGnI8PuJxIBwCH44k5iinKWjinIv6gJNOEzUGo6GMmJTWHxhKBF/tDtGnJxYTRWTBIV07tGYkqZMo7+vJ2RUEIFEwli6o7Wyg7CCCILIkuZOoLAJNRRq1qJ4p0HFICIKtXjseUabjENDTNXpqqIzc3kNKE+VDg6ktuYf8idjAGIj1sOROon051gjIx5Ez2dKIQCkHzpMIEESlkdxJNNNg9zxRuqMGhxRXEDvxVNH3Jh7T7CtnDLFpM7M7uGQJcLU7KFkEBgeUojFdm/NlInmwjK8HLNfMIEBxBzF1ZpCMs0G0mo7FUl8rEBIBgqgwWFJMgHd1FM8VBGibyLXvF+0ajj9JjERMHgEpZ8o0ZnlTVIrdVIHh5G6sgwOJorGx4g4KBoSbKhgQay/EHeRK4w5yNIjMIXVMpEiQCBBEpZFsCXR3gDUVUQQsiQIpfvgA0Dw5URiV7LLo6xEDfbLt4pomRVRp0ibf8P0DiXoBvWY3j4TSP2gI8OVZIwCIjCBjbdr5y6yEs4YpO4ggKg0pj59zLnzr/X3FtQRsiZkCaN8PdsoZiXTIpAwh3uvNrX2CURoDqojAUCIGIYkAHxwQrRWAMRETkK8Hhvz5VwtDWHCG7/8CcDelvqgZMzktz4Wmh0SAICoNi1UMYBmOAF1HASS6cBbn+NKT+dHD4qbf2ibaHQPg/T7tsJl8gqQWKxAKgMfjwq2lWALSE/fggPj5GEs8hZcRZqsXtQ1DfpEZBORnCQBgaawAAEBrGwzfXaUd8FMkSAQIotJQdxLtltNDM9xc8jq+cO3wD3cCAFhLm6q/jU+7r68n9+ItuZNoKCD84HJMoLZOuItkEbDatQNvyoXSP2goEQNxpQnuFgAzmUUzwhJAIkAQlYaqk2iiRqBI6aGQXDYmM/DRB2JDa5u4OdfVaWICPDosRMGVoyVgsoCHgmCqamGFeql1xPDw2HAFAdpOon09QL0zu5qIMQIFhgmiwmBqS6CrA3C4ij9v12IDosOA0w1W7xQ9dBwNWkvA1yue5HN2B0mWgDxQRt0BVmoiJ5rHjYGgMKAZLJN3oVgZIREgiEpDncdf7PRQGdlP3zIlsc3h0na67JWCpOkCnSNhtgoRGFI1j5OxO0R6qL9/bKSHAgn329CgsARytXzKDIkAQVQa6hGTXUVOD1XOIW58rLUtsS3JEsi7m6YcE5Crnm0JdxCTO4kODoyZzrDMaBRxjCE/4PPmlRlUTkgECKLSkG7QvN8n8tZLYQnI1oZKBJjDleQOyjNTxmwR2UFyFa4l2RLoT20jXW5s9cL9NejPOzOoXJAIEESlIVsCB/eKryUQAbl1hMYScLqAwYFEa4NerygUs+QYjxjBEkC9A4iERb+iMWIJABBN++SW2mQJEARRVswWgDHwAx8CKHKNgIw86GRCa2Kbw6VpbcD7vPn5x80WIBwChgYAgyFRRQxo4wBjJSYAiOshTQXLq29QGdElRfS+++7D1q1b4XQ6sWrVKgDAa6+9hmeeeQaHDx/GT37yE0ybVtwqOIKoVpjBIIKrHYfEhlJYAgs/DXbiqWC1tYltjgZRNDXgE3nyfT2AO48bopzJ1OsVw2RUoyKZ3QGufD+G3EFWu+idBJAlkI758+djxYoVmm2TJk3Cd77zHZx00kl6LIEgqguLVbhM7A5timWRYE0TwGbP1W6UqoaVWoE+b35PxXIn0d5ubWYQoH36H0MxAaW/EXDMZQfpYgnMnDkTXV1dmm2tra0Z9iYIomBkP3wpXEGZkPrb8AGfqCEY8OX3VCwPlun1pt7o1XGAsRQTkGsFrDaR3XQMcUxUDK9duxZr164FAKxcuRIeT/ZPF0ajMaf9Kx26HqlU4jXpdbgwfPgAzJPa4MzxZ8v3enC7DV0AbNEIzAbAyznqJx8HS47HCo+fAB8A9PWgrnUKGlTvj9cZ0S1975kyNXUMYwnI5noMNY3DIACjZzwaj7HP0jEhAosWLcKiRYuU/3u93qzf6/F4ctq/0qHrkUolXpOYUfjqww53zj9bQdfDZMFQxxEE9u4BAAwaTRjK8Vg8PCy+iccwXGvSrIXHY6JxXJ0JPX4/4PdnOErxyOZ6xCXPetThGrOfpebm9P2jjgkRIAgiN5jVJgKoerqDADEYZaAvUSiWV2BY5U5JigkwQ42oVjaNLZcLs9nBcexlBgEkAgRRmcgVvXqLgLMBfMAHplQL53FTVNcVqGsEZOyOMScCilgdY0FhQCcRWL16NXbt2gW/34+lS5diyZIlsNvtePjhhzEwMICVK1eira0N3/3ud/VYDkFUPuUIDAMiQ6ijXaSHmi25F4oBWkvAkprZxI47YeyJgBwYPsbSQwGdRGD58uVpt8+ZM0eP0xNE1cHmzgNMZtFrR8/zOhrAP9gh0jvzdY2YR7YEDF//dp6rKyGtU8A+/UWwj51d7pXkDLmDCKICYS1TwNQdPvXC4RL9c7yd+U/Xqq0FaoxALFqSGodSwAw1YJd9qdzLyAtqG0EQRPGQC8aOHCysm6bsErKmiQkQRYVEgCCIosHkMZOxWH6ZQTKKCBwblsCxDIkAQRDFo96V+L6QdEkSAd0gESAIonhIrSOAAnPmyR2kGyQCBEEUDzkmABRmCcippSQCJYdEgCCIosHqTIkbeAGBYWayiIE0hpoirYzIBKWIEgRRXOpdAOfK9LG8mNAC9HaPvh9RMCQCBEEUF6dLTB0rAHbZl8A+fVVx1kOMCIkAQRBFxXDx58AjoYKOwQzkqdYLEgGCIIoKO+0ssNF3I8YIJLcEQRBVDIkAQRBEFUMiQBAEUcWQCBAEQVQxJAIEQRBVDIkAQRBEFUMiQBAEUcWQCBAEQVQxjHPOy70IgiAIojxUvCVw6623lnsJYwq6HqnQNdFC10NLpV+PihcBgiAIIjMkAgRBEFVMxYvAokWLyr2EMQVdj1Tommih66Gl0q8HBYYJgiCqmIq3BAiCIIjMkAgQBEFUMRU7VGbbtm145JFHEI/HsXDhQnzmM58p95J0x+v14t5774XP5wNjDIsWLcInP/lJDA4O4u6770Z3dzeamprw7W9/G3a7vdzL1Y14PI5bb70Vbrcbt956K7q6urB69Wr4/X5MnToVN954I4zGiv3T0DA0NIQHHngAhw4dAmMM3/zmN9Hc3FzVn48XX3wR69atA2MMkyZNwrJly+Dz+Sr2M1KRlkA8HsdDDz2EFStW4O6778a//vUvtLe3l3tZulNTU4OvfOUruPvuu/HjH/8Yf/3rX9He3o7nnnsOp5xyCn7+85/jlFNOwXPPPVfuperKSy+9hJaWFuX/Tz75JC699FL84he/gM1mw7p168q4On155JFHMHv2bKxevRp33nknWlpaqvrz0dvbi5dffhkrV67EqlWrEI/HsWnTpor+jFSkCHz44YeYMGECxo8fD6PRiHPPPRebN28u97J0p6GhAVOnTgUAWCwWtLS0oLe3F5s3b8a8efMAAPPmzauqa9PT04OtW7di4cKFAADOOXbu3Imzzz4bADB//vyquR6BQADvvfceFixYAAAwGo2w2WxV/fkAxENkJBJBLBZDJBKBy+Wq6M9IZdgzSfT29qKxsVH5f2NjI/bs2VPGFZWfrq4u7Nu3D8cffzz6+/vR0NAAAHC5XOjv7y/z6vTj0UcfxdVXX41gMAgA8Pv9sFqtqKmpAQC43W709vaWc4m60dXVBYfDgfvuuw8HDhzA1KlTcc0111T158PtduPTn/40vvnNb6Kurg6nnXYapk6dWtGfkYq0BAgtoVAIq1atwjXXXAOr1ap5jTEGxqpjLPiWLVvgdDoV66jaicVi2LdvHxYvXoyf/exnMJlMKa6favp8AMDg4CA2b96Me++9Fw8++CBCoRC2bdtW7mWVlIq0BNxuN3p6epT/9/T0wO12l3FF5SMajWLVqlU4//zzMXfuXACA0+lEX18fGhoa0NfXB4fDUeZV6sPu3bvx1ltv4e2330YkEkEwGMSjjz6KQCCAWCyGmpoa9Pb2Vs1npbGxEY2NjZg+fToA4Oyzz8Zzzz1XtZ8PANi+fTvGjRun/Mxz587F7t27K/ozUpGWwLRp09DR0YGuri5Eo1Fs2rQJZ555ZrmXpTucczzwwANoaWnBpz71KWX7mWeeiQ0bNgAANmzYgLPOOqtcS9SVL33pS3jggQdw7733Yvny5Tj55JNx0003YdasWXj99dcBAOvXr6+az4rL5UJjYyOOHDkCQNwAW1tbq/bzAQAejwd79uxBOBwG51y5JpX8GanYiuGtW7fiscceQzwex4UXXojPfvaz5V6S7rz//vv43ve+h8mTJysm/VVXXYXp06fj7rvvhtfrrcoUQADYuXMnXnjhBdx6663o7OzE6tWrMTg4iOOOOw433ngjamtry71EXdi/fz8eeOABRKNRjBs3DsuWLQPnvKo/H3/4wx+wadMm1NTUoK2tDUuXLkVvb2/FfkYqVgQIgiCI0alIdxBBEASRHSQCBEEQVQyJAEEQRBVDIkAQBFHFkAgQBEFUMSQCBKEz119/Pd59991yL4MgAFRoxTBB5MP1118Pn88Hg8EAo9GIE044Af/+7/8Oj8cz4vu6urpwww034Pe//73SX4YgjhXIEiAIFbfccgueeOIJPPjgg3A6nXj44YfLvSSCKClkCRBEGurq6nD22WfjscceAyAq0J966il0dnbCarXiwgsvxJIlSwAAd9xxBwDgmmuuAQDcfvvtOOGEE7B27VqsWbMGPT09aGxsxI033qg0r9u/fz8ef/xxdHd3Y/bs2bj++utRV1en/w9KVD0kAgSRhnA4jE2bNinN1UwmE2644Qa0trbi0KFD+NGPfoS2tjbMmTMH3//+93HDDTfg0UcfVdxBr732Gp555hncfPPNmDZtGjo7OzWuotdeew0rVqxAXV0dbr/9dqxfvx6LFy8uy89KVDckAgSh4s4770RNTQ3C4TAcDge++93vAgBmzZql7DNlyhScd9552LVrF+bMmZP2OOvWrcPll1+O448/HgAwYcIEzeuXXHKJ0onyjDPOwP79+0vw0xDE6JAIEISKm2++Gaeeeiri8Tg2b96MO+64Q5m3+7vf/Q4HDx5ENBpFNBpVJk2lw+v1Yvz48Rlfd7lcyvd1dXUVNaSEOLagwDBBpMFgMGDu3LkwGAx4//338fOf/xxnnHEG7r//fjz22GO46KKLIPdeTDd0xePxoLOzU+9lE0TOkAgQRBo459i8eTOGhobQ0tKCYDAIu92Ouro6fPjhh9i4caOyr8PhAGNMc9NfsGABXnjhBXz00UfgnOPo0aPo7u4ux49CECNC7iCCUPHf//3fMBgMYIyhqakJ119/PSZNmoRrr70Wjz/+OB5++GHMnDkT55xzDoaGhgCIoPFnP/tZ3H777YjFYlixYgXOOecc+P1+3HPPPejt7cW4ceNwww03oKmpqcw/IUFooXkCBEEQVQy5gwiCIKoYEgGCIIgqhkSAIAiiiiERIAiCqGJIBAiCIKoYEgGCIIgqhkSAIAiiiiERIAiCqGL+P3TEeqCBT89WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This cell is to plot the both train and validation losses . It finds the training log by passing the p\n",
    "# path in the related file in the checkpoint directory.\n",
    "# After it finds , it parses the L1 values according to the given mode option\n",
    "# param : train (default) ===> plots the losses wrt train data\n",
    "# ----- \n",
    "# param : Val  ===> plots the losses wrt validation data\n",
    "# ----- \n",
    "\n",
    "def plotModel(vals,title,xtitle,ytitle,grid = False):\n",
    "    batch = np.arange(len(vals)) + 1\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xtitle)\n",
    "    plt.ylabel(ytitle)\n",
    "    plt.grid(grid)\n",
    "    plt.plot(batch,vals)\n",
    "    plt.show()\n",
    "    \n",
    "def getL1Losses(Lines, mod = \"train\"):\n",
    "    l1_values = []\n",
    "    for line in Lines:\n",
    "        if \"(L1)\" in line:\n",
    "            strr = re.sub(r\"\\s+\", \"\", line)\n",
    "            if mod == \"Val\":\n",
    "                if mod in strr:\n",
    "                    start = strr.find('(L1)')\n",
    "                    end = strr.rfind(\"(\")\n",
    "                    l1 = strr[start+4:end]\n",
    "                    l1_values.append(float(l1))\n",
    "            else:\n",
    "                if \"Val\" not in strr:\n",
    "                    start = strr.find('(L1)')\n",
    "                    end = strr.rfind(\"(\")\n",
    "                    l1 = strr[start+4:end]\n",
    "                    l1_values.append(float(l1))\n",
    "                \n",
    "    return l1_values\n",
    "\n",
    "def getLinesFromTextFile(path):\n",
    "    all_path = '/home/candoga01/' + path\n",
    "    file1 = open(all_path, 'r')\n",
    "    return file1.readlines()\n",
    "\n",
    "\n",
    "path = 'blg_561e_project/agedb-dir/checkpoints/agedb_resnet50_fds_model_sqrt_inv_fds_gau_5_2_0_1_0.9_adam_l1_0.001_256/training.log'\n",
    "\n",
    "\n",
    "# Data for Val\n",
    "lines = getLinesFromTextFile(path)\n",
    "l1_values_val = getL1Losses(lines,mod=\"Val\")\n",
    "\n",
    "# Data for Train\n",
    "lines = getLinesFromTextFile(path)\n",
    "l1_values_train = getL1Losses(lines)\n",
    "\n",
    "plotModel(l1_values_val,\"Validation Loss\",\"Batch\",\"L1 Loss\",True)\n",
    "plotModel(l1_values_train,\"Train Loss\",\"Batch\",\"L1 Loss\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
